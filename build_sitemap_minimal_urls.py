"""
Build a *structurally minimal* set of Lowe's /pl/ URLs from existing sitemap output.

This repo already contains `sitemap_comparison.json` generated by `fetch_lowes_sitemaps.py`.
That file is huge (hundreds of thousands of /pl/ URLs), but it represents the raw sitemap
universe including many refined/filter URLs that are redundant for "coverage discovery".

This script:
1) Reconstructs the full sitemap /pl/ URL set from:
   - sitemap_comparison.json["missing_from_map"]
   - LowesMap.txt minus sitemap_comparison.json["extra_in_map"]  (these are the "common" URLs)
2) Groups all sitemap /pl/ URLs by "base category id" (first numeric token in the last path segment).
3) Chooses a canonical URL per base category id (prefer unrefined `<...>/<id>` when present).

Outputs:
- master_sitemap_pl_list.txt
- sitemap_pl_groups_by_base_id.json
- sitemap_minimal_structural_urls.txt
- sitemap_duplicate_groups_structural.md
"""

from __future__ import annotations

import json
import re
from collections import defaultdict
from pathlib import Path
from urllib.parse import urlparse


BASE = "https://www.lowes.com"

SITEMAP_COMPARISON = Path("sitemap_comparison.json")
LOWES_MAP = Path("LowesMap.txt")

OUT_ALL = Path("master_sitemap_pl_list.txt")
OUT_GROUPS = Path("sitemap_pl_groups_by_base_id.json")
OUT_MINIMAL = Path("sitemap_minimal_structural_urls.txt")
OUT_MD = Path("sitemap_duplicate_groups_structural.md")


def _norm(url: str) -> str:
    url = url.strip()
    if not url:
        return ""
    if url.startswith("/"):
        url = BASE + url
    parsed = urlparse(url)
    return f"{BASE}{parsed.path.rstrip('/')}"


def _is_pl(url: str) -> bool:
    try:
        return urlparse(url).path.startswith("/pl/")
    except Exception:
        return False


_BASE_ID_RE = re.compile(r"^(\d{6,})")  # avoid junk like "0"


def base_category_id(url: str) -> str | None:
    """
    Return the base category id inferred from a /pl/ URL.

    Lowe's /pl/ URLs typically end with:
      .../<id>
      .../<id>-<refinement>
      .../<id>-<refinement>-<refinement2> (rare)

    We treat the *first* numeric token in the final path segment as the base id.
    """
    try:
        path = urlparse(url).path
    except Exception:
        return None
    if not path.startswith("/pl/"):
        return None
    seg = path.rstrip("/").split("/")[-1]
    m = _BASE_ID_RE.match(seg)
    return m.group(1) if m else None


def is_unrefined(url: str, cid: str) -> bool:
    """True if the final segment is exactly the base category id."""
    try:
        seg = urlparse(url).path.rstrip("/").split("/")[-1]
    except Exception:
        return False
    return seg == cid


def canonical_url(urls: list[str], cid: str) -> str:
    """
    Pick a canonical URL for a base id:
    1) Prefer unrefined `<...>/<cid>` if present.
    2) Else choose the shortest URL (usually least-refined).
    """
    for u in urls:
        if is_unrefined(u, cid):
            return u
    return sorted(urls, key=lambda u: (len(urlparse(u).path.split("/")), len(u)))[0]


def _load_lowes_map_urls() -> set[str]:
    if not LOWES_MAP.exists():
        return set()
    out: set[str] = set()
    for line in LOWES_MAP.read_text(encoding="utf-8", errors="ignore").splitlines():
        line = line.strip()
        if not line or line.startswith("#"):
            continue
        u = _norm(line)
        if _is_pl(u):
            out.add(u)
    return out


def main() -> None:
    if not SITEMAP_COMPARISON.exists():
        raise SystemExit("Missing sitemap_comparison.json (run fetch_lowes_sitemaps.py first).")

    comp = json.loads(SITEMAP_COMPARISON.read_text(encoding="utf-8"))
    missing_from_map = {_norm(u) for u in comp.get("missing_from_map", [])}
    extra_in_map = {_norm(u) for u in comp.get("extra_in_map", [])}

    lowes_map_urls = _load_lowes_map_urls()
    common_urls = lowes_map_urls - extra_in_map

    all_pl = {u for u in (missing_from_map | common_urls) if u and _is_pl(u)}

    OUT_ALL.write_text("\n".join(sorted(all_pl)) + "\n", encoding="utf-8")

    groups: dict[str, list[str]] = defaultdict(list)
    skipped = 0
    for u in sorted(all_pl):
        cid = base_category_id(u)
        if not cid:
            skipped += 1
            continue
        groups[cid].append(u)

    minimal: list[str] = []
    duplicate_groups: list[tuple[str, list[str], str]] = []
    for cid, urls in groups.items():
        urls_sorted = sorted(urls)
        canon = canonical_url(urls_sorted, cid)
        minimal.append(canon)
        if len(urls_sorted) > 1:
            duplicate_groups.append((cid, urls_sorted, canon))

    minimal = sorted(set(minimal))
    OUT_MINIMAL.write_text("\n".join(minimal) + "\n", encoding="utf-8")

    OUT_GROUPS.write_text(
        json.dumps(
            {
                "summary": {
                    "all_pl_urls": len(all_pl),
                    "base_category_ids": len(groups),
                    "minimal_structural_urls": len(minimal),
                    "duplicate_groups": len(duplicate_groups),
                    "skipped_urls_no_base_id": skipped,
                },
                "groups": {
                    cid: {
                        "canonical": canonical_url(urls, cid),
                        "count": len(urls),
                        "urls": urls,
                    }
                    for cid, urls in sorted(groups.items(), key=lambda kv: (-len(kv[1]), kv[0]))
                },
            },
            indent=2,
        ),
        encoding="utf-8",
    )

    # Write a human-readable report for duplicates (structural only).
    lines: list[str] = []
    lines.append("# Sitemap Structural Duplicate Groups\n")
    lines.append(
        "This report groups Lowe's `/pl/` URLs by **base category id** "
        "(first numeric token in the last path segment).\n"
    )
    lines.append(
        "Within a group, the URLs are usually refinements/filters for the same category, "
        "so we keep a single canonical URL (prefer unrefined `.../<id>` when present).\n"
    )
    lines.append("## Summary\n")
    lines.append(f"- Total sitemap `/pl/` URLs (reconstructed): {len(all_pl)}\n")
    lines.append(f"- Unique base category ids: {len(groups)}\n")
    lines.append(f"- Structural minimal URL count: {len(minimal)}\n")
    lines.append(f"- Duplicate groups (base ids with >1 URL): {len(duplicate_groups)}\n")
    lines.append(f"- Skipped (couldn't parse a base id): {skipped}\n")
    lines.append("\n## Duplicate Groups (top 200 by size)\n")

    duplicate_groups_sorted = sorted(duplicate_groups, key=lambda t: (-len(t[1]), t[0]))[:200]
    for cid, urls, canon in duplicate_groups_sorted:
        lines.append(f"\n### {cid} ({len(urls)} URLs)\n")
        lines.append(f"- Canonical: `{canon}`\n")
        for u in urls[:50]:
            prefix = "  - "
            lines.append(f"{prefix}{u}\n")
        if len(urls) > 50:
            lines.append(f"  - ... ({len(urls) - 50} more)\n")

    OUT_MD.write_text("".join(lines), encoding="utf-8")

    print(
        json.dumps(
            {
                "all_pl_urls": len(all_pl),
                "base_category_ids": len(groups),
                "minimal_structural_urls": len(minimal),
                "duplicate_groups": len(duplicate_groups),
                "skipped_urls_no_base_id": skipped,
                "outputs": {
                    "all_urls": str(OUT_ALL),
                    "groups": str(OUT_GROUPS),
                    "minimal": str(OUT_MINIMAL),
                    "report": str(OUT_MD),
                },
            },
            indent=2,
        )
    )


if __name__ == "__main__":
    main()

