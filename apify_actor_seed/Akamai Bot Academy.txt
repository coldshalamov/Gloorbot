Skip to main content

AcademyPlatform
API
SDK
CLI
Open source
Discord
Ask AI
Go to Console
Academy
CoursesTutorialsGlossary
* WEB SCRAPING
   * Web scraping basics with JS
   * Web scraping basics with Python
   * Puppeteer and Playwright course
   * API scraping
   * Anti-scraping protections
      * Anti-scraping techniques
      * Anti-scraping mitigation
   * Advanced web scraping
   * Web scraping basics with JS (old)
* APIFY PLATFORM
   * Introduction to the Apify platform
   * Getting started
   * Deploying your code to Apify
   * Expert scraping with Apify
* BUILD & PUBLISH ACTORS
   * Why publish Actors on Apify
   * How to pick an Actor idea
   * How to build an Actor
   * Apify Store basics
   * Promote your Actor
   * Interact with users
* * Web scraping
Anti-scraping protections
Copy for LLM
Understand the various anti-scraping measures different sites use to prevent bots from accessing them, and how to appear more human to fix these issues.
________________


If at any point in time you've strayed away from the Academy's demo content, and into the Wild West by writing some scrapers of your own, you may have been hit with anti-scraping measures. This is extremely common in the scraping world; however, the good thing is that there are always solutions.
This section covers the essentials of mitigating anti-scraping protections, such as proxies, HTTP headers and cookies, and a few other things to consider when working on a reliable and scalable crawler. Proper usage of the methods taught in the next lessons will allow you to extract data which is specific to a certain location, enable your crawler to browse websites as a logged-in user, and more.
In development, it is crucial to check and adjust the configurations related to our next lessons' topics, as doing this can fix blocking issues on the majority of websites.
Quick start
If you don't have time to read about the theory behind anti-scraping protections to fine-tune your scraping project and instead you need to get unblocked ASAP, here are some quick tips:
* Use high-quality proxies. Residential proxies are the least blocked. You can find many providers out there like Apify, BrightData, Oxylabs, NetNut, etc.
* Set real-user-like HTTP settings and browser fingerprints. Crawlee uses statistically generated realistic HTTP headers and browser fingerprints by default for all of its crawlers.
* Use a browser to pass bot capture challenges. We recommend Playwright with Firefox because it is not that common for scraping. You can also play with non-headless mode and adjust other fingerprint settings.
* Consider extracting data from private APIs or mobile app APIs. They are usually much less protected.
* Increase the number of request retries significantly to at least 10 with maxRequestRetries: 10. Rotate sessions after every error with maxErrorScore: 1
* If you cannot afford to use browsers for performance reasons, you can try Playwright.request or curl-impersonate as the HTTP library for Cheerio or Basic Crawlers, instead of its default got-scraping HTTP back end. These libraries have access to native code which offers much finer control over the HTTP traffic and mimics real browsers more than what can be achieved with plain Node.js implementation like got-scraping. These libraries should become part of Crawlee itself in the future.
In the vast majority of cases, this configuration should lead to success. Success doesn't mean that all requests will go through unblocked, that is not realistic. Some IP addresses and fingerprint combinations will still be blocked but the automatic retry system takes care of that. If you can get at least 10% of your requests through, you can still scrape the whole website with enough retries. The default SessionPool configuration will preserve the working sessions and eventually the success rate will increase.
If the above tips didn't help, you can try to fiddle with the following:
* Try different browsers. Crawlee & Playwright support Chromium, Firefox and WebKit out of the box. You can also try the Brave browser which can be configured for Playwright.
* Don't use browsers at all. Sometimes the anti-scraping protections are extremely sensitive to browser behavior but will allow plain HTTP requests (with the right headers) just fine. Don't forget to match the specific HTTP headers for each request.
* Decrease concurrency. Slower scraping means you can blend in better with the rest of the traffic.
* Add human-like behavior. Don't traverse the website like a bot (paginating quickly from 1 to 100). Instead, visit various types of pages, add time randomizations and you can even introduce some mouse movements and clicks.
* Try Puppeteer with the puppeteer-extra-plugin-stealth plugin. Generally, Crawlee's default configuration should have stronger bypassing but some features might land first in the stealth plugin.
* Find different sources of the data. The data might be rendered to the HTML but you could also find it in JavaScript (inlined in the HTML or in files) or in the API responses. Especially the APIs are often much less protected (if you use the right headers).
* Reverse engineer the JavaScript challenges that run on the page so you can figure out how the bypass them. This is a very advanced topic that you can read about online. We plan to introduce more content about this.
Keep in mind that there is no silver bullet solution. You can find many anti-scraping systems and each of them behaves differently depending the website's configuration. That is why "trying a few things" usually leads to success. You will find more details about these tricks in the mitigation section below.
First of all, why do websites want to block bots?
What's up with that?! A website might have a variety of reasons to block bots from accessing it. Here are a few of the main ones:
* To prevent the possibility of malicious bots from crawling the site to steal sensitive data like passwords or personal data about users.
* In order to avoid server performance hits due to bots making a large amount of requests to the website at a single time.
* To avoid their competitors to gain market insights about their business.
* To prevent bots from scraping their content and selling it to other websites or re-publishing it.
* To not skew their analytics data with bot traffic.
* If it is a social media website, they might be attempting to keep away bots programmed to mass create fake profiles (which are usually sold later).
We recommend checking out this article about legal and ethical ramifications of web scraping.
Unfortunately for these websites, they have to make compromises and tradeoffs. While super strong anti-bot protections will surely prevent the majority of bots from accessing their content, there is also a higher chance of regular users being flagged as bots and being blocked as well. Because of this, different sites have different scraping-difficulty levels based on the anti-scraping measures they take.
Going into this topic, it's important to understand that there is no one silver bullet solution to bypassing protections against bots. Even if two websites are using Cloudflare (for example), one of them might be significantly more difficult to scrape due to harsher CloudFlare configurations. It is all about configuration, not the anti-scraping tool itself.
The principles of anti-scraping protections
Anti-scraping protections can work on many different layers and use a large amount of bot-identification techniques.
1. Where you are coming from - The IP address of the incoming traffic is always available to the website. Proxies are used to emulate a different IP addresses but their quality matters a lot.
2. How you look - With each request, the website can analyze its HTTP headers, TLS version, ciphers, and other information. Moreover, if you use a browser, the website can also analyze the whole browser fingerprint and run challenges to classify your hardware (like graphics hardware acceleration).
3. What you are scraping - The same data can be extracted in many ways from a website. You can get the initial HTML or you can use a browser to render the full page or you can reverse engineer internal APIs. Each of those endpoints can be protected differently.
4. How you behave - The website can see patterns in how you are ordering your requests, how fast you are scraping, etc. It can also analyze browser behavior like mouse movement, clicks or key presses.
These are the 4 main principles that anti-scraping protections are based on.
Not all websites use all of these principles but they encompass the possibilities websites have to track and block bots. All techniques that help you mitigate anti-scraping protections are based on making yourself blend in with the crowd of regular users with each of these principles.
A bot can usually be detected in one of two ways, which follow two different types of web scraping:
1. Crawlers using HTTP requests
2. Crawlers using browser automation (usually with a headless browser)
Once a bot is detected, there are some countermeasures a website takes to prevent it from re-accessing it. The protection techniques are divided into two main categories:
1. Uses only the information provided within the HTTP request, such as headers, IP addresses, TLS versions, ciphers, etc.
2. Uses JavaScript evaluation to collect browser fingerprint, or even track the user behavior on the website. These JavaScript evaluations can also track mouse movement or keys pressed. Based on the information gathered, they can decide if the user is a bot or a human. This method is often paired with the first one.
Once one of these methods detects that the user is a bot, it will take countermeasures depending on how advanced its techniques are.
A common workflow of a website after it has detected a bot goes as follows:
1. The bot is added to the "greylist" (a list of suspicious IP addresses, fingerprints or any other value that can be used to uniquely identify the bot).
2. A Turing test is provided to the bot. Typically a captcha. If the bot succeeds, it is added to the whitelist.
3. If the captcha is failed, the bot is added to the blacklist.
One thing to keep in mind while navigating through this course is that advanced anti-scraping methods are able to identify non-humans not only by one value (such as a single header value, or IP address), but are able to identify them through more complex things such as header combinations.
Watch a conference talk by Ondra Urban, which provides an overview of various anti-scraping measures and tactics for circumventing them.
SEVERAL YEARS OLD?
Although the talk, given in 2021, features some outdated code examples, it still serves well as a general overview.
Common anti-scraping measures
Because we here at Apify scrape for a living, we have discovered many popular and niche anti-scraping techniques. We've compiled them into a short and comprehensible list here to help understand the roadblocks before this course teaches you how to get around them.
Not all issues you encounter are caused by anti-scraping systems. Sometimes, it's a configuration issue. Learn how to effectively debug your programs here.
IP rate-limiting
This is the most straightforward and standard protection, which is mainly implemented to prevent DDoS attacks, but it also works for blocking scrapers. Websites using rate limiting don't allow to more than some defined number of requests from one IP address in a certain time span. If the max-request number is low, then there is a high potential for false-positive due to IP address uniqueness, such as in large companies where hundreds of employees can share the same IP address.
Learn more about rate limiting in our rate limiting guide
Header checking
This type of bot identification is based on the given fact that humans are accessing web pages through browsers, which have specific header sets which they send along with every request. The most commonly known header that helps to detect bots is the User-Agent header, which holds a value that identifies which browser is being used, and what version it's running. Though User-Agent is the most commonly used header for the Header checking method, other headers are sometimes used as well. The evaluation is often also run based on the header consistency, and includes a known combination of browser headers.
URL analysis
Solely based on the way how the bots operate. It compares data-rich page visits and the other page visits. The ratio of the data-rich and regular pages has to be high to identify the bot and reduce false positives successfully.
Regular structure changes
By definition, this is not an anti-scraping method, but it can heavily affect the reliability of a scraper. If your target website drastically changes its CSS selectors, and your scraper is heavily reliant on selectors, it could break. In principle, websites using this method change their HTML structure or CSS selectors randomly and frequently, making the parsing of the data harder, and requiring more maintenance of the bot.
One of the best ways of avoiding the possible breaking of your scraper due to website structure changes is to limit your reliance on data from HTML elements as much as possible (see API Scraping and JavaScript objects within HTML)
IP session consistency
This technique is commonly used to entirely block the bot from accessing the website altogether. It works on the principle that every entity that accesses the site gets a token. This token is then saved together with the IP address and HTTP request information such as User-Agent and other specific headers. If the entity makes another request, but without the session token, the IP address is added on the greylist.
Interval analysis
This technique is based on analyzing the time intervals of the visit of a website. If the times are very similar, the entity is added to the greylist. This method’s premise is that the bot runs in regular intervals by, for example, a Cron job that starts every Monday. It is a long-term strategy, so it should be used as an extension. This technique needs only the information from the HTTP request to identify the frequency of the visits.
Browser fingerprinting
One of the most successful and advanced methods is collecting the browser's "fingerprint", which is a fancy name for information such as fonts, audio codecs, canvas fingerprint, graphics card, and more. Browser fingerprints are highly unique, so they are a reliable means of identifying a specific user (or bot). If the fingerprint provides different/inconsistent information, the user is added to the greylist.
It's important to note that this method also blocks all users that cannot evaluate JavaScript (such as bots sending only static HTTP requests), and combines both of the fundamental methods mentioned earlier.
Honeypots
The honeypot approach is based on providing links that only bots can see. A typical example is hidden pagination. Usually, the bot needs to go through all the pages in the pagination, so the website's last "fake" page has a hidden link for the user, but has the same selector as the real one. Once the bot visits the link, it is automatically blacklisted. This method needs only the HTTP information.
First up
In our first section, we'll be discussing more in-depth about the various anti-scraping methods and techniques websites use, as well as how to mitigate these protections.
Skip to main content

AcademyPlatform
API
SDK
CLI
Open source
Discord
Ask AI
Go to Console
Academy
CoursesTutorialsGlossary
* WEB SCRAPING
   * Web scraping basics with JS
   * Web scraping basics with Python
   * Puppeteer and Playwright course
   * API scraping
   * Anti-scraping protections
      * Anti-scraping techniques
         * Rate-limiting
         * Fingerprinting
         * Geolocation
         * Firewalls
         * Browser challenges
         * Captchas
      * Anti-scraping mitigation
   * Advanced web scraping
   * Web scraping basics with JS (old)
* APIFY PLATFORM
   * Introduction to the Apify platform
   * Getting started
   * Deploying your code to Apify
   * Expert scraping with Apify
* BUILD & PUBLISH ACTORS
   * Why publish Actors on Apify
   * How to pick an Actor idea
   * How to build an Actor
   * Apify Store basics
   * Promote your Actor
   * Interact with users
* * Web scraping
* Anti-scraping protections
Anti-scraping techniques
Copy for LLM
Understand the various common (and obscure) anti-scraping techniques used by websites to prevent bots from accessing their content.
________________


In this section, we'll be discussing some of the most common (as well as some obscure) anti-scraping techniques used by websites to detect and block/limit bots from accessing their content.
When a scraper is detected, a website can respond in a variety of ways:
"Access denied" page
This is a complete block which usually has a response status code of 403. Usually, you'll hit an Access denied page if you have bad IP address or the website is restricted in the country of the IP address.
For a better understanding of what all the HTTP status codes mean, we recommend checking out HTTP Cat which provides a highly professional description for each status code.
Captcha page
Probably the most common blocking method. The website gives you a chance to prove that you are not a bot by presenting you with a captcha. We'll be covering CAPTCHAs within this course.
Redirect
Another common method is redirecting to the home page of the site (or a different location).
Request timeout/Socket hangup
This is the cheapest defense mechanism where the website won't even respond to the request. Dealing with timeouts in a scraper can be challenging, because you have to differentiate them from regular network problems.
Custom status code or message
Similar to getting an Access denied page, but some sites send along specific status codes (eg. 503) and messages explaining what was wrong with the request.
Empty results
The website responds "normally," but pretends to not find any results. This requires manual testing to recognize the pattern.
Fake results
The website responds with data, but the data is totally fake, which is very difficult to recognize and requires extensive manual testing. Luckily, this type of response is not all too common.
Next up
In the first lesson of this course, you'll be learning about rate limiting, which is a technique used to prevent a large amount of requests from being sent from one user.
Edit this page
Previous
Anti-scraping protections
Next
Rate-limiting
Skip to main content

AcademyPlatform
API
SDK
CLI
Open source
Discord
Ask AI
Go to Console
Academy
CoursesTutorialsGlossary
* WEB SCRAPING
   * Web scraping basics with JS
   * Web scraping basics with Python
   * Puppeteer and Playwright course
   * API scraping
   * Anti-scraping protections
      * Anti-scraping techniques
         * Rate-limiting
         * Fingerprinting
         * Geolocation
         * Firewalls
         * Browser challenges
         * Captchas
      * Anti-scraping mitigation
   * Advanced web scraping
   * Web scraping basics with JS (old)
* APIFY PLATFORM
   * Introduction to the Apify platform
   * Getting started
   * Deploying your code to Apify
   * Expert scraping with Apify
* BUILD & PUBLISH ACTORS
   * Why publish Actors on Apify
   * How to pick an Actor idea
   * How to build an Actor
   * Apify Store basics
   * Promote your Actor
   * Interact with users
* * Web scraping
* Anti-scraping protections
* Anti-scraping techniques
Rate-limiting
Copy for LLM
Learn about rate-limiting, a common tactic used by websites to avoid a large and non-human rate of requests coming from a single IP address.
________________


When crawling a website, a web scraping bot will typically send many more requests from a single IP address than a human user could generate over the same period. Websites can monitor how many requests they receive from a single IP address, and block it or require a captcha test to continue making requests.
In the past, most websites had their own anti-scraping solutions, the most common of which was IP address rate-limiting. In recent years, the popularity of third-party specialized anti-scraping providers has dramatically increased, but a lot of websites still use rate-limiting to only allow a certain number of requests per second/minute/hour to be sent from a single IP; therefore, crawler requests have the potential of being blocked entirely quite quickly.
In cases when a higher number of requests is expected for the crawler, using a proxy and rotating the IPs is essential to let the crawler run as smoothly as possible and avoid being blocked.
Dealing with rate limiting by rotating proxy or session
The most popular and effective way of avoiding rate-limiting issues is by rotating proxies after every n number of requests, which makes your scraper appear as if it is making requests from various different places. Since the majority of rate-limiting solutions are based on IP addresses, rotating IPs allows a scraper to make large amounts to a website without getting restricted.
In Crawlee, proxies are automatically rotated for you when you use ProxyConfiguration and a SessionPool within a crawler. The SessionPool handles a lot of the nitty gritty of proxy rotating, especially with browser based crawlers by retiring a browser instance after a certain number of requests have been sent from it in order to use a new proxy (a browser instance must be retired in order to use a new proxy).
Here is an example of these features being used in a PuppeteerCrawler instance:
import { PuppeteerCrawler } from 'crawlee';
import { Actor } from 'apify';


const myCrawler = new PuppeteerCrawler({
   proxyConfiguration: await Actor.createProxyConfiguration({
       groups: ['RESIDENTIAL'],
   }),
   sessionPoolOptions: {
       // Note that a proxy is tied to a session
       sessionOptions: {
           // Let's say the website starts blocking requests after
           // 20 requests have been sent in the span of 1 minute from
           // a single user.
           // We can stay on the safe side and retire the browser
           // and rotate proxies after 15 pages (requests) have been opened.
           maxUsageCount: 15,
       },
   },
   // ...
});
Take a look at the Using proxies lesson to learn more about how to use proxies and rotate them in Crawlee.
Configuring a session pool
To set up the SessionPool for different rate-limiting scenarios, you can use various configuration options in sessionPoolOptions. In the example above, we used maxUsageCount within sessionOptions to prevent more than 15 requests from being sent using a session before it was thrown away; however, a maximum age can also be set using maxAgeSecs.
When dealing with frequent and unpredictable blockage, the maxErrorScore option can be set to trash a session after it's hit a certain number of errors.
To learn more about all configurations available in sessionPoolOptions, refer to the Crawlee documentation.
Don't worry too much about these configurations. Crawlee's defaults are usually good enough for the majority of use cases.
Next up
Though rate limiting is still common today, a lot of sites have improved over the years to use more complicated techniques such as browser fingerprinting, which is covered in the next lesson.
Edit this page
Previous
Anti-scraping techniques
Next
Fingerprinting
* Dealing with rate limiting by rotating proxy or session
   * Configuring a session pool
* Next up
Learn
* Academy
* Platform
API
* Reference
* Client for JavaScript
* Client for Python
SDK
* SDK for JavaScript
* SDK for Python
Other
* CLI
* Open source
More
* Crawlee
* GitHub
* Discord
* Trust Center
Skip to main content

AcademyPlatform
API
SDK
CLI
Open source
Discord
Ask AI
Go to Console
Academy
CoursesTutorialsGlossary
* WEB SCRAPING
   * Web scraping basics with JS
   * Web scraping basics with Python
   * Puppeteer and Playwright course
   * API scraping
   * Anti-scraping protections
      * Anti-scraping techniques
         * Rate-limiting
         * Fingerprinting
         * Geolocation
         * Firewalls
         * Browser challenges
         * Captchas
      * Anti-scraping mitigation
   * Advanced web scraping
   * Web scraping basics with JS (old)
* APIFY PLATFORM
   * Introduction to the Apify platform
   * Getting started
   * Deploying your code to Apify
   * Expert scraping with Apify
* BUILD & PUBLISH ACTORS
   * Why publish Actors on Apify
   * How to pick an Actor idea
   * How to build an Actor
   * Apify Store basics
   * Promote your Actor
   * Interact with users
* * Web scraping
* Anti-scraping protections
* Anti-scraping techniques
Fingerprinting
Copy for LLM
Understand browser fingerprinting, an advanced technique used by browsers to track user data and even block bots from accessing them.
________________


Browser fingerprinting is a method that some websites use to collect information about a browser's type and version, as well as the operating system being used, any active plugins, the time zone and language of the machine, the screen resolution, and various other active settings. All of this information is called the fingerprint of the browser, and the act of collecting it is called fingerprinting.
Yup! Surprisingly enough, browsers provide a lot of information about the user (and even their machine) that is accessible to websites! Browser fingerprinting wouldn't even be possible if it weren't for the sheer amount of information browsers provide, and the fact that each fingerprint is unique.
Based on research carried out by the Electronic Frontier Foundation, 84% of collected fingerprints are globally exclusive, and they found that the next 9% were in sets with a size of two. They also stated that even though fingerprints are dynamic, new ones can be matched up with old ones with 99.1% correctness. This makes fingerprinting a very viable option for websites that want to track the online behavior of their users in order to serve hyper-personalized advertisements to them. In some cases, it is also used to aid in preventing bots from accessing the websites (or certain sections of it).
What makes up a fingerprint?
To collect a good fingerprint, websites must collect them from various places.
From HTTP headers
Several HTTP headers can be used to create a fingerprint about a user. Here are some of the main ones:
1. User-Agent provides information about the browser and its operating system (including its versions).
2. Accept tells the server what content types the browser can render and send, and Content-Encoding provides data about the content compression.
3. Content-Language and Accept-Language both indicate the user's (and browser's) preferred language.
4. Referer gives the server the address of the previous page from which the link was followed.
A few other headers commonly used for fingerprinting can be seen below:
 Fingerprinted headers 

From window properties
The window is defined as a global variable that is accessible from JavaScript running in the browser. It is home to a vast amount of functions, variables, and constructors, and most of the global configuration is stored there.
Most of the attributes that are used for fingerprinting are stored under the window.navigator object, which holds methods and info about the user's state and identity starting with the User-Agent itself and ending with the device's battery status. All of these properties can be used to fingerprint a device; however, most fingerprinting solutions (such as Valve) only use the most crucial ones.
Here is a list of some of the most crucial properties on the window object used for fingerprinting:
Property
	Example
	Description
	screen.width
	1680
	Defines the width of the device screen.
	screen.height
	1050
	Defines the height of the device screen.
	screen.availWidth
	1680
	The portion of the screen width available to the browser window.
	screen.availHeight
	1050
	The portion of the screen height available to the browser window.
	navigator.userAgent
	'Mozilla/5.0 (X11; Linux x86_64; rv:90.0) Gecko/20100101 Firefox/90.0'
	Same as the HTTP header.
	navigator.platform
	'MacIntel'
	The platform the browser is running on.
	navigator.cookieEnabled
	true
	Whether or not the browser accepts cookies.
	navigator.doNotTrack
	'1'
	Indicates the browser's Do Not Track settings.
	navigator.buildID
	20181001000000
	The build ID of the browser.
	navigator.product
	'Gecko'
	The layout engine used.
	navigator.productSub
	20030107
	The version of the layout engine used.
	navigator.vendor
	'Google Inc.'
	Vendor of the browser.
	navigator.hardwareConcurrency
	4
	The number of logical processors the user's computer has available to run threads on.
	navigator.javaEnabled
	false
	Whether or not the user has enabled Java.
	navigator.deviceMemory
	8
	Approximately the amount of user memory (in gigabytes).
	navigator.language
	'en-US'
	The user's primary language.
	navigator.languages
	['en-US', 'cs-CZ', 'es']
	Other user languages.
	From function calls
Fingerprinting tools can also collect pieces of information that are retrieved by calling specific functions:
// Get the WebGL vendor information
WebGLRenderingContext.getParameter(37445);


// Get the WebGL renderer information
WebGLRenderingContext.getParameter(37446);


// Pass any codec into this function (ex. "audio/aac"). It will return
// either "maybe," "probably," or "" indicating whether
// or not the browser can play that codec. An empty
// string means that  it can't be played.
HTMLMediaElement.canPlayType('some/codec');


// can ask for a permission if it is not already enabled.
// allows you to know which permissions the user has
// enabled, and which are disabled
navigator.permissions.query('some_permission');
With canvases
This technique is based on rendering WebGL scenes to a canvas element and observing the rendered pixels. WebGL rendering is tightly connected with the hardware, and therefore provides high entropy. Here's a quick breakdown of how it works:
1. A JavaScript script creates a <canvas> element and renders some font or a custom shape.
2. The script then gets the pixel-map from the <canvas> element.
3. The collected pixel-map is stored in a cryptographic hash specific to the device's hardware.
Canvas fingerprinting takes advantage of the CSS3 feature for importing fonts into CSS (called WebFonts). This means it's not required to use just the machine's preinstalled fonts.
Here's an example of multiple WebGL scenes visibly being rendered differently on different machines:
 Differences in canvas element renderings 

From AudioContext
The AudioContext API represents an audio-processing graph built from audio modules linked together, each represented by an AudioNode (OscillatorNode).
In the simplest cases, the fingerprint can be obtained by checking for the existence of AudioContext. However, this doesn't provide very much information. In advanced cases, the technique used to collect a fingerprint from AudioContext is quite similar to the <canvas> method:
1. Audio is passed through an OscillatorNode.
2. The signal is processed and collected.
3. The collected signal is cryptographically hashed to provide a short ID.
A downfall of this method is that two same machines with the same browser will get the same ID.
From BatteryManager
The navigator.getBattery() function returns a promise which resolves with a BatteryManager interface. BatteryManager offers information about whether or not the battery is charging, and how much time is left until the battery has fully discharged/charged.
On its own this method is quite weak, but it can be potent when combined with the <canvas> and AudioContext fingerprinting techniques mentioned above.
Fingerprint example
When all is said and done, this is what a browser fingerprint might look like:
{
 "userAgent": "Mozilla/5.0 (X11; Linux x86_64; rv:90.0) Gecko/20100101 Firefox/90.0",
 "cookiesEnabled": true,
 "timezone": "Europe/Prague",
 "timezoneOffset": -60,
 "audioCodecs": {
   "ogg": "probably",
   "mp3": "maybe",
   "wav": "probably",
   "m4a": "maybe",
   "aac": "maybe"
 },
 "videoCodecs": {
   "ogg": "probably",
   "h264": "probably",
   "webm": "probably"
 },
 "videoCard": [
   "Intel Open Source Technology Center",
   "Mesa DRI Intel(R) HD Graphics 4600 (HSW GT2)"
 ],
 "productSub": "20100101",
 "hardwareConcurrency": 8,
 "multimediaDevices": {
   "speakers": 0,
   "micros": 0,
   "webcams": 0
 },
 "platform": "Linux x86_64",
 "pluginsSupport": true,
 "screenResolution": [ 1920, 1080 ],
 "availableScreenResolution": [ 1920, 1080 ],
 "colorDepth": 24,
 "touchSupport": {
   "maxTouchPoints": 0,
   "touchEvent": false,
   "touchStart": false
 },
 "languages": [ "en-US", "en" ]
}
How it works
Sites employ multiple levels and different approaches to collect browser fingerprints. However, they all have one thing in common: they are using a script written in JavaScript to evaluate the target browser's context and collect information about it (oftentimes also storing it in their database, or in a cookie). These scripts are often obfuscated and difficult to track down and understand, especially if they are anti-bot scripts.
Multiple levels of script obfuscation are used to make fingerprinting scripts unreadable and hard to find:
Randomization
The script is modified with some random JavaScript elements. Additionally, it also often incorporates a random number of whitespaces and other unusual formatting characters as well as cryptic variable and function names devoid of readable meaning.
Data obfuscation
Two main data obfuscation techniques are widely employed:
1. String splitting uses the concatenation of multiple substrings. It is mostly used alongside an eval() or document.write().
2. Keyword replacement allows the script to mask the accessed properties. This allows the script to have a random order of the substrings and makes it harder to detect.
Oftentimes, both of these data obfuscation techniques are used together.
Encoding
Built-in JavaScript encoding functions are used to transform the code into, for example, hexadecimal string. Or, a custom encoding function is used and a custom decoding function decodes the code as it is evaluated in the browser.
Detecting fingerprinting scripts
As mentioned above, many sites obfuscate their fingerprinting scripts to make them harder to detect. Luckily for us, there are ways around this.
Manual de-obfuscation
Almost all sites using fingerprinting and tracking scripts try to protect them as much as much as they can. However, it is impossible to make client-side JavaScript immune to reverse engineering. It is only possible to make reverse engineering difficult and unpleasant for the developer. The procedure used to make the code as unreadable as possible is called obfuscation.
When you want to dig inside the protection code to determine exactly which data is collected, you will probably have to deobfuscate it. Be aware that this can be a very time-consuming process. Code deobfuscation can take anywhere up to 1–2 days to be in a semi-readable state.
We recommend watching some videos from Jarrod Overson on YouTube to learn the tooling necessary to deobfuscate code.
Using browser extensions
Because of how common it has become to obfuscate fingerprinting scripts, there are many extensions that help identify fingerprinting scripts due to the fact that browser fingerprinting is such a big privacy question. Browser extensions such as Don't Fingerprint Me have been created to help detect them. In the extension's window, you can see a report on which functions commonly used for fingerprinting have been called, and which navigator properties have been accessed.
 Don&#39;t Fingerprint Me extension window 

This extension provides monitoring of only a few critical attributes, but in order to deceive anti-scraping protections, the full list is needed. However, the extension does reveal the scripts that collect the fingerprints.
Anti-bot fingerprinting
On websites which implement advanced fingerprinting techniques, they will tie the fingerprint and certain headers (such as the User-Agent header) to the IP address of the user. These sites will block a user (or scraper) if it made a request with one fingerprint and set of headers, then tries to make another request on the same proxy but with a different fingerprint.
When dealing with these cases, it's important to sync the generation of headers and fingerprints with the rotation of proxies (this is known as session rotation).
Next up
Next up, we'll be covering geolocation methods that websites use to grab the location from which a request has been made, and how they relate to anti-scraping.
Edit this page
Previous
Rate-limiting
Next
Geolocation
* What makes up a fingerprint?
   * From HTTP headers
   * From window properties
   * From function calls
   * With canvases
   * From AudioContext
   * From BatteryManager
* Fingerprint example
* How it works
   * Randomization
   * Data obfuscation
   * Encoding
* Detecting fingerprinting scripts
   * Manual de-obfuscation
   * Using browser extensions
* Anti-bot fingerprinting
* Next up
Learn
* Academy
* Platform
API
* Reference
* Client for JavaScript
* Client for Python
SDK
* SDK for JavaScript
* SDK for Python
Other
* CLI
* Open source
More
* Crawlee
* GitHub
* Discord
* Trust Center
Skip to main content

AcademyPlatform
API
SDK
CLI
Open source
Discord
Ask AI
Go to Console
Academy
CoursesTutorialsGlossary
* WEB SCRAPING
   * Web scraping basics with JS
   * Web scraping basics with Python
   * Puppeteer and Playwright course
   * API scraping
   * Anti-scraping protections
      * Anti-scraping techniques
         * Rate-limiting
         * Fingerprinting
         * Geolocation
         * Firewalls
         * Browser challenges
         * Captchas
      * Anti-scraping mitigation
   * Advanced web scraping
   * Web scraping basics with JS (old)
* APIFY PLATFORM
   * Introduction to the Apify platform
   * Getting started
   * Deploying your code to Apify
   * Expert scraping with Apify
* BUILD & PUBLISH ACTORS
   * Why publish Actors on Apify
   * How to pick an Actor idea
   * How to build an Actor
   * Apify Store basics
   * Promote your Actor
   * Interact with users
* * Web scraping
* Anti-scraping protections
* Anti-scraping techniques
Geolocation
Copy for LLM
Learn about the geolocation techniques to determine where requests are coming from, and a bit about how to avoid being blocked based on geolocation.
________________


Geolocation is yet another way websites can detect and block access or show limited data. Other than by using the Geolocation API (which requires user permission in order to receive location data), there are two main ways that websites geolocate a user (or bot) visiting it.
Cookies & headers
Certain websites might use certain location-specific/language-specific headers/cookies to geolocate a user. Some examples of these headers are Accept-Language and CloudFront-Viewer-Country (which is a custom HTTP header from CloudFront).
On targets which are utilizing just cookies and headers to identify the location from which a request is coming from, it is pretty straightforward to make requests which appear like they are coming from somewhere else.
IP address
The oldest (and still most common) way of geolocating is based on the IP address used to make the request. Sometimes, country-specific sites block themselves from being accessed from any other country (some Chinese, Indian, Israeli, and Japanese websites do this).
Proxies can be used in a scraper to bypass restrictions and to make requests from a different location. Oftentimes, proxies need to be used in combination with location-specific cookies/headers.
Override/emulate geolocation when using a browser-based scraper
When using Puppeteer, you can emulate the geolocation with the page.setGeolocation() function.
In Playwright, geolocation can be emulated by using browserContext.setGeolocation().
Overriding browser geolocation should be used in tandem with a proper proxy corresponding to the emulated geolocation. You would still likely get blocked if you, for example, used a German proxy with the overridden location set to Japan.
Edit this page
Previous
Fingerprinting
Next
Firewalls
* Cookies & headers
* IP address
* Override/emulate geolocation when using a browser-based scraper
Learn
* Academy
* Platform
API
* Reference
* Client for JavaScript
* Client for Python
SDK
* SDK for JavaScript
* SDK for Python
Other
* CLI
* Open source
More
* Crawlee
* GitHub
* Discord
* Trust Center
Skip to main content

AcademyPlatform
API
SDK
CLI
Open source
Discord
Ask AI
Go to Console
Academy
CoursesTutorialsGlossary
* WEB SCRAPING
   * Web scraping basics with JS
   * Web scraping basics with Python
   * Puppeteer and Playwright course
   * API scraping
   * Anti-scraping protections
      * Anti-scraping techniques
         * Rate-limiting
         * Fingerprinting
         * Geolocation
         * Firewalls
         * Browser challenges
         * Captchas
      * Anti-scraping mitigation
   * Advanced web scraping
   * Web scraping basics with JS (old)
* APIFY PLATFORM
   * Introduction to the Apify platform
   * Getting started
   * Deploying your code to Apify
   * Expert scraping with Apify
* BUILD & PUBLISH ACTORS
   * Why publish Actors on Apify
   * How to pick an Actor idea
   * How to build an Actor
   * Apify Store basics
   * Promote your Actor
   * Interact with users
* * Web scraping
* Anti-scraping protections
* Anti-scraping techniques
Firewalls
Copy for LLM
Understand what a web-application firewall is, how they work, and the various common techniques for avoiding them altogether.
________________


A web-application firewall (or WAF) is a tool for website admins which allows them to set various access rules for their visitors. The rules can vary on each website and are usually hard to detect; therefore, on sites using a WAF, you need to run a set of tests to test the rules and find out their limits.
One of the most common WAFs one can come across is the one from Cloudflare. It allows setting a waiting screen that runs a few tests against the visitor to detect a genuine visitor or a bot. However, not all WAFs are that easy to detect.
 Cloudflare waiting screen 

How it works
WAFs work on a similar premise as regular firewalls. Web admins define the rules, and the firewall executes them. As an example of how the WAF can work, we will take a look at Cloudflare's solution:
1. The visitor sends a request to the webpage.
2. The request is intercepted by the firewall.
3. The firewall decides if presenting a challenge (captcha) is necessary. If the user already solved a captcha in the past or nothing is suspicious, it will immediately forward the request to the application's server.
4. A captcha is presented which must be solved. Once it is solved, a cookie is stored in the visitor's browser.
5. The request is forwarded to the application's server.
 Cloudflare WAP workflow 

Since there are multiple providers, it is essential to say that the challenges are not always graphical and can be entirely server-side (without any JavaScript evaluation in the visitor browser).
Bypassing web-application firewalls
* Using proxies.
* Mocking headers.
* Overriding the browser's fingerprint (most effective).
* Farming the cookies from a website with a headless browser, then using the farmed cookies to do HTTP based scraping (most performant).
As you likely already know, there is no solution that fits all. If you are struggling to get past a WAF provider, you can try using Firefox with Playwright.
Next up
In the next lesson, we'll be covering browser challenges and specifically the Cloudflare browser challenge which is part of the Cloudflare WAF mentioned in this lesson.
Edit this page
Previous
Geolocation
Next
Browser challenges
* How it works
* Bypassing web-application firewalls
* Next up
Learn
* Academy
* Platform
API
* Reference
* Client for JavaScript
* Client for Python
SDK
* SDK for JavaScript
* SDK for Python
Other
* CLI
* Open source
More
* Crawlee
* GitHub
* Discord
* Trust Center
Skip to main content

AcademyPlatform
API
SDK
CLI
Open source
Discord
Ask AI
Go to Console
Academy
CoursesTutorialsGlossary
* WEB SCRAPING
   * Web scraping basics with JS
   * Web scraping basics with Python
   * Puppeteer and Playwright course
   * API scraping
   * Anti-scraping protections
      * Anti-scraping techniques
         * Rate-limiting
         * Fingerprinting
         * Geolocation
         * Firewalls
         * Browser challenges
         * Captchas
      * Anti-scraping mitigation
   * Advanced web scraping
   * Web scraping basics with JS (old)
* APIFY PLATFORM
   * Introduction to the Apify platform
   * Getting started
   * Deploying your code to Apify
   * Expert scraping with Apify
* BUILD & PUBLISH ACTORS
   * Why publish Actors on Apify
   * How to pick an Actor idea
   * How to build an Actor
   * Apify Store basics
   * Promote your Actor
   * Interact with users
* * Web scraping
* Anti-scraping protections
* Anti-scraping techniques
Browser challenges
Copy for LLM
Learn how to navigate browser challenges like Cloudflare's to effectively scrape data from protected websites.
Browser challenges
Browser challenges are a type of security measure that relies on browser fingerprints. These challenges typically involve a JavaScript program that collects both static and dynamic browser fingerprints. Static fingerprints include attributes such as User-Agent, video card, and number of CPU cores available. Dynamic fingerprints, on the other hand, might involve rendering fonts or objects in the canvas (known as a canvas fingerprint), or playing audio in the AudioContext. We were covering the details in the previous fingerprinting lesson.
While some browser challenges are relatively straightforward - for example, loading an image and checking if it renders correctly - others can be much more complex. One well-known example of a complex browser challenge is Cloudflare's browser screen check. In this challenge, Cloudflare visually inspects the browser screen and blocks the first request if any inconsistencies are found. This approach provides an extra layer of protection against automated attacks.
Many online protections incorporate browser challenges into their security measures, but the specific techniques used can vary.
Cloudflare browser challenge
One of the most well-known browser challenges is the one used by Cloudflare. Cloudflare has a massive dataset of legitimate canvas fingerprints and User-Agent pairs, which they use in conjunction with machine learning algorithms to detect any device property spoofing. This might include spoofed User-Agent headers, operating systems, or GPUs.
 Cloudflare browser check 

When you encounter a Cloudflare browser challenge, the platform checks your canvas fingerprint against the expected value. If there is a mismatch, the request is blocked. However, if your canvas fingerprint matches the expected value, Cloudflare issues a cookie that allows you to continue scraping - even without the browser - until the cookie is invalidated.
It's worth noting that Cloudflare's protection is highly customizable, and can be adjusted to be extremely strict or relatively loose. This makes it a powerful tool for website owners who want to protect against automated traffic, while still allowing legitimate traffic to flow through.
If you want to learn how to bypass Cloudflare challenge visit the Bypassing Cloudflare challenge article.
Next up
In the next lesson, we'll be covering CAPTCHAs, which were mentioned throughout this lesson. It's important to note that attempting to solve a captcha programmatically is the last resort - always try to avoid being presented with the captcha in the first place by using the techniques mentioned in this lesson.
Edit this page
Previous
Firewalls
Next
Captchas
* Browser challenges
* Cloudflare browser challenge
* Next up
Learn
* Academy
* Platform
API
* Reference
* Client for JavaScript
* Client for Python
SDK
* SDK for JavaScript
* SDK for Python
Other
* CLI
* Open source
More
* Crawlee
* GitHub
* Discord
* Trust Center
Skip to main content

AcademyPlatform
API
SDK
CLI
Open source
Discord
Ask AI
Go to Console
Academy
CoursesTutorialsGlossary
* WEB SCRAPING
   * Web scraping basics with JS
   * Web scraping basics with Python
   * Puppeteer and Playwright course
   * API scraping
   * Anti-scraping protections
      * Anti-scraping techniques
         * Rate-limiting
         * Fingerprinting
         * Geolocation
         * Firewalls
         * Browser challenges
         * Captchas
      * Anti-scraping mitigation
   * Advanced web scraping
      * Tips and tricks for robustness
      * crawling
   * Web scraping basics with JS (old)
* APIFY PLATFORM
   * Introduction to the Apify platform
   * Getting started
   * Deploying your code to Apify
   * Expert scraping with Apify
* BUILD & PUBLISH ACTORS
   * Why publish Actors on Apify
   * How to pick an Actor idea
   * How to build an Actor
   * Apify Store basics
   * Promote your Actor
   * Interact with users
* * Web scraping
Advanced web scraping
Copy for LLM
In the Web scraping basics for JavaScript devs course, we have learned the necessary basics required to create a scraper. In the following courses, we learned more about specific practices and techniques that will help us to solve most of the problems we will face.
In this course, we will take all of that knowledge, add a few more advanced concepts, and apply them to learn how to build a production-ready web scraper.
What does production-ready mean
To scrape large and complex websites, we need to scale two essential aspects of the scraper: crawling and data extraction. Big websites can have millions of pages and the data we want to extract requires more sophisticated parsing techniques than just selecting elements by CSS selectors or using APIs as they are.
We will also touch on monitoring, performance, anti-scraping protections, and debugging.
If you've managed to follow along with all of the courses prior to this one, then you're more than ready to take these upcoming lessons on 😎
First up
First, we will explore advanced crawling section that will help us to find all pages or products on the website.
Edit this page
Previous
Generating fingerprints
Next
Tips and tricks for robustness
* What does production-ready mean
* First up
Learn
* Academy
* Platform
API
* Reference
* Client for JavaScript
* Client for Python
SDK
* SDK for JavaScript
* SDK for Python
Other
* CLI
* Open source
More
* Crawlee
* GitHub
* Discord
* Trust Center
Skip to main content

AcademyPlatform
API
SDK
CLI
Open source
Discord
Ask AI
Go to Console
Academy
CoursesTutorialsGlossary
* WEB SCRAPING
   * Web scraping basics with JS
   * Web scraping basics with Python
   * Puppeteer and Playwright course
   * API scraping
   * Anti-scraping protections
      * Anti-scraping techniques
         * Rate-limiting
         * Fingerprinting
         * Geolocation
         * Firewalls
         * Browser challenges
         * Captchas
      * Anti-scraping mitigation
   * Advanced web scraping
      * Tips and tricks for robustness
      * crawling
         * Sitemaps vs search
         * Crawling sitemaps
         * Crawling with search
   * Web scraping basics with JS (old)
* APIFY PLATFORM
   * Introduction to the Apify platform
   * Getting started
   * Deploying your code to Apify
   * Expert scraping with Apify
* BUILD & PUBLISH ACTORS
   * Why publish Actors on Apify
   * How to pick an Actor idea
   * How to build an Actor
   * Apify Store basics
   * Promote your Actor
   * Interact with users
* * Web scraping
* Advanced web scraping
Tips and tricks for robustness
Copy for LLM
Learn how to make your automated processes more effective. Avoid common web scraping and web automation pitfalls, future-proof your programs and improve your processes.
________________


This collection of tips and tricks aims to help you make your scrapers work smoother and produce fewer errors.
Proofs and verification
Absence of evidence ≠ evidence of absence.
Make sure output remains consistent regardless of any changes at the target host/website:
* Always base all important checks on the presence of proof.
* Never build any important checks on the absence of anything.
The absence of an expected element or message does not prove an action has been (un)successful. The website might have been updated or expected content may no longer exist in the original form. The action relying on the absence of something might still be failing. Instead, it must rely on proof of presence.
Good: Rely on the presence of an element or other content confirming a successful action.
async function isPaymentSuccessful() {
   try {
       await page.waitForSelector('#PaymentAccepted');
   } catch (error) {
       return OUTPUT.paymentFailure;
   }


   return OUTPUT.paymentSuccess;
}
Avoid: Relying on the absence of an element that may have been updated or changed.
async function isPaymentSuccessful() {
   const $paymentAmount = await page.$('#PaymentAmount');


   if (!$paymentAmount) return OUTPUT.paymentSuccess;
}
Presumption of failure
Every action has failed until it has provably succeeded.
Always assume an action has failed before having a proof of success. Always verify important steps to avoid false positives or false negatives.
* False positive = false / failed outcome reported as true / successful on output.
* False negative = true / successful outcome reported as false / failed on output.
Assuming any action has been successful without direct proof is dangerous. Disprove failure actively through proof of success instead. Only then consider output valid and verified.
Good: Verify outcome through proof. Clearly disprove failure of an important action.
async function submitPayment() {
   await Promise.all([
       page.click('submitPayment'),
       page.waitForNavigation(),
   ]);


   try {
       await page.waitForFunction(
           (selector) => document.querySelector(selector).innerText.includes('Payment Success'),
           { polling: 'mutation' },
           '#PaymentOutcome',
       );
   } catch (error) {
       return OUTPUT.paymentFailure;
   }


   return OUTPUT.paymentSuccess;
}
Avoid: Not verifying an outcome. It can fail despite output claiming otherwise.
async function submitPayment() {
   await Promise.all([
       page.click('submitPayment'),
       page.waitForNavigation(),
   ]);


   return OUTPUT.paymentSuccess;
}
Targeting elements
Be both as specific and as generic as possible at the same time.
DOM element selectors
Make sure your CSS selectors have the best chance to remain valid after a website is updated.
* Prefer higher-specificity selectors over lower specificity ones (#id over .class).
* Use attribute selectors to search parts of attributes (prefix, suffix, etc.).
* Use element attributes with the lowest probability of a future change.
* Completely avoid or strip selectors of values that are clearly random.
* Completely avoid or strip selectors of values that are clearly flexible.
* Extend low-specificity selectors to reduce the probability of collisions.
Below is an example of stripping away too-specific parts of a selector that are likely random or subject to change.
#P_L_v201w3_t3_ReceiptToolStripLabel => a[id*="ReceiptToolStripLabel"]
If you are reasonably confident a page layout will remain without any dramatic future changes and need to increase the selector specificity to reduce the chance of a collision with other selectors, you can extend the selector as per the principle below.
#ReceiptToolStripLabel_P_L_v201w3_t3 => table li > a[id^="ReceiptToolStripLabel"]
Content pattern matching
Matching elements by content is already natively supported by Playwright. Playwright is a Node.js library that allows you to automate Chromium, Firefox and WebKit with a single API.
In Puppeteer, you can use custom utility functions to polyfill this functionality.
Event-bound flows
Always strive to make code as fluid as possible. Listen to events and react to them as needed by triggering consecutive actions immediately.
* Avoid any fixed-duration delays wherever possible.
* Prefer fluid flow based on the occurrence of events.
// Avoid:
await page.waitForTimeout(timeout);


// Good:
await page.waitForFunction(myFunction, options, args);


// Good:
await page.waitForFunction(() => {
   return window.location.href.includes('path');
});


// Good:
await page.waitForFunction(
   (selector) => document.querySelector(selector).innerText,
   { polling: 'mutation' },
   '[data-qa="btnAppleSignUp"]',
);
Edit this page
Previous
Advanced web scraping
Next
Sitemaps vs search
* Proofs and verification
* Presumption of failure
* Targeting elements
   * DOM element selectors
   * Content pattern matching
* Event-bound flows
Learn
* Academy
* Platform
API
* Reference
* Client for JavaScript
* Client for Python
SDK
* SDK for JavaScript
* SDK for Python
Other
* CLI
* Open source
More
* Crawlee
* GitHub
* Discord
* Trust Center
Skip to main content

AcademyPlatform
API
SDK
CLI
Open source
Discord
Ask AI
Go to Console
Academy
CoursesTutorialsGlossary
* WEB SCRAPING
   * Web scraping basics with JS
   * Web scraping basics with Python
   * Puppeteer and Playwright course
   * API scraping
   * Anti-scraping protections
      * Anti-scraping techniques
         * Rate-limiting
         * Fingerprinting
         * Geolocation
         * Firewalls
         * Browser challenges
         * Captchas
      * Anti-scraping mitigation
   * Advanced web scraping
      * Tips and tricks for robustness
      * crawling
         * Sitemaps vs search
         * Crawling sitemaps
         * Crawling with search
   * Web scraping basics with JS (old)
* APIFY PLATFORM
   * Introduction to the Apify platform
   * Getting started
   * Deploying your code to Apify
   * Expert scraping with Apify
* BUILD & PUBLISH ACTORS
   * Why publish Actors on Apify
   * How to pick an Actor idea
   * How to build an Actor
   * Apify Store basics
   * Promote your Actor
   * Interact with users
* * Web scraping
* Advanced web scraping
* crawling
Sitemaps vs search
Copy for LLM
The core crawling problem comes to down to ensuring that we reliably find all detail pages on the target website or inside its categories. This is trivial for small sites. We just open the home page or category pages and paginate to the end.
Unfortunately, most modern websites restrict pagination only to somewhere between 1 and 10,000 products. Solving this problem might seem relatively straightforward at first but there are multiple hurdles that we will explore in this lesson.
There are two main approaches to solving this problem:
* Extracting all page URLs from the website's sitemap.
* Using categories, search and filters to split the website so we get under the pagination limit.
Both of these approaches have their pros and cons so the best solution is to use both and combine the results. Here we will learn why.
Pros and cons of sitemaps
Sitemap is usually a simple XML file that contains a list of all pages on the website. They are created and maintained mainly for search engines like Google to help ensure that the website gets fully indexed there. They are commonly located at URLs like https://example.com/sitemap.xml or https://example.com/sitemap.xml.gz. We will get to work with sitemaps in the next lesson.
Pros
* Quick to set up - The logic to find all sitemaps and extract all URLs is usually simple and can be done in a few lines of code.
* Fast to run - You only need to run a single request for each sitemap that contains up to 50,000 URLs. This means you can get all the URLs in a matter of seconds.
* Usually complete - Websites have an incentive to keep their sitemaps up to date as they are used by search engines. This means that they usually contain all pages on the website.
Cons
* Does not directly reflect the website - There is no way you can ensure that all pages on the website are in the sitemap. The sitemap also can contain pages that were already removed and will return 404s. This is a major downside of sitemaps which prevents us from using them as the only source of URLs.
* Updated in intervals - Sitemaps are usually not updated in real-time. This means that you might miss some pages if you scrape them too soon after they were added to the website. Common update intervals are 1 day or 1 week.
* Hard to find or unavailable - Sitemaps are not always trivial to locate. They can be deployed on a CDN with unpredictable URLs. Sometimes they are not available at all.
* Streamed, compressed, and archived - Sitemaps are often streamed and archived with .tgz extensions and compressed with Gzip. This means that you cannot use default HTTP client settings and must handle these cases with extra code or use a scraping framework.
Pros and cons of categories, search, and filters
This approach means traversing the website like a normal user does by going through categories, setting up different filters, ranges, and sorting options. The goal is to ensure that we cover all categories or ranges where products can be located, and that for each of those we stay under the pagination limit.
The pros and cons of this approach are pretty much the opposite of relying on sitemaps.
Pros
* Directly reflects the website - With most scraping use-cases, we want to analyze the website as the regular users see it. By going through the intended user flow, we ensure that we are getting the same pages as the users.
* Updated in real-time - The website is updated in real-time so we can be sure that we are getting all pages.
* Often contain detailed data - While sitemaps are usually just a list of URLs, categories, searches and filters often contain additional data like product names, prices, categories, etc, especially if available via JSON API. This means that we can sometimes get all the data we need without going to the detail pages.
Cons
* Complex to set up - The logic to traverse the website is usually complex and can take a lot of time to get right. We will get to this in the next lessons.
* Slow to run - The traversing can require a lot of requests. Some filters or categories will have products we already found.
* Not always complete - Sometimes the combination of filters and categories will not allow us to ensure we have all products. This is especially painful for sites where we don't know the exact number of products we are looking for. The tools we'll build in the following lessons will help us with this.
Do we know how many products there are?
Most websites list a total number of detail pages somewhere. It might be displayed on the home page, search results, or be provided in the API response. We just need to make sure that this number really represents the whole site or category we are looking to scrape. By knowing the total number of products, we can tell if our approach to scrape all succeeded or if we still need to refine it.
Some sites, like Amazon, do not provide exact numbers. In this case, we have to work with what they give us and put even more effort into making our scraping logic accurate. We will tackle this in the following lessons as well.
Next up
Next, we will look into sitemap crawling. After that we will go through all the intricacies of the category, search and filter crawling, and build up tools implementing a generic approach that we can use on any website. At last, we will combine the results of both and set up monitoring and persistence to ensure we can run this regularly without any manual controls.
Edit this page
Previous
Tips and tricks for robustness
Next
Crawling sitemaps
* Pros and cons of sitemaps
   * Pros
   * Cons
* Pros and cons of categories, search, and filters
   * Pros
   * Cons
* Do we know how many products there are?
* Next up
Learn
* Academy
* Platform
API
* Reference
* Client for JavaScript
* Client for Python
SDK
* SDK for JavaScript
* SDK for Python
Other
* CLI
* Open source
More
* Crawlee
* GitHub
* Discord
* Trust Center
Skip to main content

AcademyPlatform
API
SDK
CLI
Open source
Discord
Ask AI
Go to Console
Academy
CoursesTutorialsGlossary
* WEB SCRAPING
   * Web scraping basics with JS
   * Web scraping basics with Python
   * Puppeteer and Playwright course
   * API scraping
   * Anti-scraping protections
      * Anti-scraping techniques
         * Rate-limiting
         * Fingerprinting
         * Geolocation
         * Firewalls
         * Browser challenges
         * Captchas
      * Anti-scraping mitigation
   * Advanced web scraping
      * Tips and tricks for robustness
      * crawling
         * Sitemaps vs search
         * Crawling sitemaps
         * Crawling with search
   * Web scraping basics with JS (old)
* APIFY PLATFORM
   * Introduction to the Apify platform
   * Getting started
   * Deploying your code to Apify
   * Expert scraping with Apify
* BUILD & PUBLISH ACTORS
   * Why publish Actors on Apify
   * How to pick an Actor idea
   * How to build an Actor
   * Apify Store basics
   * Promote your Actor
   * Interact with users
* * Web scraping
* Advanced web scraping
* crawling
Crawling sitemaps
Copy for LLM
In the previous lesson, we learned what is the utility (and dangers) of crawling sitemaps. In this lesson, we will go in-depth to how to crawl sitemaps.
We will look at the following topics:
* How to find sitemap URLs
* How to set up HTTP requests to download sitemaps
* How to parse URLs from sitemaps
* Using Crawlee to get all URLs in a few lines of code
How to find sitemap URLs
Sitemaps are commonly restricted to contain a maximum of 50k URLs so usually, there will be a whole list of them. There can be a master sitemap containing URLs of all other sitemaps or the sitemaps might simply be indexed in robots.txt and/or have auto-incremented URLs like /sitemap1.xml, /sitemap2.xml, etc.
Google
You can try your luck on Google by searching for site:example.com sitemap.xml or site:example.com sitemap.xml.gz and see if you get any results. If you do, you can try to download the sitemap and see if it contains any useful URLs. The success of this approach depends on the website telling Google to index the sitemap file itself which is rather uncommon.
robots.txt
If the website has a robots.txt file, it often contains sitemap URLs. The sitemap URLs are usually listed under Sitemap: directive.
Common URL paths
You can check some common URL paths, such as the following:
/sitemap.xml /product_index.xml /product_template.xml /sitemap_index.xml /sitemaps/sitemap_index.xml /sitemap/product_index.xml /media/sitemap.xml /media/sitemap/sitemap.xml /media/sitemap/index.xml
Make also sure you test the list with .gz, .tar.gz and .tgz extensions and by capitalizing the words (e.g. /Sitemap_index.xml.tar.gz).
Some websites also provide an HTML version, to help indexing bots find new content. Those include:
/sitemap /category-sitemap /sitemap.html /sitemap_index
Apify provides the Sitemap Sniffer, an open source actor that scans the URL variations automatically for you so that you don't have to check them manually.
How to set up HTTP requests to download sitemaps
For most sitemaps, you can make a single HTTP request and parse the downloaded XML text. Some sitemaps are compressed and have to be streamed and decompressed. The code can get fairly complicated, but scraping frameworks, such as Crawlee, can do this out of the box.
How to parse URLs from sitemaps
Use your favorite XML parser to extract the URLs from inside the <loc> tags. Just be careful that the sitemap might contain other URLs that you don't want to crawl (e.g. /about, /contact, or various special category sections). For specific code examples, see our Node.js guide.
Using Crawlee
Fortunately, you don't have to worry about any of the above steps if you use Crawlee, a scraping framework, which has rich traversing and parsing support for sitemap. It can traverse nested sitemaps, download, and parse compressed sitemaps, and extract URLs from them. You can get all the URLs in a few lines of code:
import { RobotsFile } from 'crawlee';


const robots = await RobotsFile.find('https://www.mysite.com');


const allWebsiteUrls = await robots.parseUrlsFromSitemaps();
Next up
That's all we need to know about sitemaps for now. Let's dive into a much more interesting topic - search, filters, and pagination.
Edit this page
Previous
Sitemaps vs search
Next
Crawling with search
* How to find sitemap URLs
   * Google
   * robots.txt
   * Common URL paths
* How to set up HTTP requests to download sitemaps
* How to parse URLs from sitemaps
* Using Crawlee
* Next up
Learn
* Academy
* Platform
API
* Reference
* Client for JavaScript
* Client for Python
SDK
* SDK for JavaScript
* SDK for Python
Other
* CLI
* Open source
More
* Crawlee
* GitHub
* Discord
* Trust Center
Skip to main content

AcademyPlatform
API
SDK
CLI
Open source
Discord
Ask AI
Go to Console
Academy
CoursesTutorialsGlossary
* WEB SCRAPING
   * Web scraping basics with JS
   * Web scraping basics with Python
   * Puppeteer and Playwright course
   * API scraping
   * Anti-scraping protections
      * Anti-scraping techniques
         * Rate-limiting
         * Fingerprinting
         * Geolocation
         * Firewalls
         * Browser challenges
         * Captchas
      * Anti-scraping mitigation
   * Advanced web scraping
      * Tips and tricks for robustness
      * crawling
         * Sitemaps vs search
         * Crawling sitemaps
         * Crawling with search
   * Web scraping basics with JS (old)
* APIFY PLATFORM
   * Introduction to the Apify platform
   * Getting started
   * Deploying your code to Apify
   * Expert scraping with Apify
* BUILD & PUBLISH ACTORS
   * Why publish Actors on Apify
   * How to pick an Actor idea
   * How to build an Actor
   * Apify Store basics
   * Promote your Actor
   * Interact with users
* * Web scraping
* Advanced web scraping
* crawling
Crawling sitemaps
Copy for LLM
In the previous lesson, we learned what is the utility (and dangers) of crawling sitemaps. In this lesson, we will go in-depth to how to crawl sitemaps.
We will look at the following topics:
* How to find sitemap URLs
* How to set up HTTP requests to download sitemaps
* How to parse URLs from sitemaps
* Using Crawlee to get all URLs in a few lines of code
How to find sitemap URLs
Sitemaps are commonly restricted to contain a maximum of 50k URLs so usually, there will be a whole list of them. There can be a master sitemap containing URLs of all other sitemaps or the sitemaps might simply be indexed in robots.txt and/or have auto-incremented URLs like /sitemap1.xml, /sitemap2.xml, etc.
Google
You can try your luck on Google by searching for site:example.com sitemap.xml or site:example.com sitemap.xml.gz and see if you get any results. If you do, you can try to download the sitemap and see if it contains any useful URLs. The success of this approach depends on the website telling Google to index the sitemap file itself which is rather uncommon.
robots.txt
If the website has a robots.txt file, it often contains sitemap URLs. The sitemap URLs are usually listed under Sitemap: directive.
Common URL paths
You can check some common URL paths, such as the following:
/sitemap.xml /product_index.xml /product_template.xml /sitemap_index.xml /sitemaps/sitemap_index.xml /sitemap/product_index.xml /media/sitemap.xml /media/sitemap/sitemap.xml /media/sitemap/index.xml
Make also sure you test the list with .gz, .tar.gz and .tgz extensions and by capitalizing the words (e.g. /Sitemap_index.xml.tar.gz).
Some websites also provide an HTML version, to help indexing bots find new content. Those include:
/sitemap /category-sitemap /sitemap.html /sitemap_index
Apify provides the Sitemap Sniffer, an open source actor that scans the URL variations automatically for you so that you don't have to check them manually.
How to set up HTTP requests to download sitemaps
For most sitemaps, you can make a single HTTP request and parse the downloaded XML text. Some sitemaps are compressed and have to be streamed and decompressed. The code can get fairly complicated, but scraping frameworks, such as Crawlee, can do this out of the box.
How to parse URLs from sitemaps
Use your favorite XML parser to extract the URLs from inside the <loc> tags. Just be careful that the sitemap might contain other URLs that you don't want to crawl (e.g. /about, /contact, or various special category sections). For specific code examples, see our Node.js guide.
Using Crawlee
Fortunately, you don't have to worry about any of the above steps if you use Crawlee, a scraping framework, which has rich traversing and parsing support for sitemap. It can traverse nested sitemaps, download, and parse compressed sitemaps, and extract URLs from them. You can get all the URLs in a few lines of code:
import { RobotsFile } from 'crawlee';


const robots = await RobotsFile.find('https://www.mysite.com');


const allWebsiteUrls = await robots.parseUrlsFromSitemaps();
Next up
That's all we need to know about sitemaps for now. Let's dive into a much more interesting topic - search, filters, and pagination.
Edit this page
Previous
Sitemaps vs search
Next
Crawling with search
* How to find sitemap URLs
   * Google
   * robots.txt
   * Common URL paths
* How to set up HTTP requests to download sitemaps
* How to parse URLs from sitemaps
* Using Crawlee
* Next up
Learn
* Academy
* Platform
API
* Reference
* Client for JavaScript
* Client for Python
SDK
* SDK for JavaScript
* SDK for Python
Other
* CLI
* Open source
More
* Crawlee
* GitHub
* Discord
* Trust Center
# Introduction to the Apify platform


**Learn all about the Apify platform, all of the tools it offers, and how it can improve your overall development experience.**


***


The https://apify.com was built to serve large-scale and high-performance web scraping and automation needs. It provides easy access to compute instances (https://docs.apify.com/academy/getting-started/actors.md), convenient request and result storages, proxies, scheduling, webhooks and more - all accessible through the **Console** web interface, https://docs.apify.com/api/v2.md, or our https://docs.apify.com/api/client/js and https://docs.apify.com/api/client/python API clients.


## Category outline


In this category, you'll learn how to become an Apify platform developer from the ground up. From creating your first account, to developing Actors, this is your one-stop-shop for understanding how the platform works, and how to work with it.


## First up


We'll start off this category light, by showing you how to create an Apify account and get everything ready for development with the platform. https://docs.apify.com/academy/getting-started.md


# Getting started


**Get started with the Apify platform by creating an account and learning about the Apify Console, which is where all Apify Actors are born!**


***


Your gateway to the Apify platform is your Apify account. The great thing about creating an account is that we support integration with both Google and GitHub, which takes only about 30 seconds!


1. Create your account on the https://console.apify.com/sign-up?asrc=developers_portal page.
2. Check your email, you should have a verification email with a link. Click it!
3. Done! 👍


## Getting to know the platform


Now that you have an account, you have access to https://console.apify.com?asrc=developers_portal, which is a wonderful place where you can use all of the features the platform has to offer, as well as manage and test your own projects.


## Next up


In our next lesson, we'll learn about something exciting - *Actors*. Actors are the living and breathing core of the Apify platform and are an extremely powerful concept. Let's jump https://docs.apify.com/academy/getting-started/actors.md!


# Actors


**What is an Actor? How do we create them? Learn the basics of what Actors are, how they work, and try out an Actor yourself right on the Apify platform!**


***


After you've followed the **Getting started** lesson, you're almost ready to start creating some Actors! But before we get into that, let's discuss what an Actor is, and a bit about how they work.


## What's an Actor?


When you deploy your script to the Apify platform, it is then called an **Actor**, which is a https://www.datadoghq.com/knowledge-center/serverless-architecture/serverless-microservices/#:~:text=Serverless%20microservices%20are%20cloud-based,suited%20for%20microservice-based%20architectures. that accepts an input and produces an output. Actors can run for a few seconds, hours or even infinitely. An Actor can perform anything from a basic action such as filling out a web form or sending an email, to complex operations such as crawling an entire website and removing duplicates from a large dataset.


Once an Actor has been pushed to the Apify platform, they can be shared to the world through the https://apify.com/store, and even monetized after going public.


Beyond scraping


Though the majority of Actors that are currently on the Apify platform are scrapers, crawlers, or automation software, Actors are not limited to scraping. They can be any program running in a Docker container.


## Actors on the Apify platform


For a super quick and dirty understanding of what a published Actor looks like, and how it works, let's run an SEO audit of *apify.com* using the https://apify.com/misceres/seo-audit-tool.


On the front page of the Actor, click the green **Try for free** button. If you're logged into your Apify account which you created during the https://docs.apify.com/academy/getting-started.md lesson, you'll be taken to the Apify Console and greeted with a page that looks like this:


![Actor configuration](/assets/images/seo-actor-config-6cde16dcb2bc752723bf7c6ed8364075.png)


This is where we can provide input to the Actor. The defaults here are just fine, so we'll leave it as is and click the green **Start** button to run it. While the Actor is running, you'll see it log some information about itself.


![Actor logs](/assets/images/actor-logs-a100ea07b38cdbe0ff6bc9cf3d808472.jpg)


After the Actor has completed its run (you'll know this when you see **SEO audit for apify.com finished.** in the logs), the results of the run can be viewed by clicking the **Results** tab, then subsequently the **View in another tab** option under **Export**.


## The "Actors" tab


While still on the platform, click on the tab with the **\</>** icon which says **Actors**. This tab is your one-stop-shop for seeing which Actors you've used recently, and which ones you've developed yourself. You will be frequently using this tab when developing and testing on the Apify platform.


![The \&quot;Actors\&quot; tab on the Apify platform](/assets/images/actors-tab-6244fff86563e1f10b96f275583162a2.jpg)


Now that you know the basics of what Actors are and how to use them, it's time to develop **an Actor of your own**!


## Next up


Get ready, because in the https://docs.apify.com/academy/getting-started/creating-actors.md, you'll be writing your very own Actor!


# Creating Actors


**This lesson offers hands-on experience in building and running Actors in Apify Console using a template. By the end of it, you will be able to build and run your first Actor using an Actor template.**


***


You can create an Actor in several ways. You can create one from your own source code hosted in a Git repository or in your local machine, for example. But in this tutorial, we'll focus on the easiest method: selecting an Actor code template. We don't need to install any special software, and everything can be done directly in Apify Console using an Apify account.


## Choose the source


Once you're in Apify Console, go to https://console.apify.com/actors, and click on the **Develop new** button in the top right-hand corner.


![Develop an Actor button](/assets/images/develop-new-actor-a499c8a2618fec73c828ddb4dcbb75b4.png)


You'll be presented with a page featuring two ways to get started with a new Actor.


1. Creating an Actor from existing source code (using Git providers or pushing the code from your local machine using Apify CLI)
2. Creating an Actor from a code template


| Existing source code                                                                                                    | Code templates                                                                                                          |
| ----------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- |
| ![Create and Actor from source code](/assets/images/create-actor-from-source-code-3b8f6761162e4c51daea94589b9e2407.png) | ![Create an Actor from code templates](/assets/images/create-actor-from-templates-80f2545ea6bf5071f073ab66af3d9973.png) |


## Creating Actor from existing source code


If you already have your code hosted by a Git provider, you can use it to create an Actor by linking the repository. If you use GitHub, you can use our https://docs.apify.com/platform/integrations/github.md to create an Actor from your public or private repository. You can also use GitLab, Bitbucket or other Git providers or external repositories.


![Create an Actor from Git repository](/assets/images/create-actor-git-0f6cdca6e156997d67fc7078944c97c9.png)


You can also push your existing code from your local machine using https://docs.apify.com/cli. This is useful when you develop your code locally and then you want to push it to the Apify Console to run the code as an Actor in the cloud. For this option, you'll need the https://docs.apify.com/cli/docs/installation on your machine. By clicking on the **Push your code using the Apify command-line interface (CLI)** button, you will be presented with instructions on how to push your code to the Apify Console.


![Push your code using the Apify CLI](/assets/images/create-actor-cli-4a172ba02eb3aeda5fc286317274f201.png)


## Creating Actor from code template


Python, JavaScript, and TypeScript have several template options that you can use.


Template library


You can select one from the list on this page or you can browse all the templates in the template library by clicking on the **View all templates** button in the right corner.


For example, let's choose the **Start with JavaScript** template and click on the template card.


![JavaScript template card](/assets/images/create-actor-template-javascript-card-c532263658eb98fa3d68a1b522c4af94.png)


You will end up on a template detail page where you can see all the important information about the template - description, included features, used technologies, and what is the use-case of this template. More importantly, there is a code preview and also instructions for how the code works.


![JavaScript template detail page](/assets/images/create-actor-template-detail-page-8ff37bb2c50a5756663f61ffca76a010.png)


### Using the template in the Web IDE


By clicking **Use this template** button you will create the Actor in Apify Console and you will be moved to the **Code** tab with the https://docs.apify.com/platform/actors/development/quick-start/web-ide.md where you can see the code of the template and start editing it.


Web IDE


The Web IDE is a great tool for developing your Actor directly in Apify Console without the need to install or use any other software.


![Web IDE](/assets/images/create-actor-web-ide-53857177e9d96389456c6d0e5feff72a.png)


### Using the template locally


If you want to use the template locally, you can again use our https://docs.apify.com/cli to download the template to your local machine.


Local development


Creating an Actor from a template locally is a great option if you want to develop your code using your local environment and IDE and then push the final solution back to the Apify Console.


When you click on the **Use locally** button, you'll be presented with instructions on how to create an Actor from this template in your local environment.


With the Apify CLI installed, you can run the following commands in your terminal:




```
apify create my-actor -t getting_started_node
```






```
cd my-actor
apify run
```




![Use the template locally](/assets/images/create-actor-template-locally-b4d9caaebe286c60cbc29017f02ab3d4.png)


## Start with scraping single page


This template is a great starting point for web scraping as it extracts data from a single website. It uses https://axios-http.com/docs/intro for downloading the page content and https://cheerio.js.org/ for parsing the HTML from the content.


Let's see what's inside the **Start with JavaScript** template. The main logic of the template lives in the `src/main.js` file.




```
// Axios - Promise based HTTP client for the browser and node.js (Read more at https://axios-http.com/docs/intro).
import { Actor } from 'apify';
import axios from 'axios';
// Cheerio - The fast, flexible & elegant library for parsing and manipulating HTML and XML (Read more at https://cheerio.js.org/).
import * as cheerio from 'cheerio';
// Apify SDK - toolkit for building Apify Actors (Read more at https://docs.apify.com/sdk/js/).


// The init() call configures the Actor for its environment. It's recommended to start every Actor with an init().
await Actor.init();


// Structure of input is defined in input_schema.json
const input = await Actor.getInput();
const { url } = input;


// Fetch the HTML content of the page.
const response = await axios.get(url);


// Parse the downloaded HTML with Cheerio to enable data extraction.
const $ = cheerio.load(response.data);


// Extract all headings from the page (tag name and text).
const headings = [];
$('h1, h2, h3, h4, h5, h6').each((i, element) => {
    const headingObject = {
        level: $(element).prop('tagName').toLowerCase(),
        text: $(element).text(),
    };
    console.log('Extracted heading', headingObject);
    headings.push(headingObject);
});


// Save headings to Dataset - a table-like storage.
await Actor.pushData(headings);


// Gracefully exit the Actor process. It's recommended to quit all Actors with an exit().
await Actor.exit();
```




The Actor takes the `url` from the input and then:


1. Sends a request to the URL.
2. Downloads the page's HTML content.
3. Extracts headings (H1 - H6) from the page.
4. Stores the extracted data.


The extracted data is stored in the https://docs.apify.com/platform/storage/dataset.md where you can preview it and download it. We'll show how to do that later in  section.


Customize template


Feel free to play around with the code and add some more features to it. For example, you can extract all the links from the page or extract all the images or completely change the logic of this template.


Keep in mind that this template uses https://docs.apify.com/academy/deploying-your-code/input-schema.md defined in the `.actor/input_schema.json` file and linked to the `.actor/actor.json`. If you want to change the input schema, you need to change it in those files as well.


Learn more about the Actor input and output https://docs.apify.com/academy/getting-started/inputs-outputs.md.


## Build the Actor


In order to run the Actor, you need to https://docs.apify.com/platform/actors/development/builds-and-runs/builds.md it first. Click on the **Build** button at the bottom of the page or **Build now** button right under the code editor.


![Build the Actor](/assets/images/build-actor-5aaefc12ec3684c08bd92818b88e3576.png)


After you've clicked the **Build** button, it'll take around 5–10 seconds to complete the build. You'll know it's finished when you see a green **Start** button.


![Start button](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAARgAAABsCAMAAACGlF3dAAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAHWaVRYdFhNTDpjb20uYWRvYmUueG1wAAAAAAA8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIiB4OnhtcHRrPSJYTVAgQ29yZSA2LjAuMCI+CiAgIDxyZGY6UkRGIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+CiAgICAgIDxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSIiCiAgICAgICAgICAgIHhtbG5zOmV4aWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20vZXhpZi8xLjAvIj4KICAgICAgICAgPGV4aWY6UGl4ZWxZRGltZW5zaW9uPjEwODwvZXhpZjpQaXhlbFlEaW1lbnNpb24+CiAgICAgICAgIDxleGlmOlBpeGVsWERpbWVuc2lvbj4yODA8L2V4aWY6UGl4ZWxYRGltZW5zaW9uPgogICAgICAgICA8ZXhpZjpVc2VyQ29tbWVudD5TY3JlZW5zaG90PC9leGlmOlVzZXJDb21tZW50PgogICAgICA8L3JkZjpEZXNjcmlwdGlvbj4KICAgPC9yZGY6UkRGPgo8L3g6eG1wbWV0YT4KG+KORAAAAAlwSFlzAAAWJQAAFiUBSVIk8AAAAeBQTFRFAIon8/j0hr2JCIsqWqhhyuHKhb2I////0NXrlMWXEYws/f79YKtn0ufUOZlGZ69us9a1v93B/P383e3eYKtm/v7+8ffyjcGRSJ9S+/371enXRp9QJZI3MJY/K5Q8i8CP3O3dzuTOE40vcLN1tte46/TricCOQ55Oebh+rtOvIZE16fTpP5xL3Ozd0ebTmcicOJlFsta0+fv59fn10ObRYaxor9Sxc7V4S6BUq9KuW6ljf7qEX6plX6pm2OrZLZQ8ir+OzeTOzuXPx+DJ7/bw+vz5cbR2y+TNm8idKZM7fbmBoMuj5PHk1Nnta7Bx5+n05uj07/D43uDwNphE9/j7+vz6PptJd7V8xN/FZK1pudi75fLmFY4v2eva5PHljsGRvNq+8Pfxos2l1urYM5dAXqtm6/Tsir+Nl8eaWadhs9a0kMOUmMebkcSVx+DILJU9wN3CLZQ9zuTPjcGQjMGQcrR34/Djmcid9vr3T6NZbbJzqdCssdWzIZE2LpU9p8+pG48xocykvdu/fLiBTaJWhr6KHJAyqNCr7PXtM5dCa7Fx6vPqiL+Nncqhaa9vxuDHSqFV4O/i8vjxgbuFO5pHqtGt3+7gmMicVKZenMmgrtOwu9m95/Lny+LLe7d/pM2m1hvhngAABDpJREFUeNrt3Od301YYBvAnKO4TIQF2HCdx9t6TEDLYe4V0pGxK997QQQu0ZbTs1ULLavuv9oMkK461aGwn5r7307XOPbrHv3N1l3RfUJJnghAIjMAIjMAIjMAIjMAIjMAIjCSBERiBKS7M4CdbNzZh2SdysXdo2rg1Phgd5huURlo8DACgJiLMtnqoBYPa1igwnwGqwQDD4TDboCIMWkNh6tWEqQ2DOQM1YbJ74JdzYFqgKgxaXIXXK3Jg4urCxF2FV3NhutSFaXYVXsmF2aQuTJWr8FouTJO6MF+7CgdzYaAuzDyLCoERGIERGIERGIERGIERGIERmP8Do1MvFxife+gCk5PKrcK6wHjDRJAZ+PVSx+VG5WDCaC5WWqXWTigHEyTTNuQW6wUA9BiGEd58+g3jbgnDxMpjITTvkmRn66rqTKkVJFeH3r2MXFXKMIAeKNNNctc4AHSlyH6VYIJpPiT5uZXtIHkYA2YvyX9Msw8A0DB7oLfHfrAumOY0MPDXw0dalbmWnDHNeyUN48jEPKZ775CssrI/TU1NzeENp5INAPbVWfmxKwDwEsnGapLr651C1YuH8fjspWgwAY3mGMk6bd6F+TDdaeeHcceGuc98w+gL/51eTJhM9Qtr1VIkP/i0L3NhReJPkn8nEpNAjGT/08TvJE/YMGTliVNdDYmEQY4lErfz8Cjpz+OSdxjfRtNrXd65o8dpN5nOV0uR6wAgSaY0G2Zv/jtf/TlcCgDj1L+wo1nv3DW943A2DNpOngQATJCctmCOF2JU0qO7FALGntLk9MAff2FPfblr0mu47hs4QnKNBWMWZLjWI7sUACZgpqd9O/pdiiTTv2XDPBh95qDZMCsLM4/RIy948w1T7j9i20uDvQbJI1kwPZVunYWFsWR0FBsmdGEAAO0k35oPM0KSp44+efxv4WGgR9wgKdYE76vNmz9y8vYglIEZJjkBAONFgIGuo8gwgc2lkzR+cVvMqA1zAwDqyEMAgOPeMP0vxCLSp3fpINn5AwDsTpMcB7Cb5B4N1vdu7QBuefUxnSS7X+Bth4Y0SZYNJsdIcgYA5kjSuGZan3XdXFft2fkOkWTZnhLfqPIfjNC90y021AZ7omutlewlJOs8YKbztVZaSpjAlygrjzpbmwfsK/vOnyW5H0DHIZKVV0dIHgP+IOmuN9uTJN8uZZhYaMmR/ed+/rEv6wm73mBlVs9Oar6kje9ppQsTW/J3bssTRl64eU4mY/KKdhkngREYgREYgREYgREYgREYgRGY4sAofFj0y8DDonK82Od4sRxI9zmQLiEMfEIYtKgLkwwMesEaVWHeDA6TIoF1fCMOtaoJsz08eNewijDfRwn3drpWNZja7REDBNaoBRM1QCDJlnhz1RYVYLa83xxPShBSic4qMAIjMAIjMAIjMAIjMJIERmAERmAEZsnTf45EnbI+9eB+AAAAAElFTkSuQmCC)


## Fill the input


And now we are ready to run the Actor. But before we do that, let's give the Actor some input by going to the `Input` tab.


The input tab is where you can provide the Actor with some meaningful input. In this case, we'll be providing the Actor with a URL to scrape. For now, we'll use the prefilled value of https://apify.com/ (`https://apify.com/`).


You can change the website you want to extract the data from by changing the URL in the input field.


![Input tab](/assets/images/actor-input-tab-93256e980a452661e0a608910bddecb1.png)


## Run the Actor


Once you have provided the Actor with some URL you want to extract the data from, click **Start** button and wait a few seconds. You should see the Actor run logs in the **Last run** tab.


![Actor run logs](/assets/images/actor-run-1c928e9040dac9112be91f2bfbfde02f.png)


After the Actor finishes, you can preview or download the extracted data by clicking on the **Export X results** button.


![Export results](/assets/images/actor-run-dataset-a27223a2b496df661e18f8e311c9bfc4.png)


And that's it! You've just created your first Actor and extracted data from a website 🎉.


## Next up


We've created an Actor, but how can we give it more complex inputs and make it do stuff based on these inputs? This is exactly what we'll be discussing in the https://docs.apify.com/academy/getting-started/inputs-outputs.md's activity.


# Inputs & outputs


**Create an Actor from scratch which takes an input, processes that input, and then outputs a result that can be used elsewhere.**


***


Actors, as any other programs, take inputs and generate outputs. The Apify platform has a way how to specify what inputs the Actor expects, and a way to temporarily or permanently store its results.


In this lesson, we'll be demonstrating inputs and outputs by building an Actor which takes two numbers as input, adds them up, and then outputs the result.


## Accept input into an Actor


Let's first create another new Actor using the same template as before. Feel free to refer to the https://docs.apify.com/academy/getting-started/creating-actors.md for a refresher on how to do this.


Replace all of the code in **main.js** with this code snippet:




```
import { Actor } from 'apify';


await Actor.init();


// Grab our numbers which were inputted
const { num1, num2 } = await Actor.getInput();


// Calculate the solution
const solution = num1 + num2;


// Push the solution to the dataset
await Actor.pushData({ solution });


await Actor.exit();
```




Then, replace everything in `INPUT_SCHEMA.json` with this:


Optional step


This step isn't necessary, as the Actor will still be able to take input in JSON format without it, however, we are providing the content for this Actor's input schema in this lesson, as it will give the Apify platform a blueprint off of which it can generate a nice UI for your inputs, as well as validate their values.




```
{
    "title": "Number adder",
    "type": "object",
    "schemaVersion": 1,
    "properties": {
        "num1": {
            "title": "1st Number",
            "type": "integer",
            "description": "First number.",
            "editor": "number"
        },
        "num2": {
            "title": "2nd Number",
            "type": "integer",
            "description": "Second number.",
            "editor": "number"
        }
    },
    "required": ["num1", "num2"]
}
```




Learn more


If you're interested in learning more about how the code works, and what the `INPUT_SCHEMA.json` means, read about https://docs.apify.com/sdk/js/docs/examples/accept-user-input and https://docs.apify.com/sdk/js/docs/examples/add-data-to-dataset in the Apify SDK documentation, and refer to the https://docs.apify.com/platform/actors/development/actor-definition/input-schema/specification/v1.md#integer.


Finally, **Save** and **Build** the Actor just as you did in the previous lesson.


## Configuring an Actor with inputs


By default, after running a build, the **Last build** tab will be selected, where you can see all of the logs related to building the Actor. Inputs can be configured within the **Input** tab.


![Configuring inputs](/assets/images/configure-inputs-0efc6f6ade028079e5da7b87e966bdcf.jpg)


Enter any two numbers you'd like, then press **Start**. The Actor's run should be completed almost immediately.


## View Actor results


Since we've pushed the result into the default dataset, it, and some info about it, can be viewed in two places inside the Last Run tab:


1. **Export** button
2. **Storage** → **Dataset** (scroll below the main view)


On the results tab, there are a whole lot of options for which format to view/download the data in. Keep the default of **JSON** selected, and click on **Preview**.


![Dataset preview](/assets/images/dataset-preview-da23f5956de7eccb38a691f09fd3dd1c.png)


There's our solution! Did it work for you as well? Now, we can download the data right from the Dataset tab to be used elsewhere, or even programmatically retrieve it by using https://docs.apify.com/api/v2.md (we'll be discussing how to do this in the next lesson).


It's important to note that the default dataset of the Actor, which we pushed our solution to, will be retained for 7 days. If we wanted the data to be retained for an indefinite period of time, we'd have to use a named dataset. For more information about named storages vs unnamed storages, read a bit about https://docs.apify.com/platform/storage/usage.md#data-retention.


## Next up


In https://docs.apify.com/academy/getting-started/apify-api.md's fun activity, you'll learn how to call the Actor we created in this lesson programmatically using one of Apify's most powerful tools - the Apify API.


# The Apify API


**Learn how to use the Apify API to programmatically call your Actors, retrieve data stored on the platform, view Actor logs, and more!**


***


https://docs.apify.com/api/v2.md is your ticket to the Apify platform without even needing to access the https://console.apify.com?asrc=developers_portal web-interface. The API is organized around RESTful HTTP endpoints.


In this lesson, we'll be learning how to use the Apify API to call an Actor and view its results. We'll be using the Actor we created in the previous lesson, so if you haven't already gotten that one set up, go ahead do that before moving forward if you'd like to follow along.


## Finding your endpoint


Within one of your Actors on the https://console.apify.com?asrc=developers_portal (we'll use the **adding-actor** from the previous lesson), click on the **API** button in the top right-hand corner:


![The \&quot;API\&quot; button on an Actor\&#39;s page on the Apify Console](/assets/images/api-tab-1fb75598685ed64e58605cd51734d19c.jpg)


You should see a long list of API endpoints that you can copy and paste elsewhere, or even test right within the **API** modal. Go ahead and copy the endpoint labeled **Run Actor synchronously and get dataset items**. It should look something like this:




```
https://api.apify.com/v2/acts/YOUR_USERNAME~adding-actor/run-sync?token=YOUR_TOKEN
```




Single endpoint


In this lesson, we'll only be focusing on this one endpoint, as it is the most popularly used one, however, don't let this limit your curiosity! Take a look at the other endpoints in the **API** window to learn about everything you can do to your Actor programmatically.


Now, let's move over to our favorite HTTP client (in this lesson we'll use https://docs.apify.com/academy/tools/insomnia.md in order to prepare and send the request).


## Providing input


Our **adding-actor** takes in two input values (`num1` and `num2`). When using the Actor on the platform, provide these fields either through the UI generated by the **INPUT\_SCHEMA.json**, or directly in JSON format. When providing input when making an API call to run an Actor, the input must be provided in the **body** of the POST request as a JSON object.


![Providing input](/assets/images/provide-input-16fe316e976462f5e2d9ede9158b6b8b.jpg)


## Parameters


Let's say we want to run our **adding-actor** via API and view its results in CSV format at the end. We'll achieve this by passing the **format** parameter with a value of **csv** to change the output format:




```
https://api.apify.com/v2/acts/YOUR_USERNAME~adding-actor/run-sync-get-dataset-items?token=YOUR_TOKEN_HERE&format=csv
```




Additional parameters can be passed to this endpoint. You can learn about them in our https://docs.apify.com/api/v2/act-run-sync-get-dataset-items-post.md


Token security


Network components can record visited URLs, so it's more secure to send the token as a HTTP header, not as a parameter. The header should look like `Authorization: Bearer YOUR_TOKEN`. Popular HTTP clients, such as https://docs.apify.com/academy/tools/postman.md or https://docs.apify.com/academy/tools/insomnia.md, provide a convenient way to configure the Authorization header for all your API requests.


## Sending the request


If you're not using an HTTP client, you can send the request through your terminal with this command:




```
curl -d '{"num1":1, "num2":8}' -H "Content-Type: application/json" -X POST "https://api.apify.com/v2/acts/YOUR_USERNAME~adding-actor/run-sync-get-dataset-items?token=YOUR_TOKEN_HERE&format=csv"
```




Here's the response we got:


![API response](/assets/images/api-csv-response-486ba68d3939c6f5c9328f8fefa5c7a2.png)


And there it is! The Actor was run with our inputs of **num1** and **num2**, then the dataset results were returned back to us in CSV format.


## Apify API's many features


What we've done in this lesson only scratches the surface of what the Apify API can do. Right from Insomnia, or from any HTTP client, you can manage https://docs.apify.com/api/v2/storage-datasets.md and https://docs.apify.com/api/v2/storage-key-value-stores.md, add to https://docs.apify.com/api/v2/storage-request-queues.md, https://docs.apify.com/api/v2/act-put.md, and much more! Basically, whatever you can do on the platform's web interface, you also do through the API.


## Next up


https://docs.apify.com/academy/getting-started/apify-client.md, we'll be learning about how to use Apify's JavaScript and Python clients to interact with the API right within our code.


# Apify client


**Interact with the Apify API in your code by using the apify-client package, which is available for both JavaScript and Python.**


***


Now that you've gotten your toes wet with interacting with the Apify API through raw HTTP requests, you're ready to become familiar with the **Apify client**, which is a package available for both JavaScript and Python that allows you to interact with the API in your code without explicitly needing to make any GET or POST requests.


This lesson will provide code examples for both Node.js and Python, so regardless of the language you are using, you can follow along!


## Examples


You can access `apify-client` examples in the Console Actor detail page. Click the **API** button and then the **API Client** dropdown button.


![API button](/assets/images/api-button-16287c6b358ebf6ad02c35f2ece5c333.png)


## Installing and importing


If you are going to use the client in Node.js, use this command within one of your projects to install the package through npm:




```
npm install apify-client
```




In Python, you can install it from PyPI with this command:




```
pip install apify-client
```




After installing the package, let's make a file named `client.js` (or `client.py` for Python) and import the Apify client like so:


* Node.js
* Python




```
// client.js
import { ApifyClient } from 'apify-client';
```






```
# client.py
from apify_client import ApifyClient
```




## Running an Actor


In the last lesson, we ran the **adding-actor** and retrieved its dataset items. That's exactly what we're going to do now; however, by using the Apify client instead.


Before we can use the client though, we must create a new instance of the `ApifyClient` class and pass it our API token from the https://console.apify.com/account?tab=integrations&asrc=developers_portal on the Apify Console:


* Node.js
* Python




```
const client = new ApifyClient({
    token: 'YOUR_TOKEN',
});
```






```
client = ApifyClient(token='YOUR_TOKEN')
```




Environment variables


If you are planning on publishing your code to a public GitHub/Gitlab repository or anywhere else online, be sure to set your API token as en environment variable, and never hardcode it directly into your script.


Now that we've got our instance, we can point to an Actor using the https://docs.apify.com/api/client/js/reference/class/ApifyClient#actor function, then call the Actor with some input with the https://docs.apify.com/api/client/js/reference/class/ApifyClient#actor function - the first parameter of which is the input for the Actor.


* Node.js
* Python




```
const run = await client.actor('YOUR_USERNAME/adding-actor').call({
    num1: 4,
    num2: 2,
});
```






```
run = client.actor('YOUR_USERNAME/adding-actor').call(run_input={
    'num1': 4,
    'num2': 2
})
```




Learn more


Learn more about the `.call()` function in our https://docs.apify.com/api/client/js/reference/class/ApifyClient#actor.


## Downloading dataset items


Once an Actor's run has completed, it will return a **run info** object that looks something like this:


![Run info object](/assets/images/run-info-5744283cdcb67851aa05d10ef782d69d.jpg)


The `run` variable we created in the last section points to the **run info** object of the run we created with the `.call()` function, which means that through this variable, we can access the run's `defaultDatasetId`. This ID can then be passed into the `client.dataset()` function.


* Node.js
* Python




```
const dataset = client.dataset(run.defaultDatasetId);
```






```
dataset = client.dataset(run['defaultDatasetId'])
```




Finally, we can download the items in the dataset by using the **list items** function, then log them to the console.


* Node.js
* Python




```
const { items } = await dataset.listItems();


console.log(items);
```






```
items = dataset.list_items().items


print(items)
```




The final code for running the Actor and fetching its dataset items looks like this:


* Node.js
* Python




```
// client.js
import { ApifyClient } from 'apify-client';


const client = new ApifyClient({
    token: 'YOUR_TOKEN',
});


const run = await client.actor('YOUR_USERNAME/adding-actor').call({
    num1: 4,
    num2: 2,
});


const dataset = client.dataset(run.defaultDatasetId);


const { items } = await dataset.listItems();


console.log(items);
```






```
# client.py
from apify_client import ApifyClient


client = ApifyClient(token='YOUR_TOKEN')


actor = client.actor('YOUR_USERNAME/adding-actor').call(run_input={
    'num1': 4,
    'num2': 2
})


dataset = client.dataset(run['defaultDatasetId'])


items = dataset.list_items().items


print(items)
```




## Updating an Actor


If you check the **Settings** tab within your **adding-actor**, you'll notice that the default timeout being set to the Actor is **360 seconds**. This is a bit overkill considering the fact that the Actor is only adding two numbers together - the run should never take more than 20 seconds (even this is a generous number). The default memory being allocated to the Actor is **256 MB**, which is reasonable for our purposes.


Let's change these two Actor settings via the Apify client using the https://docs.apify.com/api/client/js/reference/class/ActorClient#update function. This function will call the **update Actor** endpoint, which can take `defaultRunOptions` as an input property. You can find the shape of the `defaultRunOptions` in the https://docs.apify.com/api/v2/act-put.md. Perfect!


First, we'll create a pointer to our Actor, similar to before (except this time, we won't be using `.call()` at the end):


* Node.js
* Python




```
const actor = client.actor('YOUR_USERNAME/adding-actor');
```






```
actor = client.actor('YOUR_USERNAME/adding-actor')
```




Then, we'll call the `.update()` method on the `actor` variable we created and pass in our new **default run options**:


* Node.js
* Python




```
await actor.update({
    defaultRunOptions: {
        build: 'latest',
        memoryMbytes: 256,
        timeoutSecs: 20,
    },
});
```






```
actor.update(default_run_build='latest', default_run_memory_mbytes=256, default_run_timeout_secs=20)
```




After running the code, go back to the **Settings** page of **adding-actor**. If your default options now look like this, then it worked!:


![New run defaults](/assets/images/new-defaults-ba42f0ce8c11e3b3a26e55d07f2d77b5.jpg)


## Overview


You can do so much more with the Apify client than running Actors, updating Actors, and downloading dataset items. The purpose of this lesson was to get you comfortable using the client in your own projects, as it's the absolute best developer tool for integrating the Apify platform with an external system.


For a more in-depth understanding of the Apify API client, give these a quick lookover:


* https://docs.apify.com/api/client/js
* https://docs.apify.com/api/client/python


## Next up


Now that you're familiar and a bit more comfortable with the Apify platform, you're ready to start deploying your code to Apify! In the https://docs.apify.com/academy/deploying-your-code.md, you'll learn how to take any project written in any programming language and turn it into an Actor.


# Deploying your code to Apify


**In this course learn how to take an existing project of yours and deploy it to the Apify platform as an Actor.**


***


This section discusses how to use your newfound knowledge of the Apify platform and Actors from the https://docs.apify.com/academy/getting-started.md section to deploy your existing project's code to the Apify platform as an Actor. Any program running in a Docker container can become an Apify Actor.


Apify provides detailed guidance on how to deploy Node.js and Python programs as Actors, but apart from that you're not limited in what programming language you choose for your scraper.


![Supported languages](/assets/images/supported-languages-2b3aced02908c1def900dbace072201a.jpg)


Here are a few examples of Actors in other languages:


* https://apify.com/lukaskrivka/rust-actor-example
* https://apify.com/jirimoravcik/go-actor-example
* https://apify.com/jirimoravcik/julia-actor-example


## The "Actorization" workflow


Follow these four main steps to turn a piece of code into an Actor:


1. Handle https://docs.apify.com/academy/deploying-your-code/inputs-outputs.md.
2. Create an https://docs.apify.com/academy/deploying-your-code/input-schema.md *(optional)*.
3. Add a https://docs.apify.com/academy/deploying-your-code/docker-file.md.
4. https://docs.apify.com/academy/deploying-your-code/deploying.md to the Apify platform!


## Our example project


For this section, we'll be turning this example project into an Actor:


* JavaScript
* Python




```
// index.js
const addAllNumbers = (...nums) => nums.reduce((total, curr) => total + curr, 0);


console.log(addAllNumbers(1, 2, 3, 4)); // -> 10
```






```
# index.py
def add_all_numbers (nums):
    total = 0


    for num in nums:
        total += num


    return total


print(add_all_numbers([1, 2, 3, 4])) # -> 10
```




Language examples


For all lessons in this section, we'll have examples for both Node.js and Python so that you can follow along in either language.


## Next up


https://docs.apify.com/academy/deploying-your-code/inputs-outputs.md, we'll be learning how to accept input into our Actor as well as deliver output.


# Managing Actor inputs and outputs


**Learn to accept input into your Actor, process it, and return output. This concept applies to Actors in any language.**


***


Most of the time when you're creating a project, you are expecting some sort of input from which your software will run off. Oftentimes as well, you want to provide some sort of output once your software has completed running. Apify provides a convenient way to handle inputs and deliver outputs.


Understanding inputs and outputs is essential because they are read/written differently depending on where the Actor is running:


* If your Actor is running locally, the inputs/outputs are usually provided in the filesystem, and environment variables are injected either by you, the developer, or by the Apify CLI by running the project with the `apify run` command.


* While running in a Docker container on the platform, environment variables are automatically injected, and inputs & outputs are provided and modified using Apify's REST API.


## A bit about storage


You can read/write your inputs/outputs: to the https://docs.apify.com/platform/storage/key-value-store.md, or to the https://docs.apify.com/platform/storage/dataset.md. The key-value store can be used to store any sort of unorganized/unrelated data in any format, while the data pushed to a dataset typically resembles a table with columns (fields) and rows (items). Each Actor's run is allocated both a default dataset and a default key-value store.


When running locally, these storages are accessible through the **storage** folder within your project's root directory, while on the platform they are accessible via Apify's API.


## Accepting input


You can utilize multiple ways to accept input into your project. The option you go with depends on the language you have written your project in. If you are using Node.js for your repo's code, you can use the https://www.npmjs.com/package/apify package. Otherwise, you can use the useful environment variables automatically set up for you by Apify to write utility functions which read the Actor's input and return it.


### Accepting input with the Apify SDK


Since we're using Node.js, let's install the `apify` package by running the following command:




```
npm install apify
```




Now, let's import `Actor` from `apify` and use the `Actor.getInput()` function to grab our input.




```
// index.js
import { Actor } from 'apify';


// We must initialize and exit the Actor. The rest of our code
// goes in between these two.
await Actor.init();


const input = await Actor.getInput();
console.log(input);


await Actor.exit();
```




If we run this right now, we'll see **null** in our terminal - this is because we never provided any sort of test input, which should be provided in the default key-value store. The `Actor.getInput()` function has detected that there is no **storage** folder and generated one for us.


![Default key-value store filepath](/assets/images/filepath-6c643f3e6fc1e05a2c8e477557a9dd4e.jpg)


We'll now add an **INPUT.json** file within **storage/key\_value\_stores/default** to match what we're expecting in our code.




```
{
    "numbers": [5, 5, 5, 5]
}
```




Then we can add our example project code from earlier. It will grab the input and use it to generate a solution which is logged into the console.




```
// index.js
import { Actor } from 'apify';


await Actor.init();


const { numbers } = await Actor.getInput();


const addAllNumbers = (...nums) => nums.reduce((total, curr) => total + curr, 0);


const solution = addAllNumbers(...numbers);


console.log(solution);


await Actor.exit();
```




Cool! When we run `node index.js`, we see **20**.


### Accepting input without the Apify SDK


Alternatively, when writing in a language other than JavaScript, we can create our own `get_input()` function which utilizes the Apify API when the Actor is running on the platform. For this example, we are using the https://docs.apify.com/academy/getting-started/apify-client.md for Python to access the API.




```
# index.py
import json
from os import environ
from apify_client import ApifyClient


client = ApifyClient(token='YOUR_TOKEN')


# If being run on the platform, the "APIFY_IS_AT_HOME" environment variable
# will be "1". Otherwise, it will be undefined/None
def is_on_apify():
    return 'APIFY_IS_AT_HOME' in environ


# Get the input
def get_input():
    if not is_on_apify():
        with open('./apify_storage/key_value_stores/default/INPUT.json') as actor_input:
            return json.load(actor_input)


    kv_store = client.key_value_store(environ.get('APIFY_DEFAULT_KEY_VALUE_STORE_ID'))
    return kv_store.get_record('INPUT')['value']


def add_all_numbers(nums):
    total = 0


    for num in nums:
        total += num


    return total


actor_input = get_input()['numbers']
solution = add_all_numbers(actor_input)
print(solution)
```




> For a better understanding of the API endpoints for reading and modifying key-value stores, check the https://docs.apify.com/api/v2/storage-key-value-stores.md.


## Writing output


Similarly to reading input, you can write the Actor's output either by using the Apify SDK in Node.js or by manually writing a utility function to do so.


### Writing output with the Apify SDK


In the SDK, we can write to the dataset with the `Actor.pushData()` function. Let's go ahead and write the solution of the `addAllNumbers()` function to the dataset store using this function:




```
// index.js


// This is our example project code from earlier.
// We will use the Apify input as its input.
import { Actor } from 'apify';


await Actor.init();


const { numbers } = await Actor.getInput();


const addAllNumbers = (...nums) => nums.reduce((total, curr) => total + curr, 0);


const solution = addAllNumbers(...numbers);


// And save its output to the default dataset
await Actor.pushData({ solution });


await Actor.exit();
```




### Writing output without the Apify SDK


Just as with the custom `get_input()` utility function, you can write a custom `set_output()` function as well if you cannot use the Apify SDK.


Storage location


You can read and write your output anywhere; however, it is standard practice to use a folder named `storage`.




```
# index.py
import json
from os import environ
from apify_client import ApifyClient


client = ApifyClient(token='YOUR_TOKEN')


def is_on_apify():
    return 'APIFY_IS_AT_HOME' in environ


def get_input():
    if not is_on_apify():
        with open('./apify_storage/key_value_stores/default/INPUT.json') as actor_input:
            return json.load(actor_input)


    kv_store = client.key_value_store(environ.get('APIFY_DEFAULT_KEY_VALUE_STORE_ID'))
    return kv_store.get_record('INPUT')['value']


# Push the solution to the dataset
def set_output(data):
    if not is_on_apify():
        with open('./apify_storage/datasets/default/solution.json', 'w') as output:
            return output.write(json.dumps(data, indent=2))


    dataset = client.dataset(environ.get('APIFY_DEFAULT_DATASET_ID'))
    dataset.push_items([json.dumps(data, indent=4)])


def add_all_numbers(nums):
    total = 0


    for num in nums:
        total += num


    return total


actor_input = get_input()['numbers']
solution = add_all_numbers(actor_input)
set_output({'solution': solution})
```




## Testing locally


Since we've changed our code a lot from the way it originally was by wrapping it in the Apify SDK to accept inputs and return outputs, we most definitely should test it locally before worrying about pushing it to the Apify platform.


After running our script, there should be a single item in the default dataset that looks like this:




```
{
    "solution": 20
}
```




## Next up


That's it! We've now added all of the files and code necessary to convert our software into an Actor. In the https://docs.apify.com/academy/deploying-your-code/input-schema.md, we'll be learning how to generate a user interface for our Actor's input so that users don't have to provide the input in raw JSON format.


# How to write Actor input schema


**Learn how to generate a user interface on the platform for your Actor's input with a single file - the INPUT\_SCHEMA.json file.**


***


Though writing an https://docs.apify.com/platform/actors/development/actor-definition/input-schema.md for an Actor is not a required step, it's definitely an ideal one. The Apify platform will read the `INPUT_SCHEMA.json` file within the root of your project and generate a user interface for entering input into your Actor, which makes it significantly easier for non-developers (and even developers) to configure and understand the inputs your Actor can receive. Because of this, we'll be writing an input schema for our example Actor.


JSON requirement


Without an input schema, the users of our Actor will have to provide the input in JSON format, which can be problematic for those who are not familiar with JSON.


## Schema title & description


In the root of our project, we'll create a file named **INPUT\_SCHEMA.json** and start writing the first part of the schema.




```
{
    "title": "Adding Actor input",
    "description": "Add all values in list of numbers with an arbitrary length.",
    "type": "object",
    "schemaVersion": 1
}
```




The **title** and **description** describe what the input schema is for, and a bit about what the Actor itself does.


## Properties


In order to define all of the properties our Actor is expecting, we must include them within an object with a key of **properties**.




```
{
    "title": "Adding Actor input",
    "description": "Add all values in list of numbers with an arbitrary length.",
    "type": "object",
    "schemaVersion": 1,
    "properties": {
        "numbers": {
            "title": "Number list",
            "description": "The list of numbers to add up."
        }
    }
}
```




Each property's key corresponds to the name we're expecting within our code, while the **title** and **description** are what the user will see when configuring input on the platform.


## Property types & editor types


Within our new **numbers** property, there are two more fields we must specify. Firstly, we must let the platform know that we're expecting an array of numbers with the **type** field. Then, we should also instruct Apify on which UI component to render for this input property. In our case, we have an array of numbers, which means we should use the **json** editor type that we discovered in the https://docs.apify.com/platform/actors/development/actor-definition/input-schema/specification/v1.md#array of the input schema documentation. We could also use **stringList**, but then we'd have to parse out the numbers from the strings.




```
{
    "title": "Adding Actor input",
    "description": "Add all values in list of numbers with an arbitrary length.",
    "type": "object",
    "schemaVersion": 1,
    "properties": {
        "numbers": {
            "title": "Number list",
            "description": "The list of numbers to add up.",
            "type": "array",
            "editor": "json"
        }
    }
}
```




## Required fields


The great thing about building an input schema is that it will automatically validate your inputs based on their type, maximum value, minimum value, etc. Sometimes, you want to ensure that the user will always provide input for certain fields, as they are crucial to the Actor's run. This can be done by using the **required** field and passing in the names of the fields you'd like to require.




```
{
    "title": "Adding Actor input",
    "description": "Add all values in list of numbers with an arbitrary length.",
    "type": "object",
    "schemaVersion": 1,
    "properties": {
        "numbers": {
            "title": "Number list",
            "description": "The list of numbers to add up.",
            "type": "array",
            "editor": "json"
        }
    },
    "required": ["numbers"]
}
```




For our case, we've made the **numbers** field required, as it is crucial to our Actor's run.


## Final thoughts


Here is what the input schema we wrote will render on the platform:


![Rendered UI from input schema](/assets/images/rendered-ui-74b1f9f74dce9ba83249f733716a0745.png)


Later on, we'll be building more complex input schemas, as well as discussing how to write quality input schemas that allow the user to understand the Actor.


It's not expected to memorize all of the fields that properties can take or the different editor types available, which is why it's always good to reference the https://docs.apify.com/platform/actors/development/actor-definition/input-schema.md when writing a schema.


## Next up


In the https://docs.apify.com/platform/actors/development/actor-definition/dataset-schema.md, we'll learn how to generate an appealing Overview table to display our Actor's results in real time, so users can get immediate feedback about the data being extracted.


# Creating dataset schema


**Learn how to generate an appealing Overview table interface to preview your Actor results in real time on the Apify platform.**


***


The dataset schema generates an interface that enables users to instantly preview their Actor results in real time.


![Dataset Schema](/assets/images/output-schema-example-42bf91c1c1f39834fad5bbedf209acaa.png)


In this quick tutorial, you will learn how to set up an output tab for your own Actor.


## Implementation


Firstly, create a `.actor` folder in the root of your Actor's source code. Then, create a `actor.json` file in this folder, after which you'll have .actor/actor.json.


![.actor/actor.json](/assets/images/actor-json-example-7f3c312c187b9f6f86879594a769f35f.webp)


Next, copy-paste the following template code into your `actor.json` file.




```
{
    "actorSpecification": 1,
    "name": "___ENTER_ACTOR_NAME____",
    "title": "___ENTER_ACTOR_TITLE____",
    "version": "1.0.0",
    "storages": {
        "dataset": {
            "actorSpecification": 1,
            "views": {
                "overview": {
                    "title": "Overview",
                    "transformation": {
                        "fields": [
                            "___EXAMPLE_NUMERIC_FIELD___",
                            "___EXAMPLE_PICTURE_URL_FIELD___",
                            "___EXAMPLE_LINK_URL_FIELD___",
                            "___EXAMPLE_TEXT_FIELD___",
                            "___EXAMPLE_BOOLEAN_FIELD___"
                        ]
                    },
                    "display": {
                        "component": "table",
                        "properties": {
                            "___EXAMPLE_NUMERIC_FIELD___": {
                                "label": "ID",
                                "format": "number"
                            },
                            "___EXAMPLE_PICTURE_URL_FIELD___": {
                                "format": "image"
                            },
                            "___EXAMPLE_LINK_URL_FIELD___": {
                                "label": "Clickable link",
                                "format": "link"
                            }
                        }
                    }
                }
            }
        }
    }
}
```




To configure the dataset schema, replace the fields in the template with the relevant fields to your Actor.


For reference, you can use the https://github.com/PerVillalva/zappos-scraper-actor/blob/main/.actor/actor.json as an example of how the final implementation of the output tab should look in a live Actor.




```
{
    "actorSpecification": 1,
    "name": "zappos-scraper",
    "title": "Zappos Scraper",
    "description": "",
    "version": "1.0.0",
    "storages": {
        "dataset": {
            "actorSpecification": 1,
            "title": "Zappos.com Dataset",
            "description": "",
            "views": {
                "products": {
                    "title": "Overview",
                    "description": "It can take about one minute until the first results are available.",
                    "transformation": {
                        "fields": [
                            "imgUrl",
                            "brand",
                            "name",
                            "SKU",
                            "inStock",
                            "onSale",
                            "price",
                            "url"
                        ]
                    },
                    "display": {
                        "component": "table",
                        "properties": {
                            "imgUrl": {
                                "label": "Product image",
                                "format": "image"
                            },
                            "url": {
                                "label": "Link",
                                "format": "link"
                            },
                            "brand": {
                                "format": "text"
                            },
                            "name": {
                                "format": "text"
                            },
                            "SKU": {
                                "format": "text"
                            },
                            "inStock": {
                                "format": "boolean"
                            },
                            "onSale": {
                                "format": "boolean"
                            },
                            "price": {
                                "format": "text"
                            }
                        }
                    }
                }
            }
        }
    }
}
```




Note that the fields specified in the dataset schema should match the object keys of your resulting dataset.


Also, if your desired label has the same name as the defined object key, then you don't need to specify a label name. The schema will, by default, show a capitalized version of the key and even split camel case into separate words and capitalize all of them.


The matching object for the Zappos Scraper shown in the example above will look something like this:




```
const results = {
    url: request.loadedUrl,
    imgUrl: $('#stage button[data-media="image"] img[itemprop="image"]').attr('src'),
    brand: $('span[itemprop="brand"]').text().trim(),
    name: $('meta[itemprop="name"]').attr('content'),
    SKU: $('*[itemprop~="sku"]').text().trim(),
    inStock: !request.url.includes('oosRedirected=true'),
    onSale: !$('div[itemprop="offers"]').text().includes('OFF'),
    price: $('span[itemprop="price"]').text(),
};
```




## Final result


Great! Now that everything is set up, it's time to run the Actor and admire your Actor's brand new output tab.


Additional resources


Visit the https://docs.apify.com/platform/actors/development/actor-definition/dataset-schema.md for more detailed information about how to implement this feature.


A few seconds after running the Actor, you should see its results displayed in the `Overview` table.


![Output table overview](/assets/images/output-schema-final-example-0beffd41c710a5438a8fe1c4a72f0f07.webp)


## Next up


In the https://docs.apify.com/academy/deploying-your-code/docker-file.md, we'll learn about a very important file that is required for our project to run on the Apify platform - the Dockerfile.


# Creating Actor Dockerfile


**Learn to write a Dockerfile for your project so it can run in a Docker container on the Apify platform.**


***


The **Dockerfile** is a file which gives the Apify platform (or Docker, more specifically) instructions on how to create an environment for your code to run in. Every Actor must have a Dockerfile, as Actors run in Docker containers.


Local testing


Actors on the platform are always run in Docker containers, however, they can also be run in local Docker containers. This is not common practice though, as it requires more setup and a deeper understanding of Docker. For testing, it's best to run the Actor on the local OS (this requires you to have the underlying runtime installed, such as Node.js, Python, Rust, GO, etc).


## Base images


If your project doesn't already contain a Dockerfile, don't worry! Apify offers https://docs.apify.com/sdk/js/docs/guides/docker-images that are optimized for building and running Actors on the platform, which can be found on https://hub.docker.com/u/apify. When using a language for which Apify doesn't provide a base image, https://hub.docker.com/ provides a ton of free Docker images for most use-cases, upon which you can create your own images.


DockerHub images


You can see all of Apify's Docker images https://hub.docker.com/u/apify.


At the base level, each Docker image contains a base operating system and usually also a programming language runtime (such as Node.js or Python). You can also find images with preinstalled libraries or install them yourself during the build step.


Once you find the base image you need, you can add it as the initial `FROM` statement:




```
FROM apify/actor-node:16
```




VSCode extension


For syntax highlighting in your Dockerfiles, download the https://code.visualstudio.com/docs/containers/overview#_installation.


## Writing the file


The rest of the Dockerfile is about copying the source code from the local filesystem into the container's filesystem, installing libraries, and setting the `RUN` command (which falls back to the parent image).


Custom images


If you are not using a base image from Apify, then you should specify how to launch the source code of your Actor with the `CMD` instruction.


Here's the Dockerfile for our Node.js example project's Actor:


* Node.js Dockerfile
* Python Dockerfile




```
FROM apify/actor-node:16


# Second, copy just package.json and package-lock.json since they are the only files
# that affect npm install in the next step
COPY package*.json ./


# Install npm packages, skip optional and development dependencies to keep the
# image small. Avoid logging too much and print the dependency tree for debugging
RUN npm --quiet set progress=false \
 && npm install --only=prod --no-optional \
 && echo "Installed npm packages:" \
 && (npm list --all || true) \
 && echo "Node.js version:" \
 && node --version \
 && echo "npm version:" \
 && npm --version


# Next, copy the remaining files and directories with the source code.
# Since we do this after npm install, quick build will be really fast
# for simple source file changes.
COPY . ./
```






```
# First, specify the base Docker image.
# You can also use any other image from Docker Hub.
FROM apify/actor-python:3.9


# Second, copy just requirements.txt into the Actor image,
# since it should be the only file that affects "pip install" in the next step,
# in order to speed up the build
COPY requirements.txt ./


# Install the packages specified in requirements.txt,
# Print the installed Python version, pip version
# and all installed packages with their versions for debugging
RUN echo "Python version:" \
 && python --version \
 && echo "Pip version:" \
 && pip --version \
 && echo "Installing dependencies from requirements.txt:" \
 && pip install -r requirements.txt \
 && echo "All installed Python packages:" \
 && pip freeze


# Next, copy the remaining files and directories with the source code.
# Since we do this after installing the dependencies, quick build will be really fast
# for most source file changes.
COPY . ./


# Specify how to launch the source code of your Actor.
# By default, the main.py file is run
CMD python3 main.py
```




## Examples


The examples above show how to deploy Actors written in Node.js or Python, but you can use any language. As an inspiration, here are a few examples for other languages: Go, Rust, Julia.


* GO Actor Dockerfile
* Rust Actor Dockerfile
* Julia Actor Dockerfile




```
FROM golang:1.17.1-alpine


WORKDIR /app
COPY . .


RUN go mod download


RUN go build -o /example-actor
CMD ["/example-actor"]
```






```
# Image with prebuilt Rust. We use the newest 1.* version
# https://hub.docker.com/_/rust
FROM rust:1


# We copy only package setup so we cache building all dependencies
COPY Cargo* ./


# We need to have dummy main.rs file to be able to build
RUN mkdir src && echo "fn main() {}" > src/main.rs


# Build dependencies only
# Since we do this before copying  the rest of the files,
# the dependencies will be cached by Docker, allowing fast
# build times for new code changes
RUN cargo build --release


# Delete dummy main.rs
RUN rm -rf src


# Copy rest of the files
COPY . ./


# Build the source files
RUN cargo build --release


CMD ["./target/release/actor-example"]
```






```
FROM julia:1.7.1-alpine


WORKDIR /app
COPY . .


RUN julia install.jl


CMD ["julia", "main.jl"]
```




## Next up


In the https://docs.apify.com/academy/deploying-your-code/deploying.md, we'll push our code directly to the Apify platform, or create and integrate a new Actor on the Apify platform with our project's GitHub repository.


# Publishing your Actor


**Push local code to the platform, or create an Actor and integrate it with a Git repository for automatic rebuilds.**


***


Once you've **actorified** your code, there are two ways to deploy it to the Apify platform. You can either push the code directly from your local machine onto the platform, or you can create a blank Actor in the web interface, and then integrate its source code with a GitHub repository.


## With a Git repository


Before we deploy our project onto the Apify platform, let's ensure that we've pushed the changes we made in the last 3 lessons into our remote GitHub repository.


Git integration


The benefit of using this method is that any time you push to the Git repository, the code on the platform is also updated and the Actor is automatically rebuilt. Also, you don't have to use a GitHub repository - you can use GitLab or any other service you'd like.


### Creating the Actor


Before anything can be integrated, we've gotta create a new Actor. Let's head over to our https://console.apify.com?asrc=developers_portal, navigate to the **Development** subsection and click on the **Develop new** button, then select the **Empty** template.


![Create new button](/assets/images/develop-new-actor-a499c8a2618fec73c828ddb4dcbb75b4.png)


### Changing source code location


In the **Source** tab on the new Actor's page, we'll click the dropdown menu under **Source code** and select **Git repository**. By default, this is set to **Web IDE**.


![Select source code location](/assets/images/select-source-location-8b84116417145746c275463c49e24baa.png)


Now we'll paste the link to our GitHub repository into the **Git URL** text field and click **Save**.


### Adding the webhook to the repository


The final step is to click on **API** in the top right corner of our Actor's page:


![API button](/assets/images/api-button-4384acadb7883bbad6c7f363c0c1a37c.jpg)


And scroll through all of the links until we find the **Build Actor** API endpoint. Now we'll copy this endpoint's URL, head back over to our GitHub repository and navigate to **Settings > Webhooks > Add webhook**. The final thing to do is to paste the URL and save the webhook.


![Adding a webhook to your GitHub repository](/assets/images/ci-github-integration-2ee82ac772eb3280155b7027a4259528.png)


That's it! The Actor should now pull its source code from the repository and automatically build.


## Without a GitHub repository (using the Apify CLI)


CLI prerequisite


If you don't yet have the Apify CLI, learn how to install it and log in by following along with https://docs.apify.com/academy/tools/apify-cli.md about it.


If you're logged in to the Apify CLI, the `apify push` command can be used to push the code straight onto the Apify platform from your local machine (no GitHub repository required), where it will automatically be built for you. Prior to running this command, make sure that you have an **.actor/actor.json** file at the root of the project. If you don't already have one, you can use `apify init .` to automatically generate one for you.


One important thing to note is that you can use a `.gitignore` file to exclude files from being pushed. When you use `apify push` without a `.gitignore`, the full folder contents will be pushed, meaning that even the `storage` and `node_modules` will be pushed. These files are unnecessary to push, as they are both generated on the platform.


Development only


The `apify push` command should only really be used for quickly pushing and testing Actors on the platform during development. If you are ready to make your Actor public, use a Git repository instead, as you will reap the benefits of using Git and others will be able to contribute to the project.


## Deployed!


Great! Once you've pushed your Actor to the platform, you will find it listed under the **Actors** tab. When using the `apify push` command, you will have access to the multifile editor. For details about using the multifile editor, refer to https://docs.apify.com/academy/getting-started/creating-actors.md#web-ide.


![Deployed Actor on the Apify platform](/assets/images/actor-page-e3c2002c5e585e896614af6e3e38838e.jpg)


The next step is to test your Actor and experiment with the vast amount of features the platform has to offer.


## Wrap up


That's it! In this short section, you've learned how to take your code written in any programming language and turn it into a usable Actor that can run on the Apify platform! The next step is to start looking into the https://docs.apify.com/platform/actors/publishing.md program, which allows you to monetize your work.


# Expert scraping with Apify


**After learning the basics of Actors and Apify, learn to develop pro-level scrapers on the Apify platform with this advanced course.**


***


This course will teach you the nitty gritty of what it takes to build pro-level scrapers with Apify. We recommend that you've at least looked through all of the other courses in the academy prior to taking this one.


## Preparations


Before developing a pro-level Apify scraper, there are some important things you should have at least a bit of knowledge about (knowing the basics of each is enough to continue through this section), as well as some things that you should have installed on your system.


> If you've already gone through the https://docs.apify.com/academy/scraping-basics-javascript.md and the first courses of the https://docs.apify.com/academy/apify-platform.md, you will be more than well equipped to continue on with the lessons in this course.


### Crawlee, Apify SDK, and the Apify CLI


If you're feeling ambitious, you don't need to have any prior experience with Crawlee to get started with this course; however, at least 5–10 minutes of exposure is recommended. If you haven't yet tried out Crawlee, you can refer to the https://docs.apify.com/academy/scraping-basics-javascript/framework.md lesson of the **Web scraping basics for JavaScript devs** course. To familiarize yourself with the Apify SDK, you can refer to the https://docs.apify.com/academy/apify-platform.md category.


The Apify CLI will play a core role in the running and testing of the Actor you will build, so if you haven't gotten it installed already, please refer to https://docs.apify.com/academy/tools/apify-cli.md.


### Git


In one of the later lessons, we'll be learning how to integrate our Actor on the Apify platform with a GitHub repository. For this, you'll need to understand at least the basics of https://git-scm.com/docs. Here's a https://product.hubspot.com/blog/git-and-github-tutorial-for-beginners to help you get started with Git.


### Docker


Docker is a massive topic on its own, but don't be worried! We only expect you to know and understand the very basics of it, which can be learned about in https://docs.docker.com/guides/docker-overview/ (10 minute read).


### The basics of Actors


Part of this course will be learning more in-depth about Actors; however, some basic knowledge is already assumed. If you haven't yet gone through the https://docs.apify.com/academy/getting-started/actors.md lesson of the **Apify platform** course, it's highly recommended to at least give it a glance before moving forward.


## First up


https://docs.apify.com/academy/expert-scraping-with-apify/actors-webhooks.md, we'll be learning in-depth about integrating Actors with each other using webhooks.


> Each lesson will have a short *(and optional)* quiz that you can take at home to test your skills and knowledge related to the lesson's content. Some questions have straight factual answers, but some others can have varying opinionated answers.


# Webhooks & advanced Actor overview


**Learn more advanced details about Actors, how they work, and the default configurations they can take. Also, learn how to integrate your Actor with webhooks.**


Updates coming


This lesson is subject to change because it currently relies on code from our archived **Web scraping basics for JavaScript devs** course. For now you can still access the archived course, but we plan to completely retire it in a few months. This lesson will be updated to remove the dependency.


***


Thus far, you've run Actors on the platform and written an Actor of your own, which you published to the platform yourself using the Apify CLI; therefore, it's fair to say that you are becoming more familiar and comfortable with the concept of **Actors**. Within this lesson, we'll take a more in-depth look at Actors and what they can do.


## Advanced Actor overview


In this course, we'll be working out of the Amazon scraper project from the **Web scraping basics for JavaScript devs** course. If you haven't already built that project, you can do it in https://docs.apify.com/academy/scraping-basics-javascript/legacy/challenge.md. We've made a few small modifications to the project with the Apify SDK, but 99% of the code is still the same.


Take another look at the files within your Amazon scraper project. You'll notice that there is a **Dockerfile**. Every single Actor has a Dockerfile (the Actor's **Image**) which tells Docker how to spin up a container on the Apify platform which can successfully run the Actor's code. "Apify Actors" is a serverless platform that runs multiple Docker containers. For a deeper understanding of Actor Dockerfiles, refer to the https://docs.apify.com/sdk/js/docs/guides/docker-images#example-dockerfile.


## Webhooks


Webhooks are a powerful tool that can be used for just about anything. You can set up actions to be taken when an Actor reaches a certain state (started, failed, succeeded, etc). These actions usually take the form of an API call (generally a POST request).


## Learning 🧠


Prior to moving forward, please read over these resources:


* Read about https://docs.apify.com/platform/actors/running.md.
* Learn about https://docs.apify.com/platform/integrations/webhooks.md, which we will implement in the next lesson.
* Learn https://docs.apify.com/academy/api/run-actor-and-retrieve-data-via-api.md using Apify's REST API.


## Knowledge check 📝


1. How do you allocate more CPU for an Actor's run?
2. Within itself, can you get the exact time that an Actor was started?
3. What are the types of default storages connected to an Actor's run?
4. Can you change the allocated memory of an Actor while it's running?
5. How can you run an Actor with Puppeteer on the Apify platform with headless mode set to `false`?


## Our task


In this task, we'll be building on top of what we already created in the https://docs.apify.com/academy/scraping-basics-javascript/legacy/challenge.md course's final challenge, so keep those files safe!


Once our Amazon Actor has completed its run, we will, rather than sending an email to ourselves, call an Actor through a webhook. The Actor called will be a new Actor that we will create together, which will take the dataset ID as input, then subsequently filter through all of the results and return only the cheapest one for each product. All of the results of the Actor will be pushed to its default dataset.


https://docs.apify.com/academy/expert-scraping-with-apify/solutions/integrating-webhooks.md


## Next up


This course's https://docs.apify.com/academy/expert-scraping-with-apify/managing-source-code.md is brief, but discusses a very important topic: managing your code and storing it in a safe place.


# Managing source code


**Learn how to manage your Actor's source code more efficiently by integrating it with a GitHub repository. This is standard on the Apify platform.**


***


In this brief lesson, we'll discuss how to better manage an Actor's source code. Up 'til now, you've been developing your scripts locally, and then pushing the code directly to the Actor on the Apify platform; however, there is a much more optimal (and standard) way.


## Learning 🧠


Thus far, every time we've updated our code on the Apify platform, we've used the `apify push` CLI command; however, this can be problematic for a few reasons - mainly because, if someone else wants to make a change to/maintain your code, they don't have access to it, as it is on your local machine.


If you're not yet familiar with Git, please get familiar with it through the https://git-scm.com/docs, then take a quick moment to read about https://docs.apify.com/platform/integrations/github.md in the Apify docs.


Also, try to explore the **Multifile editor** in one of the Actors you developed in the previous lessons before moving forward.


## Knowledge check 📝


1. Do you have to rebuild an Actor each time the source code is changed?
2. In Git, what is the difference between **pushing** changes and making a **pull request**?
3. Based on your knowledge and experience, is the `apify push` command worth using (in your opinion)?


https://docs.apify.com/academy/expert-scraping-with-apify/solutions/managing-source.md


## Our task


First, we must initialize a GitHub repository (you can use Gitlab if you like, but this lesson's examples will be using GitHub). Then, after pushing our main Amazon Actor's code to the repo, we must switch its source code to use the content of the GitHub repository instead.


## Integrating GitHub source code


First, let's create a repository. This can be done https://kbroman.org/github_tutorial/pages/init.html, but in this lesson, we'll do it by creating the remote repository on GitHub's website:


![Create a new GitHub repo](/assets/images/github-new-repo-1e45ed3d75fdb3672b6253b016e1186d.png)


Then, we'll run the commands it tells us in our terminal (while within the **demo-actor** directory) to initialize the repository locally, and then push all of the files to the remote one.


After you've created your repo, navigate on the Apify platform to the Actor we called **demo-actor**. In the **Source** tab, click the dropdown menu under **Source code** and select **Git repository**. By default, this is set to **Web IDE**, which is what we've been using so far.


![Select source code location](/assets/images/select-source-location-8b84116417145746c275463c49e24baa.png)


Then, go ahead and paste the link to your repository into the **Git URL** text field and click **Save**.


The final step is to click on **API** in the top right corner of your Actor's page:


![API button](/assets/images/api-button-4384acadb7883bbad6c7f363c0c1a37c.jpg)


And scroll through all of the links until you find the **Build Actor** API endpoint. Copy this endpoint's URL, then head back over to your GitHub repository and navigate to **Settings > Webhooks > Add webhook**. The final thing to do is to paste the URL and save the webhook.


![Adding a webhook to your GitHub repo](/assets/images/ci-github-integration-2ee82ac772eb3280155b7027a4259528.png)


And you're done! 🎉


## Quick chat about code management


This was a bit of overhead, but the good news is that you don't ever have to configure this stuff again for this Actor. Now, every time the content of your **main**/**master** branch changes, the Actor on the Apify platform will rebuild based on the newest code.


Think of it as combining two steps into one! Normally, you'd have to do a `git push` from your terminal in order to get the newest code onto GitHub, then run `apify push` to push it to the platform.


It's also important to know that GitHub/Gitlab repository integration is standard practice. As projects grow and the number of contributors and maintainers increases, it only makes sense to have a GitHub repository integrated with the project's Actor. For the remainder of this course, all Actors created will be integrated with a GitHub repository.


## Next up


https://docs.apify.com/academy/expert-scraping-with-apify/tasks-and-storage.md, you'll learn about the different ways to store scraped data, as well as how to utilize a cool feature to run pre-configured Actors.


# Tasks & storage


**Understand how to save the configurations for Actors with Actor tasks. Also, learn about storage and the different types Apify offers.**


***


Both of these are very different things; however, they are also tied together in many ways. **Tasks** run Actors, Actors return data, and data is stored in different types of **Storages**.


## Tasks


Tasks are a very useful feature which allow us to save pre-configured inputs for Actors. This means that rather than configuring the Actor every time, or rather than having to save screenshots of various different Actor configurations, you can store the configurations right in your Apify account instead, and run the Actor at will with them.


## Storage


Storage allows us to save persistent data for further processing. As you'll learn, there are two main storage options on the Apify platform, as well as two main storage types (**named** and **unnamed**) with one big difference between them.


## Learning 🧠


* Check out https://docs.apify.com/platform/actors/running/tasks.md.
* Read about the https://docs.apify.com/platform/storage/dataset.md on the Apify platform.
* Understand the https://docs.apify.com/platform/storage/usage.md#named-and-unnamed-storages.
* Learn about the https://docs.apify.com/sdk/js/reference/class/Dataset and https://docs.apify.com/sdk/js/reference/class/KeyValueStore objects in the Apify SDK.


## Knowledge check 📝


1. What is the relationship between Actors and tasks?
2. What are the differences between default (unnamed) and named storage? Which one would you use for everyday usage?
3. What is data retention, and how does it work for all types of storages (default and named)?


https://docs.apify.com/academy/expert-scraping-with-apify/solutions/using-storage-creating-tasks.md


## Next up


The https://docs.apify.com/academy/expert-scraping-with-apify/apify-api-and-client.md is very exciting, as it will unlock the ability to seamlessly integrate your Apify Actors into your own external projects and applications with the Apify API.


# Tasks & storage


**Understand how to save the configurations for Actors with Actor tasks. Also, learn about storage and the different types Apify offers.**


***


Both of these are very different things; however, they are also tied together in many ways. **Tasks** run Actors, Actors return data, and data is stored in different types of **Storages**.


## Tasks


Tasks are a very useful feature which allow us to save pre-configured inputs for Actors. This means that rather than configuring the Actor every time, or rather than having to save screenshots of various different Actor configurations, you can store the configurations right in your Apify account instead, and run the Actor at will with them.


## Storage


Storage allows us to save persistent data for further processing. As you'll learn, there are two main storage options on the Apify platform, as well as two main storage types (**named** and **unnamed**) with one big difference between them.


## Learning 🧠


* Check out https://docs.apify.com/platform/actors/running/tasks.md.
* Read about the https://docs.apify.com/platform/storage/dataset.md on the Apify platform.
* Understand the https://docs.apify.com/platform/storage/usage.md#named-and-unnamed-storages.
* Learn about the https://docs.apify.com/sdk/js/reference/class/Dataset and https://docs.apify.com/sdk/js/reference/class/KeyValueStore objects in the Apify SDK.


## Knowledge check 📝


1. What is the relationship between Actors and tasks?
2. What are the differences between default (unnamed) and named storage? Which one would you use for everyday usage?
3. What is data retention, and how does it work for all types of storages (default and named)?


https://docs.apify.com/academy/expert-scraping-with-apify/solutions/using-storage-creating-tasks.md


## Next up


The https://docs.apify.com/academy/expert-scraping-with-apify/apify-api-and-client.md is very exciting, as it will unlock the ability to seamlessly integrate your Apify Actors into your own external projects and applications with the Apify API.


# Migrations & maintaining state


**Learn about what Actor migrations are and how to handle them properly so that the state is not lost and runs can safely be resurrected.**


***


We already know that Actors are Docker containers that can be run on any server. This means that they can be allocated anywhere there is space available, making them very efficient. Unfortunately, there is one big caveat: Actors move - a lot. When an Actor moves, it is called a **migration**.


On migration, the process inside of an Actor is completely restarted and everything in its memory is lost, meaning that any values stored within variables or classes are lost.


When a migration happens, you want to do a so-called "state transition", which means saving any data you care about so the Actor can continue right where it left off before the migration.


## Learning 🧠


Read this https://docs.apify.com/platform/actors/development/builds-and-runs/state-persistence.md on migrations and dealing with state transitions.


Before moving forward, read about Actor https://docs.apify.com/sdk/js/docs/upgrading/upgrading-to-v3#events and how to listen for them.


## Knowledge check 📝


1. Actors have an option in the **Settings** tab to **Restart on error**. Would you use this feature for regular Actors? When would you use this feature?
2. Migrations happen randomly, but by https://docs.apify.com/platform/actors/running/runs-and-builds.md#aborting-runs, you can simulate a similar situation. Try this out on the platform and observe what happens. What changes occur, and what remains the same for the restarted Actor's run?
3. Why don't you (usually) need to add any special migration handling code for a standard crawling/scraping Actor? Are there any features in the Crawlee/Apify SDK that handle this under the hood?
4. How can you intercept the migration event? How much time do you have after this event happens and before the Actor migrates?
5. When would you persist data to the default key-value store instead of to a named key-value store?


## Our task


Once again returning to our Amazon **demo-actor**, let's say that we need to store an object in memory (as a variable) containing all of the scraped ASINs as keys and the number of offers scraped from each ASIN as values. The object should follow this format:




```
{
    "B079ZJ1BPR": 3,
    "B07D4R4258": 21
}
```




Every 10 seconds, we should log the most up-to-date version of this object to the console. Additionally, the object should be able to solve Actor migrations, which means that even if the Actor were to migrate, its data would not be lost upon resurrection.


https://docs.apify.com/academy/expert-scraping-with-apify/solutions/handling-migrations.md


## Next up


You might have already noticed that we've been using the **RESIDENTIAL** proxy group in the `proxyConfiguration` within our Amazon scraping Actor. But what does that mean? Learn why we've used this group, about proxies, and about avoiding anti-scraping measures in the https://docs.apify.com/academy/expert-scraping-with-apify/bypassing-anti-scraping.md.


# Bypassing anti-scraping methods


**Learn about bypassing anti-scraping methods using proxies and proxy/session rotation together with Crawlee and the Apify SDK.**


***


Effectively bypassing anti-scraping software is one of the most crucial, but also one of the most difficult skills to master. The different types of https://docs.apify.com/academy/anti-scraping.md can vary a lot on the web. Some websites aren't even protected at all, some require only moderate IP rotation, and some cannot even be scraped without using advanced techniques and workarounds. Additionally, because the web is evolving, anti-scraping techniques are also evolving and becoming more advanced.


It is generally quite difficult to recognize the anti-scraping protections a page may have when first inspecting it, so it is important to thoroughly investigate a site prior to writing any lines of code, as anti-scraping measures can significantly change your approach as well as complicate the development process of an Actor. As your skills expand, you will be able to spot anti-scraping measures quicker, and better evaluate the complexity of a new project.


You might have already noticed that we've been using the **RESIDENTIAL** proxy group in the `proxyConfiguration` within our Amazon scraping Actor. But what does that mean? This is a proxy group from https://apify.com/proxy which has been preventing us from being blocked by Amazon this entire time. We'll be learning more about proxies and Apify Proxy in this lesson.


## Learning 🧠


* Skim https://apify.com/proxy for a general idea of Apify Proxy.
* Give the https://docs.apify.com/platform/proxy.md a solid readover (feel free to skip most of the examples).
* Check out the https://docs.apify.com/academy/anti-scraping.md.
* Gain a solid understanding of the https://crawlee.dev/api/core/class/SessionPool.
* Look at a few Actors on the https://apify.com/store. How are they utilizing proxies?


## Knowledge check 📝


1. What are the different types of proxies that Apify proxy offers? What are the main differences between them?
2. Which proxy groups do users get on the free plan? Can they access the proxy from their computer?
3. How can you prevent an error from occurring if one of the proxy groups that a user has is removed? What are the best practices for these scenarios?
4. Does it make sense to rotate proxies when you are logged into a website?
5. Construct a proxy URL that will select proxies **only from the US**.
6. What do you need to do to rotate a proxy (one proxy usually has one IP)? How does this differ for CheerioCrawler and PuppeteerCrawler?
7. Name a few different ways how a website can prevent you from scraping it.


## Our task


This time, we're going to build a trivial proxy-session manager for our Amazon scraping Actor. A session should be used a maximum of 5 times before being rotated; however, if a request fails, the IP should be rotated immediately.


Additionally, the proxies used by our scraper should now only be from the US.


https://docs.apify.com/academy/expert-scraping-with-apify/solutions/rotating-proxies.md


## Next up


Up https://docs.apify.com/academy/expert-scraping-with-apify/saving-useful-stats.md, we'll be learning about how to save useful stats about our run, which becomes more and more useful as a project scales.


# Saving useful run statistics


**Understand how to save statistics about an Actor's run, what types of statistics you can save, and why you might want to save them for a large-scale scraper.**


***


Using Crawlee and the Apify SDK, we are now able to collect and format data coming directly from websites and save it into a Key-Value store or Dataset. This is great, but sometimes, we want to store some extra data about the run itself, or about each request. We might want to store some extra general run information separately from our results or potentially include statistics about each request within its corresponding dataset item.


The types of values that are saved are totally up to you, but the most common are error scores, number of total saved items, number of request retries, number of CAPTCHAs hit, etc. Storing these values is not always necessary, but can be valuable when debugging and maintaining an Actor. As your projects scale, this will become more and more useful and important.


## Learning 🧠


Before moving on, give these valuable resources a quick lookover:


* Refamiliarize with the various available data on the https://crawlee.dev/api/core/class/Request.
* Learn about the https://crawlee.dev/api/browser-crawler/interface/BrowserCrawlerOptions#failedRequestHandler.
* Understand how to use the https://crawlee.dev/api/browser-crawler/interface/BrowserCrawlerOptions#errorHandler function to handle request failures.
* Ensure you are comfortable using https://docs.apify.com/sdk/js/docs/guides/result-storage#key-value-store and https://docs.apify.com/sdk/js/docs/guides/result-storage#dataset, and understand the differences between the two storage types.


## Knowledge check 📝


1. Why might you want to store statistics about an Actor's run (or a specific request)?
2. In our Amazon scraper, we are trying to store the number of retries of a request once its data is pushed to the dataset. Where would you get this information? Where would you store it?
3. What is the difference between the `failedRequestHandler` and `errorHandler`?


## Our task


In our Amazon Actor, each dataset result must now have the following extra keys:




```
{
    "dateHandled": "date-here", // the date + time at which the request was handled
    "numberOfRetries": 4, // the number of retries of the request before running successfully
    "currentPendingRequests": 24 // the current number of requests left pending in the request queue
}
```




Also, an object including these values should be persisted during the run in th Key-Value store and logged to the console every 10 seconds:




```
{
    "errors": { // all of the errors for every request path
        "some-site.com/products/123": [
            "error1",
            "error2"
        ]
    },
    "totalSaved": 43 // total number of saved items throughout the entire run
}
```




https://docs.apify.com/academy/expert-scraping-with-apify/solutions/saving-stats.md


## Wrap up


Wow, you've learned a whole lot in this course, so give yourself the pat on the back that you deserve! If you were able to follow along with this course, that means that you're officially an **Apify pro**, and that you're equipped with all of the knowledge and tools you need to build awesome scalable web-scrapers either for your own personal projects or for the Apify platform.


Congratulations! 🎉


# Solutions


**View all of the solutions for all of the activities and tasks of this course. Please try to complete each task on your own before reading the solution!**


***


The final section of each lesson in this course will be a task which you as the course-taker are expected to complete before moving on to the next lesson. Each task's completion and understanding plays an important role in the ability to continue through the course.


If you ever get stuck, or if you feel like your solution could be more optimal, you can always refer to the **Solutions** section of the course. Each solution will have all of the code and explanations needed to understand it.


**Please** try to do each task **on your own** prior to checking out the solution!


# Integrating webhooks


**Learn how to integrate webhooks into your Actors. Webhooks are a super powerful tool, and can be used to do almost anything!**


Updates coming


This lesson is subject to change because it currently relies on code from our archived **Web scraping basics for JavaScript devs** course. For now you can still access the archived course, but we plan to completely retire it in a few months. This lesson will be updated to remove the dependency.


***


In this lesson we'll be writing a new Actor and integrating it with our beloved Amazon scraping Actor. First, we'll navigate to the same directory where our **demo-actor** folder lives, and run `apify create filter-actor` *(once again, you can name the Actor whatever you want, but for this lesson, we'll be calling the new Actor **filter-actor**)*. When prompted about the programming language, select **JavaScript**:




```
$ apify create filter-actor
? Choose the programming language of your new Actor:
❯ JavaScript
  TypeScript
  Python
```




Then use the arrow down key to select **Empty JavaScript Project**:




```
$ apify create filter-actor
✔ Choose the programming language of your new Actor: JavaScript
? Choose a template for your new Actor. Detailed information about the template will be shown in the next step.
  Crawlee + Playwright + Chrome
  Crawlee + Playwright + Camoufox
  Bootstrap CheerioCrawler
  Cypress
❯ Empty JavaScript Project
  Standby JavaScript Project
  ...
```




As a last step, confirm the choices by **Install template** and wait until our new Actor is ready.


## Building the new Actor


First of all, we should clear out any of the boilerplate code within **main.js** to get a clean slate:




```
// main.js
import { Actor } from 'apify';


await Actor.init();


// ...


await Actor.exit();
```




We'll be passing the ID of the Amazon Actor's default dataset along to the new Actor, so we can expect that as an input:




```
const { datasetId } = await Actor.getInput();
const dataset = await Actor.openDataset(datasetId);
// ...
```




Accessing Cloud Datasets Locally


You will need to use `forceCloud` option - `Actor.openDataset(<name/id>, { forceCloud: true });` - to open dataset from platform storage while running Actor locally.


Next, we'll grab hold of the dataset's items with the `dataset.getData()` function:




```
const { items } = await dataset.getData();
```




While several methods can achieve the goal output of this Actor, using the https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/reduce is the most concise approach




```
const filtered = items.reduce((acc, curr) => {
    // Grab the price of the item matching our current
    // item's ASIN in the map. If it doesn't exist, set
    // "prevPrice" to null
    const prevPrice = acc?.[curr.asin] ? +acc[curr.asin].offer.slice(1) : null;


    // Grab the price of our current offer
    const price = +curr.offer.slice(1);


    // If the item doesn't yet exist in the map, add it.
    // Or, if the current offer's price is less than the
    // saved one, replace the saved one
    if (!acc[curr.asin] || prevPrice > price) acc[curr.asin] = curr;


    // Return the map
    return acc;
}, {});
```




The results should be an array, so we can take the map we just created and push an array of its values to the Actor's default dataset:




```
await Actor.pushData(Object.values(filtered));
```




Our final code looks like this:




```
import { Actor } from 'apify';


await Actor.init();


const { datasetId } = await Actor.getInput();
const dataset = await Actor.openDataset(datasetId);


const { items } = await dataset.getData();


const filtered = items.reduce((acc, curr) => {
    const prevPrice = acc?.[curr.asin] ? +acc[curr.asin].offer.slice(1) : null;
    const price = +curr.offer.slice(1);


    if (!acc[curr.asin] || prevPrice > price) acc[curr.asin] = curr;


    return acc;
}, {});


await Actor.pushData(Object.values(filtered));


await Actor.exit();
```




Cool! But **wait**, don't forget to configure the **INPUT\_SCHEMA.json** file as well! It's not necessary to do this step, as we'll be calling the Actor through Apify's API within a webhook, but it's still good to get into the habit of writing quality input schemas that describe the input values your Actors are expecting.




```
{
    "title": "Amazon Filter Actor",
    "type": "object",
    "schemaVersion": 1,
    "properties": {
        "datasetId": {
            "title": "Dataset ID",
            "type": "string",
            "description": "Enter the ID of the dataset.",
            "editor": "textfield"
        }
    },
    "required": ["datasetId"]
}
```




Now we're done, and we can push it up to the Apify platform with the `apify push` command:




```
$ apify push
Info: Created Actor with name filter-actor on Apify.
Info: Deploying Actor 'filter-actor' to Apify.
Run: Updated version 0.0 for Actor filter-actor.
Run: Building Actor filter-actor
(timestamp) ACTOR: Extracting Actor documentation from README.md
(timestamp) ACTOR: Building Docker image.
...
(timestamp) ACTOR: Pushing Docker image to repository.
(timestamp) ACTOR: Build finished.
Actor build detail https://console.apify.com/actors/Yk1bieximsduYDydP#/builds/0.0.1
Actor detail https://console.apify.com/actors/Yk1bieximsduYDydP
Success: Actor was deployed to Apify cloud and built there.
```




## Setting up the webhook


We'll use the https://docs.apify.com/academy/api/run-actor-and-retrieve-data-via-api.md to set up the webhook. To compose the HTTP request, we'll need either the ID of our Actor or its technical name. Let's take a second look at the end of the output of the `apify push` command:




```
...
Actor build detail https://console.apify.com/actors/Yk1bieximsduYDydP#/builds/0.0.1
Actor detail https://console.apify.com/actors/Yk1bieximsduYDydP
Success: Actor was deployed to Apify cloud and built there.
```




The URLs tell us that our Actor's ID is `Yk1bieximsduYDydP`. With this `actorId`, and our `token`, which is retrievable through **Settings > Integrations** on the Apify Console, we can construct a link which will call the Actor:




```
https://api.apify.com/v2/acts/Yk1bieximsduYDydP/runs?token=YOUR_TOKEN_HERE
```




We can also use our username and the name of the Actor like this:




```
https://api.apify.com/v2/acts/USERNAME~filter-actor/runs?token=YOUR_TOKEN_HERE
```




Whichever one you choose is totally up to your preference.


Next, within the Amazon scraping Actor, we will click the **Integrations** tab and choose **Webhook**, then fill out the details to look like this:


![Configuring a webhook](/assets/images/adding-webhook-c76d2f73bb0cadcf48620b59db1a1a9c.jpg)


We have chosen to run the webhook once the Actor has succeeded, which means that its default dataset will surely be populated. Since the filtering Actor is expecting the default dataset ID of the Amazon Actor, we use the `resource` variable to grab hold of the `defaultDatasetId`.


Click **Save**, then run the Amazon **demo-actor** again.


## Making sure it worked


If everything worked, then at the end of the **demo-actor**'s run, we should see this within the **Integrations** tab:


![Webhook succeeded](/assets/images/webhook-succeeded-f95ddb172f63747d28dc72e5cdbb9c21.png)


Additionally, we should be able to see that our **filter-actor** was run, and have access to its dataset:


![Dataset preview](/assets/images/dataset-preview-711de106446452a93cc8c15675d77a4d.png)


## Quiz answers 📝


**Q: How do you allocate more CPU for an Actor's run?**


**A:** On the platform, more memory can be allocated in the Actor's input configuration, and the default allocated CPU can be changed in the Actor's **Settings** tab. When running locally, you can use the **APIFY\_MEMORY\_MBYTES** environment variable to set the allocated CPU. 4GB is equal to 1 CPU core on the Apify platform.


**Q: Within itself, can you get the exact time that an Actor was started?**


**A:** Yes. The time the Actor was started can be retrieved through the `startedAt` property from the `Actor.getEnv()` function, or directly from `process.env.APIFY_STARTED_AT`


**Q: What are the types of default storages connected to an Actor's run?**


Every Actor's run is given a default key-value store and a default dataset. The default key-value store by default has the `INPUT` and `OUTPUT` keys. The Actor's request queue is also stored.


**Q: Can you change the allocated memory of an Actor while it's running?**


**A:** Not while it's running. You'd need to stop it and run a new one. However, there is an option to soft abort an Actor, then resurrect then run with a different memory configuration.


**Q: How can you run an Actor with Puppeteer on the Apify platform with headless mode set to `false`?**


**A:** This can be done by using the `actor-node-puppeteer-chrome` Docker image and making sure that `launchContext.launchOptions.headless` in `PuppeteerCrawlerOptions` is set to `false`.


## Wrap up


See that?! Integrating webhooks is a piece of cake on the Apify platform! You'll soon discover that the platform factors away a lot of complex things and allows you to focus on what's most important - developing and releasing Actors.


# Managing source


**View in-depth answers for all three of the quiz questions that were provided in the corresponding lesson about managing source code.**


***


In the lesson corresponding to this solution, we discussed an extremely important topic: source code management. Though we solved the task right in the lesson, we've still included the quiz answers here.


## Quiz answers


**Q: Do you have to rebuild an Actor each time the source code is changed?**


**A:** Yes. It needs to be built into an image, saved in a registry, and later on run in a container.


**Q: In Git, what is the difference between pushing changes and making a pull request?**


**A:** Pushing changes to the remote branch based on the content on the local branch. The pushing of code changes is usually made to a branch parallel to the one you want to eventually push it to.


When creating a pull request, the code is meant to be reviewed, or at least pass all the test suites before being merged into the target branch.


**Q: Based on your knowledge and experience, is the `apify push` command worth using (in your opinion)?**


**A:** The `apify push` command can sometimes be useful when testing ideas; however, it is much more ideal to use GitHub integration rather than directly pushing to the platform.


# Using storage & creating tasks


## Quiz answers 📝


**Q: What is the relationship between Actors and tasks?**


**A:** Tasks are pre-configured runs of Actors. The configurations of an Actor can be saved as a task so that it doesn't have to be manually configured every single time.


**Q: What are the differences between default (unnamed) and named storage? Which one would you use for everyday usage?**


**A:** Unnamed storage is persisted for only 7 days, while named storage is persisted indefinitely. For everyday usage, it is best to use default unnamed storages unless the data should explicitly be persisted for more than 7 days.


> With named storages, it's easier to verify that you're using the correct store, as they can be referred to by name rather than by an ID.


**Q: What is data retention, and how does it work for all types of storages (default and named)?**


**A:** Default/unnamed storages expire after 7 days unless otherwise specified. Named storages are retained indefinitely.


## Wrap up


You've learned how to use the different storage options available on Apify, the two different types of storage, as well as how to create tasks for Actors.


# Using the Apify API & JavaScript client


**Learn how to interact with the Apify API directly through the well-documented RESTful routes, or by using the proprietary Apify JavaScript client.**


***


Since we need to create another Actor, we'll once again use the `apify create` command and start from an empty template. This time, let's call our project **actor-caller**:




```
$ apify create filter-caller
? Choose the programming language of your new Actor:
❯ JavaScript
  TypeScript
  Python
```




Again, use the arrow down key to select **Empty JavaScript Project**:




```
$ apify create filter-actor
✔ Choose the programming language of your new Actor: JavaScript
? Choose a template for your new Actor. Detailed information about the template will be shown in the next step.
  Crawlee + Playwright + Chrome
  Crawlee + Playwright + Camoufox
  Bootstrap CheerioCrawler
  Cypress
❯ Empty JavaScript Project
  Standby JavaScript Project
  ...
```




Confirm the choices by **Install template** and wait until our new Actor is ready. Now let's also set up some boilerplate, grabbing our inputs and creating a constant variable for the task:




```
import { Actor } from 'apify';
import axios from 'axios';


await Actor.init();


const { useClient, memory, fields, maxItems } = await Actor.getInput();


const TASK = 'YOUR_USERNAME~demo-actor-task';


// our future code will go here


await Actor.exit();
```




## Calling a task via JavaScript client


When using the `apify-client` package, you can create a new client instance by using `new ApifyClient()`. Within the Apify SDK however, it is not necessary to even install the `apify-client` package, as the `Actor.newClient()` function is available for use.


We'll start by creating a function called `withClient()` and creating a new client, then calling the task:




```
const withClient = async () => {
    const client = Actor.newClient();
    const task = client.task(TASK);


    const { id } = await task.call({ memory });
};
```




After the task has run, we'll grab hold of its dataset, then attempt to download the items, plugging in our `maxItems` and `fields` inputs. Then, once the data has been downloaded, we'll push it to the default key-value store under a key named **OUTPUT.csv**.




```
const withClient = async () => {
    const client = Actor.newClient();
    const task = client.task(TASK);


    const { id } = await task.call({ memory });


    const dataset = client.run(id).dataset();


    const items = await dataset.downloadItems('csv', {
        limit: maxItems,
        fields,
    });


    // If the content type is anything other than JSON, it must
    // be specified within the third options parameter
    return Actor.setValue('OUTPUT', items, { contentType: 'text/csv' });
};
```




## Calling a task via API


First, we'll create a function (right under the `withClient()`) function named `withAPI` and instantiate a new variable which represents the API endpoint to run our task:




```
const withAPI = async () => {
    const uri = `https://api.apify.com/v2/actor-tasks/${TASK}/run-sync-get-dataset-items?`;
};
```




To add the query parameters to the URL, we could create a super long string literal, plugging in all of our input values; however, there is a much better way: https://nodejs.org/api/url.html#new-urlsearchparams. By using `URLSearchParams`, we can add the query parameters in an object:




```
const withAPI = async () => {
    const uri = `https://api.apify.com/v2/actor-tasks/${TASK}/run-sync-get-dataset-items?`;
    const url = new URL(uri);


    url.search = new URLSearchParams({
        memory,
        format: 'csv',
        limit: maxItems,
        fields: fields.join(','),
        token: process.env.APIFY_TOKEN,
    });
};
```




Finally, let's make a `POST` request to our endpoint. You can use any library you want, but in this example, we'll use https://www.npmjs.com/package/axios. Don't forget to run `npm install axios` if you're going to use this package too!




```
const withAPI = async () => {
    const uri = `https://api.apify.com/v2/actor-tasks/${TASK}/run-sync-get-dataset-items?`;
    const url = new URL(uri);


    url.search = new URLSearchParams({
        memory,
        format: 'csv',
        limit: maxItems,
        fields: fields.join(','),
        token: process.env.APIFY_TOKEN,
    });


    const { data } = await axios.post(url.toString());


    return Actor.setValue('OUTPUT', data, { contentType: 'text/csv' });
};
```




## Finalizing the Actor


Now, since we've written both of these functions, all we have to do is write a conditional statement based on the boolean value from `useClient`:




```
if (useClient) await withClient();
else await withAPI();
```




And before we push to the platform, let's not forget to write an input schema in the **INPUT\_SCHEMA.JSON** file:




```
{
  "title": "Actor Caller",
  "type": "object",
  "schemaVersion": 1,
  "properties": {
    "memory": {
      "title": "Memory",
      "type": "integer",
      "description": "Select memory in megabytes.",
      "default": 4096,
      "maximum": 32768,
      "unit": "MB"
    },
    "useClient": {
      "title": "Use client?",
      "type": "boolean",
      "description": "Specifies whether the Apify JS client, or the pure Apify API should be used.",
      "default": true
    },
    "fields": {
      "title": "Fields",
      "type": "array",
      "description": "Enter the dataset fields to export to CSV",
      "prefill": ["title", "url", "price"],
      "editor": "stringList"
    },
    "maxItems": {
      "title": "Max items",
      "type": "integer",
      "description": "Fill the maximum number of items to export.",
      "default": 10
    }
  },
  "required": ["useClient", "memory", "fields", "maxItems"]
}
```




## Final code


To ensure we're on the same page, here is what the final code looks like:




```
import { Actor } from 'apify';
import axios from 'axios';


await Actor.init();


const { useClient, memory, fields, maxItems } = await Actor.getInput();


const TASK = 'YOUR_USERNAME~demo-actor-task';


const withClient = async () => {
    const client = Actor.newClient();
    const task = client.task(TASK);


    const { id } = await task.call({ memory });


    const dataset = client.run(id).dataset();


    const items = await dataset.downloadItems('csv', {
        limit: maxItems,
        fields,
    });


    return Actor.setValue('OUTPUT', items, { contentType: 'text/csv' });
};


const withAPI = async () => {
    const uri = `https://api.apify.com/v2/actor-tasks/${TASK}/run-sync-get-dataset-items?`;
    const url = new URL(uri);


    url.search = new URLSearchParams({
        memory,
        format: 'csv',
        limit: maxItems,
        fields: fields.join(','),
        token: process.env.APIFY_TOKEN,
    });


    const { data } = await axios.post(url.toString());


    return Actor.setValue('OUTPUT', data, { contentType: 'text/csv' });
};


if (useClient) {
    await withClient();
} else {
    await withAPI();
}


await Actor.exit();
```




## Quiz answers 📝


**Q: What is the relationship between the Apify API and Apify client? Are there any significant differences?**


**A:** The Apify client mimics the Apify API, so there aren't any super significant differences. It's super handy as it helps with managing the API calls (parsing, error handling, retries, etc) and even adds convenience functions.


The one main difference is that the Apify client automatically uses https://docs.apify.com/api/client/js/docs#retries-with-exponential-backoff to deal with errors.


**Q: How do you pass input when running an Actor or task via API?**


**A:** The input should be passed into the **body** of the request when running an actor/task via API.


**Q: Do you need to install the `apify-client` npm package when already using the `apify` package?**


**A:** No. The Apify client is available right in the SDK with the `Actor.newClient()` function.


## Wrap up


That's it! Now, if you want to go above and beyond, you should create a GitHub repository for this Actor, integrate it with a new one on the Apify platform, and test if it works there as well (with multiple input configurations).


# Handling migrations


**Get real-world experience of maintaining a stateful object stored in memory, which will be persisted through migrations and even graceful aborts.**


***


Let's first head into our **demo-actor** and create a new file named **asinTracker.js** in the **src** folder. Within this file, we are going to build a utility class which will allow us to store, modify, persist, and log our tracked ASIN data.


Here's the skeleton of our class:




```
// asinTracker.js
class ASINTracker {
    constructor() {
        this.state = {};


        // Log the state to the console every ten
        // seconds
        setInterval(() => console.log(this.state), 10000);
    }


    // Add an offer to the ASIN's offer count
    // If ASIN doesn't exist yet, set it to 0
    incrementASIN(asin) {
        if (this.state[asin] === undefined) {
            this.state[asin] = 0;
            return;
        }


        this.state[asin] += 1;
    }
}


// It is only a utility class, so we will immediately
// create an instance of it and export that. We only
// need one instance for our use case.
module.exports = new ASINTracker();
```




Multiple techniques exist for storing data in memory; however, this is the most modular way, as all state-persistence and modification logic will be held in this file.


Here is our updated **routes.js** file which is now utilizing this utility class to track the number of offers for each product ASIN:




```
// routes.js
import { createCheerioRouter } from '@crawlee/cheerio';
import { BASE_URL, OFFERS_URL, labels } from './constants';
import tracker from './asinTracker';
import { dataset } from './main.js';


export const router = createCheerioRouter();


router.addHandler(labels.START, async ({ $, crawler, request }) => {
    const { keyword } = request.userData;


    const products = $('div > div[data-asin]:not([data-asin=""])');


    for (const product of products) {
        const element = $(product);
        const titleElement = $(element.find('.a-text-normal[href]'));


        const url = `${BASE_URL}${titleElement.attr('href')}`;


        // For each product, add it to the ASIN tracker
        // and initialize its collected offers count to 0
        tracker.incrementASIN(element.attr('data-asin'));


        await crawler.addRequest([{
            url,
            label: labels.PRODUCT,
            userData: {
                data: {
                    title: titleElement.first().text().trim(),
                    asin: element.attr('data-asin'),
                    itemUrl: url,
                    keyword,
                },
            },
        }]);
    }
});


router.addHandler(labels.PRODUCT, async ({ $, crawler, request }) => {
    const { data } = request.userData;


    const element = $('div#productDescription');


    await crawler.addRequests([{
        url: OFFERS_URL(data.asin),
        label: labels.OFFERS,
        userData: {
            data: {
                ...data,
                description: element.text().trim(),
            },
        },
    }]);
});


router.addHandler(labels.OFFERS, async ({ $, request }) => {
    const { data } = request.userData;


    const { asin } = data;


    for (const offer of $('#aod-offer')) {
        // For each offer, add 1 to the ASIN's
        // offer count
        tracker.incrementASIN(asin);


        const element = $(offer);


        await dataset.pushData({
            ...data,
            sellerName: element.find('div[id*="soldBy"] a[aria-label]').text().trim(),
            offer: element.find('.a-price .a-offscreen').text().trim(),
        });
    }
});
```




## Persisting state


The **persistState** event is automatically fired (by default) every 60 seconds by the Apify SDK while the Actor is running and is also fired when the **migrating** event occurs.


In order to persist our ASIN tracker object, let's use the `Actor.on` function to listen for the **persistState** event and store it in the key-value store each time it is emitted.




```
// asinTracker.js
import { Actor } from 'apify';
// We've updated our constants.js file to include the name
// of this new key in the key-value store
const { ASIN_TRACKER } = require('./constants');


class ASINTracker {
    constructor() {
        this.state = {};


        Actor.on('persistState', async () => {
            await Actor.setValue(ASIN_TRACKER, this.state);
        });


        setInterval(() => console.log(this.state), 10000);
    }


    incrementASIN(asin) {
        if (this.state[asin] === undefined) {
            this.state[asin] = 0;
            return;
        }


        this.state[asin] += 1;
    }
}


module.exports = new ASINTracker();
```




## Handling resurrections


Great! Now our state will be persisted every 60 seconds in the key-value store. However, we're not done. Let's say that the Actor migrates and is resurrected. We never actually update the `state` variable of our `ASINTracker` class with the state stored in the key-value store, so as our code currently stands, we still don't support state-persistence on migrations.


In order to fix this, let's create a method called `initialize` which will be called at the very beginning of the Actor's run, and will check the key-value store for a previous state under the key **ASIN-TRACKER**. If a previous state does live there, then it will update the class' `state` variable with the value read from the key-value store:




```
// asinTracker.js
import { Actor } from 'apify';
import { ASIN_TRACKER } from './constants';


class ASINTracker {
    constructor() {
        this.state = {};


        Actor.on('persistState', async () => {
            await Actor.setValue(ASIN_TRACKER, this.state);
        });


        setInterval(() => console.log(this.state), 10000);
    }


    async initialize() {
        // Read the data from the key-value store. If it
        // doesn't exist, it will be undefined
        const data = await Actor.getValue(ASIN_TRACKER);


        // If the data does exist, replace the current state
        // (initialized as an empty object) with the data
        if (data) this.state = data;
    }


    incrementASIN(asin) {
        if (this.state[asin] === undefined) {
            this.state[asin] = 0;
            return;
        }


        this.state[asin] += 1;
    }
}


module.exports = new ASINTracker();
```




We'll now call this function at the top level of the **main.js** file to ensure it is the first thing that gets called when the Actor starts up:




```
// main.js


// ...
import tracker from './asinTracker';


// The Actor.init() function should be executed before
// the tracker's initialization
await Actor.init();


await tracker.initialize();
// ...
```




That's everything! Now, even if the Actor migrates (or is gracefully aborted and then resurrected), this `state` object will always be persisted.


## Quiz answers 📝


**Q: Actors have an option in the Settings tab to Restart on error. Would you use this feature for regular Actors? When would you use this feature?**


**A:** It's not best to use this option by default. If it fails, there must be a reason, which would need to be thought through first - meaning that the edge case of failing should be handled when resurrecting the Actor. The state should be persisted beforehand.


**Q: Migrations happen randomly, but by https://docs.apify.com/platform/actors/running/runs-and-builds.md#aborting-runs, you can simulate a similar situation. Try this out on the platform and observe what happens. What changes occur, and what remains the same for the restarted Actor's run?**


**A:** After aborting or throwing an error mid-process, it manages to start back from where it was upon resurrection.


**Q: Why don't you (usually) need to add any special migration handling code for a standard crawling/scraping Actor? Are there any features in Crawlee or Apify SDK that handle this under the hood?**


**A:** Because Apify SDK handles all of the migration handling code for us. If you want to add custom migration-handling code, you can use `Actor.events` to listen for the `migrating` or `persistState` events to save the current state in key-value store (or elsewhere).


**Q: How can you intercept the migration event? How much time do you have after this event happens and before the Actor migrates?**


**A:** By using the `Actor.on` function. You have a maximum of a few seconds before shutdown after the `migrating` event has been fired.


**Q: When would you persist data to the default key-value store instead of to a named key-value store?**


**A:** Persisting data to the default key-value store would help when handling an Actor's run state or with storing metadata about the run (such as results, miscellaneous files, or logs). Using a named key-value store allows you to persist data at the account level to handle data across multiple Actor runs.


## Wrap up


In this activity, we learned how to persist custom values on an interval as well as after Actor migrations by using the `persistState` event and the key-value store. With this knowledge, you can safely increase your Actor's performance by storing data in variables and then pushing them to the dataset periodically/at the end of the Actor's run as opposed to pushing data immediately after it's been collected.


One important thing to note is that this workflow can be used to replace the usage of `userData` to pass data between requests, as it allows for the creation of a "global store" which all requests have access to at any time.


# Rotating proxies/sessions


**Learn firsthand how to rotate proxies and sessions in order to avoid the majority of the most common anti-scraping protections.**


***


If you take a look at our current code for the Amazon scraping Actor, you might notice this snippet:




```
const proxyConfiguration = await Actor.createProxyConfiguration({
    groups: ['RESIDENTIAL'],
});
```




We didn't provide much explanation for this initially, as it was not directly relevant to the lesson at hand. When you https://docs.apify.com/academy/anti-scraping/mitigation/using-proxies.md and pass it to a crawler, Crawlee will make the crawler automatically rotate through the proxies. This entire time, we've been using the **RESIDENTIAL** proxy group to avoid being blocked by Amazon.


> Go ahead and try commenting out the proxy configuration code then running the scraper. What happens?


In order to rotate sessions, we must utilize the https://crawlee.dev/api/core/class/AutoscaledPool, which we've actually also already been using by setting the **useSessionPool** option in our crawler's configuration to **true**. The SessionPool advances the concept of proxy rotation by tying proxies to user-like sessions and rotating those instead. In addition to a proxy, each user-like session has cookies attached to it (and potentially a browser fingerprint as well).


## Configuring SessionPool


Let's go ahead and add a **sessionPoolOptions** key to our crawler's configuration so that we can modify the default settings:




```
const crawler = new CheerioCrawler({
    requestList,
    requestQueue,
    proxyConfiguration,
    useSessionPool: true,
    // This is where our session pool
    // configuration lives
    sessionPoolOptions: {
        // We can add options for each
        // session created by the session
        // pool here
        sessionOptions: {


        },
    },
    maxConcurrency: 50,
    // ...
});
```




Now, we'll use the **maxUsageCount** key to force each session to be thrown away after 5 uses and **maxErrorScore** to trash a session once it receives an error.




```
const crawler = new CheerioCrawler({
    requestList,
    requestQueue,
    proxyConfiguration,
    useSessionPool: true,
    sessionPoolOptions: {
        sessionOptions: {
            maxUsageCount: 5,
            maxErrorScore: 1,
        },
    },
    maxConcurrency: 50,
    // ...
});
```




And that's it! We've successfully configured the session pool to match the task's requirements.


## Limiting proxy location


The final requirement was to use proxies only from the US. Back in our **ProxyConfiguration**, we need to add the **countryCode** key and set it to **US**:




```
const proxyConfiguration = await Actor.createProxyConfiguration({
    groups: ['RESIDENTIAL'],
    countryCode: 'US',
});
```




## Quiz answers


**Q: What are the different types of proxies that Apify proxy offers? What are the main differences between them?**


**A:** Datacenter, residential, and Google SERP proxies with sub-groups. Datacenter proxies are fast and cheap but have a higher chance of being blocked on certain sites in comparison to residential proxies, which are IP addresses located in homes and offices around the world. Google SERP proxies are specifically for Google.


**Q: Which proxy groups do users get on the free plan? Can they access the proxy from their computer?**


**A:** All users have access to the **BUYPROXIES94952**, **GOOGLE\_SERP** and **RESIDENTIAL** groups. Free users cannot access the proxy from outside the Apify platform (paying users can).


**Q: How can you prevent an error from occurring if one of the proxy groups that a user has is removed? What are the best practices for these scenarios?**


**A:** By making the proxy for the scraper to use be configurable by the user through the Actor's input. That way, they can switch proxies if the Actor stops working due to proxy-related issues. It can also be done by using the **AUTO** proxy instead of specific groups.


**Q: Does it make sense to rotate proxies when you are logged into a website?**


**A:** No, because most websites tie an IP address to a session. If you start making requests with cookies used with a different IP address, the website might see it as unusual activity and either block the scraper or automatically log out.


**Q: Construct a proxy URL that will select proxies only from the US.**


**A:** `http://country-US:<proxy_password>@proxy.apify.com:8000`


**Q: What do you need to do to rotate a proxy (one proxy usually has one IP)? How does this differ for CheerioCrawler and PuppeteerCrawler?**


**A:** Making a new request with the proxy endpoint above will automatically rotate it. Sessions can also be used to automatically do this. While proxy rotation is fairly straightforward for Cheerio, it's more complex in Puppeteer, as you have to retire the browser each time a new proxy is rotated in. The SessionPool will automatically retire a browser when a session is retired. Sessions can be manually retired with `session.retire()`.


**Q: Name a few different ways how a website can prevent you from scraping it.**


**A:** IP detection and rate-limiting, browser/fingerprint detection, user behavior tracking, etc.


## Wrap up


In this solution, you learned one of the most important concepts in web scraping - proxy/session rotation. With your newfound knowledge of the SessionPool, you'll be (practically) unstoppable!


# Saving run stats


**Implement the saving of general statistics about an Actor's run, as well as adding request-specific statistics to dataset items.**


***


The code in this solution will be similar to what we already did in the **Handling migrations** solution; however, we'll be storing and logging different data. First, let's create a new file called **Stats.js** and write a utility class for storing our run stats:




```
import Actor from 'apify';


class Stats {
    constructor() {
        this.state = {
            errors: {},
            totalSaved: 0,
        };
    }


    async initialize() {
        const data = await Actor.getValue('STATS');


        if (data) this.state = data;


        Actor.on('persistState', async () => {
            await Actor.setValue('STATS', this.state);
        });


        setInterval(() => console.log(this.state), 10000);
    }


    addError(url, errorMessage) {
        if (!this.state.errors?.[url]) this.state.errors[url] = [];
        this.state.errors[url].push(errorMessage);
    }


    success() {
        this.state.totalSaved += 1;
    }
}


module.exports = new Stats();
```




Cool, very similar to the **AsinTracker** class we wrote earlier. We'll now import **Stats** into our **main.js** file and initialize it along with the ASIN tracker:




```
// ...
import Stats from './Stats.js';


await Actor.init();
await asinTracker.initialize();
await Stats.initialize();
// ...
```




## Tracking errors


In order to keep track of errors, we must write a new function within the crawler's configuration called **errorHandler**. Passed into this function is an object containing an **Error** object for the error which occurred and the **Request** object, as well as information about the session and proxy which were used for the request.




```
const crawler = new CheerioCrawler({
    proxyConfiguration,
    useSessionPool: true,
    sessionPoolOptions: {
        persistStateKey: 'AMAZON-SESSIONS',
        sessionOptions: {
            maxUsageCount: 5,
            maxErrorScore: 1,
        },
    },
    maxConcurrency: 50,
    requestHandler: router,
    // Handle all failed requests
    errorHandler: async ({ error, request }) => {
        // Add an error for this url to our error tracker
        Stats.addError(request.url, error?.message);
    },
});
```




## Tracking total saved


Now, we'll increment our **totalSaved** count for every offer added to the dataset.




```
router.addHandler(labels.OFFERS, async ({ $, request }) => {
    const { data } = request.userData;


    const { asin } = data;


    for (const offer of $('#aod-offer')) {
        tracker.incrementASIN(asin);
        // Add 1 to totalSaved for every offer
        Stats.success();


        const element = $(offer);


        await dataset.pushData({
            ...data,
            sellerName: element.find('div[id*="soldBy"] a[aria-label]').text().trim(),
            offer: element.find('.a-price .a-offscreen').text().trim(),
        });
    }
});
```




## Saving stats with dataset items


Still, in the **OFFERS** handler, we need to add a few extra keys to the items which are pushed to the dataset. Luckily, all of the data required by the task is accessible in the context object.




```
router.addHandler(labels.OFFERS, async ({ $, request }) => {
    const { data } = request.userData;


    const { asin } = data;


    for (const offer of $('#aod-offer')) {
        tracker.incrementASIN(asin);
        // Add 1 to totalSaved for every offer
        Stats.success();


        const element = $(offer);


        await dataset.pushData({
            ...data,
            sellerName: element.find('div[id*="soldBy"] a[aria-label]').text().trim(),
            offer: element.find('.a-price .a-offscreen').text().trim(),
            // Store the handledAt date or current date if that is undefined
            dateHandled: request.handledAt || new Date().toISOString(),
            // Access the number of retries on the request object
            numberOfRetries: request.retryCount,
            // Grab the number of pending requests from the requestQueue
            currentPendingRequests: (await requestQueue.getInfo()).pendingRequestCount,
        });
    }
});
```




## Quiz answers


**Q: Why might you want to store statistics about an Actor's run (or a specific request)?**


**A:** If certain types of requests are error-prone, you might want to save stats about the run to look at them later to either eliminate or better handle the errors. Things like **dateHandled** can be generally useful information.


**Q: In our Amazon scraper, we are trying to store the number of retries of a request once its data is pushed to the dataset. Where would you get this information? Where would you store it?**


**A:** This information is available directly on the request object under the property **retryCount**.


**Q: What is the difference between the `failedRequestHandler` and `errorHandler`?**


**A:** `failedRequestHandler` runs after a request has failed and reached its `maxRetries` count. `errorHandler` runs on every failure and retry.


# How to build Actors


At Apify, we try to make building web scraping and automation straightforward. You can customize our universal scrapers with JavaScript for quick tweaks, use our code templates for rapid setup in JavaScript, TypeScript, or Python, or build from scratch using our JavaScript and Python SDKs or Crawlee libraries for Node.js and Python for ultimate flexibility and control. This guide offers a quick overview of our tools to help you find the right fit for your needs.


## Three ways to build Actors


1. https://apify.com/scrapers/universal-web-scrapers — customize our boilerplate tools to your needs with a bit of JavaScript and setup.


2. https://apify.com/templates for web scraping projects — for a quick project setup to save you development time (includes JavaScript, TypeScript, and Python templates).


3. Open-source libraries and SDKs


   <!-- -->


   1. https://docs.apify.com/sdk/js/ & https://docs.apify.com/sdk/python/ — for creating your own solution from scratch on the Apify platform using our free development kits. Involves more coding but offers infinite flexibility.
   2. https://crawlee.dev/ and https://crawlee.dev/python — for creating your own solutions from scratch using our free web automation libraries. Involves even more coding but offers infinite flexibility. There’s also no need to host these on the platform.


## Universal scrapers & what are they for


https://apify.com/scrapers/universal-web-scrapers were built to provide an intuitive UI plus configuration that will help you start extracting data as quickly as possible. Usually, you just provide a https://docs.apify.com/tutorials/apify-scrapers/getting-started#the-page-function and set up one or two parameters, and you're good to go.


Since scraping and automation come in various forms, we decided to build not just one, but *six* scrapers. This way, you can always pick the right tool for the job. Let's take a look at each particular tool and its advantages and disadvantages.


| Scraper                  | Technology                                               | Advantages                                                                                                  | Disadvantages                                                                                          | Best for                                        |
| ------------------------ | -------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------ | ----------------------------------------------- |
| 🌐 Web Scraper           | Headless Chrome Browser                                  | Simple, fully JavaScript-rendered pages                                                                     | Executes only client-side JavaScript                                                                   | Websites with heavy client-side JavaScript      |
| 👐 Puppeteer Scraper     | Headless Chrome Browser                                  | Powerful Puppeteer functions, executes both server-side and client-side JavaScript                          | More complex                                                                                           | Advanced scraping with client/server-side JS    |
| 🎭 Playwright Scraper    | Cross-browser support with Playwright library            | Cross-browser support, executes both server-side and client-side JavaScript                                 | More complex                                                                                           | Cross-browser scraping with advanced features   |
| 🍩 Cheerio Scraper       | HTTP requests + Cheerio parser (JQuery-like for servers) | Simple, fast, cost-effective                                                                                | Pages may not be fully rendered (lacks JavaScript rendering), executes only server-side JavaScript     | High-speed, cost-effective scraping             |
| ⚠️ JSDOM Scraper         | JSDOM library (Browser-like DOM API)                     | + Handles client-side JavaScript<br />+ Faster than full-browser solutions<br />+ Ideal for light scripting | Not for heavy dynamic JavaScript, executes server-side code only, depends on pre-installed NPM modules | Speedy scraping with light client-side JS       |
| 🍲 BeautifulSoup Scraper | Python-based, HTTP requests + BeautifulSoup parser       | Python-based, supports recursive crawling and URL lists                                                     | No full-featured web browser, not suitable for dynamic JavaScript-rendered pages                       | Python users needing simple, recursive crawling |


### How do I choose the right universal web scraper to start with?


🎯 Decision points:


* Use 🌐 https://apify.com/apify/web-scraper if you need simplicity with full browser capabilities and client-side JavaScript rendering.
* Use 🍩 https://apify.com/apify/cheerio-scraper for fast, cost-effective scraping of static pages with simple server-side JavaScript execution.
* Use 🎭 https://apify.com/apify/playwright-scraper when cross-browser compatibility is crucial.
* Use 👐 https://apify.com/apify/puppeteer-scraper for advanced, powerful scraping where you need both client-side and server-side JavaScript handling.
* Use ⚠️ https://apify.com/apify/jsdom-scraper for lightweight, speedy scraping with minimal client-side JavaScript requirements.
* Use 🍲 https://apify.com/apify/beautifulsoup-scraper for Python-based scraping, especially with recursive crawling and processing URL lists.


To make it easier, here's a short questionnaire that guides you on selecting the best scraper based on your specific use case:


Questionnaire


1. Is the website content rendered with a lot of client-side JavaScript?


   <!-- -->


   * Yes:


     <!-- -->


     * Do you need full browser capabilities?


       <!-- -->


       * Yes: use Web Scraper or Playwright Scraper
       * No, but I still want advanced features: use Puppeteer Scraper


   * No:


     <!-- -->


     * Do you prioritize speed and cost-effectiveness?


       <!-- -->


       * Yes: use Cheerio Scraper
       * No: use JSDOM Scraper


2. Do you need cross-browser support for scraping?


   <!-- -->


   * Yes:\*\* use Playwright Scraper
   * No:\*\* continue to the next step.


3. Is your preferred scripting language Python?\*\*


   <!-- -->


   * Yes:\*\* use BeautifulSoup Scraper
   * No:\*\* continue to the next step.


4. Are you dealing with static pages or lightweight client-side JavaScript?\*\*


   <!-- -->


   * Static pages: use Cheerio Scraper or BeautifulSoup Scraper


   * Light client-side JavaScript:


     <!-- -->


     * Do you want a balance between speed and client-side JavaScript handling?


       <!-- -->


       * Yes: use JSDOM Scraper
       * No: use Web Scraper or Puppeteer Scraper


5. Do you need to support recursive crawling or process lists of URLs?


   <!-- -->


   * Yes, and I prefer Python: use BeautifulSoup Scraper
   * Yes, and I prefer JavaScript: use Web Scraper or Cheerio Scraper
   * No: choose based on other criteria above.


This should help you navigate through the options and choose the right scraper based on the website’s complexity, your scripting language preference, and your need for speed or advanced features.


📚 Resources:


* How to use https://www.youtube.com/watch?v=5kcaHAuGxmY to scrape any website
* How to use https://www.youtube.com/watch?v=1KqLLuIW6MA to scrape the web
* Learn about our $1/month https://apify.com/pricing/creator-plan that encourages devs to build Actors based on universal scrapers


## Web scraping code templates


Similar to our universal scrapers, our https://apify.com/templates also provide a quick start for developing web scrapers, automation scripts, and testing tools. Built on popular libraries like BeautifulSoup for Python or Playwright for JavaScript, they save time on setup, allowing you to focus on customization. Though they require more coding than universal scrapers, they're ideal for those who want a flexible foundation while still needing room to tailor their solutions.


| Code template  | Supported libraries                                   | Purpose                                    | Pros                                                                                         | Cons                                                                                                       |
| -------------- | ----------------------------------------------------- | ------------------------------------------ | -------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |
| 🐍 Python      | Requests, BeautifulSoup, Scrapy, Selenium, Playwright | Creating scrapers Automation Testing tools | - Simplifies setup - Supports major Python libraries                                         | - Requires more manual coding (than universal scrapers)- May be restrictive for complex tasks              |
| ☕️ JavaScript | Playwright, Selenium, Cheerio, Cypress, LangChain     | Creating scrapers Automation Testing tools | - Eases development with pre-set configurations - Flexibility with JavaScript and TypeScript | - Requires more manual coding (than universal scrapers)- May be restrictive for tasks needing full control |


📚 Resources:


* https://www.youtube.com/watch?v=u-i-Korzf8w using a web scraper template.


## Toolkits and libraries


### Apify JavaScript and Python SDKs


https://docs.apify.com/sdk/js/ are designed for developers who want to interact directly with the Apify platform. It allows you to perform tasks like saving data in Apify Datasets, running Apify Actors, and accessing the key-value store. Ideal for those who are familiar with https://docs.apify.com/sdk/js/ and https://docs.apify.com/sdk/python/, SDKs provide the tools needed to develop software specifically on the Apify platform, offering complete freedom and flexibility within the JavaScript ecosystem.


* *Best for*: interacting with the Apify platform (e.g., saving data, running Actors, etc)
* *Pros*: full control over platform-specific operations, integrates seamlessly with Apify services
* *Cons*: requires writing boilerplate code, higher complexity with more room for errors


### Crawlee


https://crawlee.dev/ (for both Node.js and https://crawlee.dev/python) is a powerful web scraping library that focuses on tasks like extracting data from web pages, automating browser interactions, and managing complex scraping workflows. Unlike the Apify SDK, Crawlee does not require the Apify platform and can be used independently for web scraping tasks. It handles complex operations like concurrency management, auto-scaling, and request queuing, allowing you to concentrate on the actual scraping tasks.


* *Best for*: web scraping and automation (e.g., scraping paragraphs, automating clicks)
* *Pros*: full flexibility in web scraping tasks, does not require the Apify platform, leverages the JavaScript ecosystem
* *Cons*: requires more setup and coding, higher chance of mistakes with complex operations


### Combining Apify SDK and Crawlee


While these tools are distinct, they can be combined. For example, you can use Crawlee to scrape data from a page and then use the Apify SDK to save that data in an Apify dataset. This integration allows developers to make use of the strengths of both tools while working within the Apify ecosystem.


📚 Resources:


* Introduction to https://www.youtube.com/watch?v=g1Ll9OlFwEQ
* Crawlee https://crawlee.dev/blog
* Webinar on scraping with https://www.youtube.com/watch?v=iAk1mb3v5iI: how to create scrapers in JavaScript and TypeScript
* Step-by-step video guide: https://www.youtube.com/watch?v=yTRHomGg9uQ in Node.js with Crawlee
* Webinar on how to use https://www.youtube.com/watch?v=ip8Ii0eLfRY
* Introduction to Apify's https://www.youtube.com/watch?v=C8DmvJQS3jk


## Code templates vs. universal scrapers vs. libraries


Basically, the choice here depends on how much flexibility you need and how much coding you're willing to do. More flexibility → more coding.


https://apify.com/scrapers/universal-web-scrapers are simple to set up but are less flexible and configurable. Our https://crawlee.dev/, on the other hand, enable the development of a standard https://nodejs.org/ or Python application, so be prepared to write a little more code. The reward for that is almost infinite flexibility.


https://apify.com/templates are sort of a middle ground between scrapers and libraries. But since they are built on libraries, they are still on the rather more coding than less coding side. They will only give you a starter code to begin with. Please take this into account when choosing the way to build your scraper, and if in doubt — just ask us, and we'll help you out.


## Switching sides: How to transfer an existing solution from another platform


You can also take advantage of the Apify platform's features without having to modify your existing scraping or automation solutions.


### Integrating Scrapy spiders


The Apify platform fully supports Scrapy spiders. By https://apify.com/run-scrapy-in-cloud, you can take advantage of features like scheduling, monitoring, scaling, and API access, all without needing to modify your original spider. This process is made easy with the https://docs.apify.com/cli/, which allows you to convert your Scrapy spider into an Apify Actor with just a few commands. Once deployed, your spider can run in the cloud, offering a reliable and scalable solution for your web scraping needs.


Additionally, you can monetize your spiders by https://apify.com/partners/actor-developers on Apify Store, potentially earning passive income from your work while benefiting from the platform’s extensive features.


### ScrapingBee, ScrapingAnt, ScraperAPI


To make the transition from these platforms easier, we've also created https://apify.com/apify/super-scraper-api. This API is an open-source REST API designed for scraping websites by simply passing a URL and receiving the rendered HTML content in return. This service functions as a cost-effective alternative to other scraping services like ScrapingBee, ScrapingAnt, and ScraperAPI. It supports dynamic content rendering with a headless browser, can use various proxies to avoid blocking, and offers features such as capturing screenshots of web pages. It is ideal for large-scale scraping tasks due to its scalable nature.


To use SuperScraper API, you can deploy it with an Apify API token and access it via HTTP requests. The API supports multiple parameters for fine-tuning your scraping tasks, including options for rendering JavaScript, waiting for specific elements, and handling cookies and proxies. It also allows for custom data extraction rules and JavaScript execution on the scraped pages. Pricing is based on actual usage, which can be cheaper or more expensive than competitors, depending on the configuration.


📚 Resources:


* https://docs.apify.com/cli/docs/integrating-scrapy
* Scrapy monitoring: how to https://blog.apify.com/scrapy-monitoring-spidermon/
* Run ScrapingBee, ScraperAPI, and ScrapingAnt on Apify — https://www.youtube.com/watch?v=YKs-I-2K1Rg


## General resources


* Creating your Actor: https://docs.apify.com/academy/getting-started/creating-actors
* Use it, build it or buy it? https://help.apify.com/en/articles/3024655-choosing-the-right-solution
* How to programmatically retrieve data with the https://www.youtube.com/watch?v=ViYYDHSBAKM&t=0s
* Improved way to https://www.youtube.com/watch?v=8QJetr-BYdQ
* Webinar on https://www.youtube.com/watch?v=4nxStxC1BJM on Apify Store
* 6 things you should know before buying or https://blog.apify.com/6-things-to-know-about-web-scraping/
* For a comprehensive guide on creating your first Actor, visit the https://docs.apify.com/academy.


# Wrap open-source as an Actor


Apify is a cloud platform with a https://apify.com/store of 6,000+ web scraping and automation tools called *Actors*. These tools are used for extracting data from social media, search engines, maps, e-commerce sites, travel portals, and general websites.


Most Actors are developed by a global creator community, and some are developed by Apify. We have 18k monthly active users/developers on the platform (growing 138% YoY). Last month, we paid out $170k to creators (growing 118% YoY), and in total, over the program's history, we paid out almost $2M to them.


## What are Actors


Under the hood, Actors are programs packaged as Docker images, that accept a well-defined JSON input, perform an action, and optionally produce a well-defined JSON output. This makes it easy to auto-generate user interfaces for Actors and integrate them with one another or with external systems. For example, we have user-friendly integrations with Zapier, Make, LangChain, MCP, OpenAPI, and SDKs for TypeScript/Python, CLI, etc. etc.


Actors are a new way to build reusable serverless micro-apps that are easy to develop, share, integrate, and build upon—and, importantly, monetize. While Actors are our invention, we’re in the process of making them an open standard. Learn more at https://whitepaper.actor/.


While most Actors on our marketplace are web scrapers or crawlers, there are ever more Actors for other use cases including data processing, web automation, API backend, or https://apify.com/store/categories/agents. In fact, any piece of software that accepts input, performs a job, and can run in Docker, can be *Actorized* simply by adding an `.actor` directory to it with a couple of JSON files.


## Why Actorize


By publishing your service or project at https://apify.com/store your project will benefit from:


1. *Expanded reach*: Your tool instantly becomes available to Apify's user community and connects with popular automation platforms like https://www.make.com, https://n8n.io/, and https://zapier.com/.
2. *Multiple monetization paths*: Choose from flexible pricing models (monthly subscriptions, pay-per-result, or pay-per-event).
3. *AI integration*: Your Actor can serve as a tool for AI agents through Apify's MCP (Model Context Protocol) server, creating new use cases and opportunities while you earn 80% of all revenues.


Open-Source Benefits


For open-source developers, Actorization adds value without extra costs:


* Host your code in the cloud for easy user trials (no local installs needed).
* Avoid managing cloud infrastructure—users cover the costs.
* Earn income through https://apify.com/partners/open-source-fair-share via GitHub Sponsors or direct payouts.
* Publish and monetize 10x faster than building a micro-SaaS, with Apify handling infra, billing, and access to 700,000+ monthly visitors and 70,000 signups.


For example, IBM’s https://github.com/docling-project/docling merged our pull request that actorized their open-source GitHub repo (24k stars) and added the Apify Actor badge to the README:


![Docling Apify badge](/assets/images/docling-apify-badge-3b6ad8beefffa23d0ffcc9bc92d593bb.png)


### Example Actorized projects


You can Actorize various projects ranging from open-source libraries, throughout existing SaaS services, up to MCP server:


| Name            | Type                   | Source                                      | Actor                                                     |
| --------------- | ---------------------- | ------------------------------------------- | --------------------------------------------------------- |
| Parsera         | SaaS service           | https://parsera.org/                        | https://apify.com/parsera-labs/parsera                    |
| Monolith        | Open source library    | https://github.com/Y2Z/monolith             | https://apify.com/snshn/monolith                          |
| Crawl4AI        | Open source library    | https://github.com/unclecode/crawl4ai       | https://apify.com/janbuchar/crawl4ai                      |
| Docling         | Open source library    | https://github.com/docling-project/docling  | https://apify.com/vancura/docling/source-code             |
| Playwright MCP  | Open source MCP server | https://github.com/microsoft/playwright-mcp | https://apify.com/jiri.spilka/playwright-mcp-server       |
| Browserbase MCP | SaaS MCP server        | https://www.browserbase.com/                | https://apify.com/agentify/browserbase-mcp-server/api/mcp |


### What projects are suitable for Actorization


Use these criteria to decide if your project is a good candidate for Actorization:


1. *Is it self-contained?* Does the project work non-interactively, with a well-defined, preferably structured input and output format? Positive examples include various data processing utilities, web scrapers and other automation scripts. Negative examples are GUI applications or applications that run indefinitely. If you want to run HTTP APIs on Apify, you can do so using https://docs.apify.com/platform/actors/development/programming-interface/standby.md.
2. *Can the state be stored in Apify storages?* If the application has state that can be stored in a small number of files it can utilize https://docs.apify.com/platform/storage/key-value-store.md, or if it processes records that can be stored in Apify’s https://docs.apify.com/platform/storage/request-queue.md. If the output consists of one or many similar JSON objects, it can utilize https://docs.apify.com/platform/storage/dataset.md.
3. *Can it be containerized?* The project needs to be able to run in a Docker container. Apify currently does not support GPU workloads. External services (e.g., databases) need to be managed by developer.
4. *Can it use Apify tooling?* Javascript/Typescript applications and Python applications can be Actorized with the help of the https://docs.apify.com/sdk.md, which makes easy for your code to interacts with the Apify platform. Applications that can be run using just the CLI can also be Actorized using the Apify CLI by writing a simple shell script that retrieves user input using https://docs.apify.com/cli, then runs your application and sends the results back to Apify (also using the CLI). If your application is implemented differently, you can still call the https://docs.apify.com/api/v2.md directly - it’s just HTTP and pretty much every language has support for that but the implementation is less straightforward.


## Actorization guide


This guide outlines the steps to convert your application into an Apify https://docs.apify.com/platform/actors.md. Follow the documentation links for detailed information - this guide provides an overview rather than exhaustive instructions.


### 1. Add Actor metadata - the `.actor` folder


The Apify platform requires your Actor repository to have a `.actor` folder at the root level, which contains the metadata needed to build and run the Actor.


For existing projects, you can add the `.actor` folder using the https://docs.apify.com/cli/docs/reference#apify-init-actorname.


In case you're starting a new project, we strongly advise to start with a https://apify.com/templates using the https://docs.apify.com/cli/docs/reference#apify-create-actorname based on your usecase


* https://apify.com/templates/ts-empty


* https://apify.com/templates/python-empty


* https://apify.com/templates/cli-start


* https://apify.com/templates/python-mcp-server


* … and many others, check out for comprehensive list https://apify.com/templates


  Quick Start for beginners


  For a step-by-step introduction to creating your first Actor (including tech stack choices and development paths), see https://docs.apify.com/platform/actors/development/quick-start.md.


The newly created `.actor` folder contains an `actor.json` file - a manifest of the Actor. See https://docs.apify.com/platform/actors/development/actor-definition/actor-json.md for more details


You must also make sure your Actor has a Dockerfile and that it installs everything needed to successfully run your application. Check out https://docs.apify.com/platform/actors/development/actor-definition/dockerfile.md by Apify. If you don't want to use these, you are free to use any image as the base of your Actor.


When launching the Actor, the Apify platform will simply run your Docker image. This means that a) you need to configure the `ENTRYPOINT` and `CMD` directives so that it launches your application and b) you can test your image locally using Docker.


These steps are the bare minimum you need to run your code on Apify. The rest of the guide will help you flesh it out better.


### 2. Define input and output


Most Actors accept an input and produce an output. As part of Actorization, you need to define the input and output structure of your application.


For detailed information, read the docs for https://docs.apify.com/platform/actors/development/actor-definition/input-schema.md, https://docs.apify.com/platform/actors/development/actor-definition/dataset-schema.md, and general https://docs.apify.com/platform/storage.md.


#### Design guidelines


1. If your application has some arguments or options, those should be part of the input defined by input schema.
2. If there is a configuration file or if your application is configured with environment variables, those should also be part of the input. Ideally, nested structures should be “unpacked”, i.e., try not to accept deeply nested structures in your input. Start with less input options and expand later.
3. If the output is a single file, you’ll probably want your Actor to output a single dataset item that contains a public URL to the output file stored in the Apify key-value store
4. If the output has a table-like structure or a series of JSON-serializable objects, you should output each row or object as a separate dataset item
5. If the output is a single key-value record, your Actor should return a single dataset item


### 3. Handle state persistence (optional)


If your application performs a number of well-defined subtasks, the https://docs.apify.com/platform/storage/request-queue.md lets you pause and resume execution on job restart. This is important for long-running jobs that might be migrated between servers at some point. In addition, this allows the Apify platform to display the progress to your users in the UI.


A lightweight alternative to the request queue is simply storing the state of your application as a JSON object in the key-value store and checking for that when your Actor is starting.


Fully-fledged Actors will often combine these two approaches for maximum reliability. More on this topic you find in the https://docs.apify.com/platform/actors/development/builds-and-runs/state-persistence.md article.


### 4. Write Actorization code


Perhaps the most important part of the Actorization process is writing the code that will be executed when the Apify platform launches your Actor.


Unless you’re writing an application targeted directly on the Apify platform, this will have the form of a script that calls your code and integrates it with the Apify Storages


Apify provides SDKs for https://docs.apify.com/sdk/js and https://docs.apify.com/sdk/python plus a https://docs.apify.com/cli allowing an easy interaction with Apify platform from command line.


Check out https://docs.apify.com/platform/actors/development/programming-interface.md documentation article for details on interacting with the Apify platform in your Actor's code.


### 5. Deploy the Actor


Deployment to Apify platform can be done easily via `apify push` command of https://docs.apify.com/cli and for details see https://docs.apify.com/platform/actors/development/deployment.md documentation.


### 6. Publish and monetize


For details on publishing the Actor in https://apify.com/store see the https://docs.apify.com/platform/actors/publishing.md. You can also follow our guide on https://docs.apify.com/academy/actor-marketing-playbook/actor-basics/how-to-create-an-actor-readme.md and https://docs.apify.com/academy/actor-marketing-playbook/promote-your-actor/checklist.md.


# How to create a great input schema


Optimizing your input schema. Learn to design and refine your input schema with best practices for a better user experience.


***


## What is an input schema


You've succeeded: your user has:


1. Found your Actor on Google.
2. Explored the Actor's landing page.
3. Decided to try it.
4. Created an Apify account.


Now they’re on your Actor's page in Apify Console. The SEO fight is over. What’s next?


Your user is finally one-on-one with your Actor — specifically, its input schema. This is the moment when they try your Actor and decide whether to stick with it. The input schema is your representative here, and you want it to work in your favor.


Technically, the input schema is a `JSON` object with various field types supported by the Apify platform, designed to simplify the use of the Actor. Based on the input schema you define, the Apify platform automatically generates a *user interface* for your Actor.


Of course, you can create an Actor without setting up an elaborate input schema. If your Actor is designed for users who don't need a good interface (e.g. they’ll use a JSON object and call it via API), you can skip this guide. But most users engage with Actors in Manual mode, aka the Actor interface. If your Actor is complex or you’re targeting regular users who need an intuitive interface, it's essential to consider their experience.


In this article, *we’ll refer to the input schema as the user interface* of your Actor and focus exclusively on it.


Understand input schemas


To fully understand the recommendations in this blog post, you’ll first need to familiarize yourself with the https://docs.apify.com/platform/actors/development/actor-definition/input-schema. This context is essential to make good use of the insights shared here.


## The importance of a good input schema


It can feel intimidating when facing the Apify platform for the first time. You only have a few seconds for a user to assess the ease of using your Actor.


If something goes wrong or is unclear with the input, an ideal user will first turn to the tooltips in the input schema. Next, they might check the README or tutorials, and finally, they’ll reach out to you through the **Issues** tab. However, many users won’t go through all these steps — they may simply get overwhelmed and abandon the tool altogether.


A well-designed input schema is all about managing user expectations, reducing cognitive load, and preventing frustration. Ideally, a good input schema, as your first line of interaction, should:


* Make the tool as easy to use as possible
* Reduce the user’s cognitive load and make them feel confident about using and paying for it
* Give users enough information and control to figure things out on their own
* Save you time on support by providing clear guidance
* Prevent incorrect or harmful tool usage, like overcharges or scraping personal information by default


### Reasons to rework an input schema


* Your Actor is complex and has many input fields
* Your Actor offers multiple ways to set up input (by URL, search, profile, etc.)
* You’re adding new features to your Actor
* Certain uses of the Actor have caveats that need to be communicated immediately
* Users frequently ask questions about specific fields


👀 Input schema can be formatted using basic HTML.


## Most important elements of the input schema


You can see the full list of elements and their technical characteristics in https://docs.apify.com/academy/deploying-your-code/input-schema: titles, tooltips, toggles, prefills, etc. That's not what this guide is about. It's not enough to just create an input schema, you should ideally aim to place and word its elements to the user's advantage: to alleviate the user's cognitive load and make the acquaintance and usage of your tool as smooth as possible.


Unfortunately, when it comes to UX, there's only so much you can achieve armed with HTML alone. Here are the best elements to focus on, along with some best practices for using them effectively:


* **`description` at the top**


  * As the first thing users see, the description needs to provide crucial information and a sense of reassurance if things go wrong. Key points to mention: the easiest way to try the Actor, links to a guide, and any disclaimers or other similar Actors to try.


    ![Input schema description example](/assets/images/description-sshot-4a31a900bc58209d44032f409cf8eed6.png)


  * Descriptions can include multiple paragraphs. If you're adding a link, it’s best to use the `target_blank` property so your user doesn’t lose the original Actor page when clicking.


* **`title` of the field (regular bold text)**


  * This is the default way to name a field.


  * Keep it brief. The user’s flow should be 1. title → 2. tooltip → 3. link in the tooltip. Ideally, the title alone should provide enough clarity. However, avoid overloading the title with too much information. Instead, make the title as concise as possible, expand details in the tooltip, and include a link in the tooltip for full instructions.


    ![Input schema input example](/assets/images/title-sshot-59c5431c3d78f35f398c1c55d930b806.png)


* **`prefill`, the default input**


  * this is your chance to show rather than tell


    <!-- -->


    * Keep the **prefilled number** low. Set it to 0 if it's irrelevant for a default run.
    * Make the **prefilled text** example simple and easy to remember.
    * If your Actor accepts various URL formats, add a few different **prefilled URLs** to show that possibility.
    * Use the **prefilled date** format that the user is expected to follow. This way, they can learn the correct format without needing to check the tooltip.
    * There’s also a type of field that looks like a prefill but isn’t — usually a `default` field. It’s not counted as actual input but serves as a mock input to show users what to type or paste. It is gray and disappears after clicking on it. Use this to your advantage.


* **toggle**


  * The toggle is a boolean field. A boolean field represents a yes/no choice.


  * How would you word this toggle: **Skip closed places** or **Scrape open places only**? And should the toggle be enabled or disabled by default?


    ![Input schema toggle example](/assets/images/toggle-sshot-b27af75e3ef46c83a61ef2bad6670a56.png)


    * You have to consider this when you're choosing how to word the toggle button and which choice to set up as the default. If you're making this more complex than it's needed (e.g. by using negation as the ‘yes’ choice), you're increasing your user's cognitive load. You also might get them to receive way less, or way more, data than they need from a default run.
    * In our example, we assume the default user wants to scrape all places but still have the option to filter out closed ones. However, they have to make that choice consciously, so we keep the toggle disabled by default. If the toggle were enabled by default, users might not notice it, leading them to think the tool isn't working properly when it returns fewer results than expected.


* **sections or `sectionCaption` (BIG bold text) and `sectionDescription`**


  * A section looks like a wrapped toggle list.


    ![Input schema sections example](/assets/images/sections-sshot-fc6cbd06170d0a33c1c9ab909bd8d6d1.png)


  * It is useful to section off non-default ways of input or extra features. If your tool is complex, don't leave all fields in the first section. Just group them by topic and section them off (see the screenshot above ⬆️)


    * You can add a description to every section. Use `sectionDescription` only if you need to provide extra information about the section (see the screenshot below ⬇️.
    * sometimes `sectionDescription` is used as a space for disclaimers so the user is informed of the risks from the outset instead of having to click on the tooltip.


    ![Input schema section description example](/assets/images/section-description-sshot-3f2616cb044875c2841e131fe408554c.png)


* tooltips or `description` to the title


  * To see the tooltip's text, the user needs to click on the `?` icon.


  * This is your space to explain the title and what's going to happen in that field: any terminology, referrals to other fields of the tool, examples that don't fit the prefill, or caveats can be detailed here. Using HTML, you can add links, line breaks, code, and other regular formatting here. Use this space to add links to relevant guides, video tutorials, screenshots, issues, or readme parts if needed.


  * Wording in titles vs. tooltips. Titles are usually nouns. They have a neutral tone and simply inform on what content this field is accepting (**Usernames**).


    * Tooltips to those titles are usually verbs in the imperative that tell the user what to do (*Add, enter, use*).
    * This division is not set in stone, but the reason why the tooltip is an imperative verb is because, if the user is clicking on the tooltip, we assume they are looking for clarifications or instructions on what to do.


    ![Input schema tooltips example](/assets/images/tooltips-sshot-956de479172bfe492e0e8b98a06e6e01.png)


* emojis (visual component)


  * Use them to attract attention or as visual shortcuts. Use emojis consistently to invoke a user's iconic memory. The visual language should match across the whole input schema (and README) so the user can understand what section or field is referred to without reading the whole title.
    <!-- -->
    * Don't overload the schema with emojis. They attract attention, so you need to use them sparingly.


tip


Read more on the use of emojis: https://docs.apify.com/academy/actor-marketing-playbook/actor-basics/actors-and-emojis.md


## Example of an improved input schema


1. A well-used `description` space. The description briefly introduces possible scraping options, visual language (sections represented by emojis), the easiest way to try the tool, and a link to a tutorial in case of issues. The description isn't too long, uses different formatting, and looks reassuring.
2. The main section is introduced and visually separated from the rest. This is the space for the user to try the first run before they can discover the other options.
3. The title says right away that this field refers to multiple other fields, not only the first section.
4. `prefill` is a small number (so in case users run the tool with default settings, it doesn't take too long and isn't expensive for them) and uses the language of the target website (not results or posts, *videos*).
5. The tooltip expands with more details and refers to other sections it's applicable to using matching emojis.
6. Section names are short. Sections are grouped by content type.
7. More technical parameters lack emojis. They are formatted this way to attract less attention and visually inform the user that this section is the most optional to set.
8. Visual language is unified across the whole input schema. Emojis are used as a shortcut for the user to understand what section or field is referred to without actually reading the whole title.


![Input schema example](/assets/images/improved-input-schema-example-193dcc1c44cbcc8db6016ced168d8dc5.png)


### Example of a worse input schema


The version above was the improved input schema. Here's what this tool's input schema looked like before:


1. Brief and dry description, with little value for the user, easy to miss. Most likely, the user already knows this info because what this Actor does is described in the Actor SEO description, description, and README.
2. The field title is wordy and reads a bit techie: it uses terminology that's not the most accurate for the target website (*posts*) and limiting terms (*max*). The field is applicable for scraping by hashtags (field above) and by profile (section below). Easy detail to miss.
3. The prefilled number is too high. If the user runs the Actor with default settings, they might spend a lot of money, and it will take some time. Users often just leave if an Actor takes a long time to complete on the first try.
4. The tooltip simply reiterates what is said in the title. Could've been avoided if the language of the title wasn't so complex.
5. Merging two possible input types into one (profiles and URLs) can cause confusion. Verbose, reminds the user about an unrelated field (hashtags).
6. This section refers to profiles but is separate. The user had to make extra effort to scrape profiles. They have to move across 3 sections: (use Max posts from section 1, use Profiles input from section 2, use Date sorting filters from section 3).
7. The proxy and browser section invites the users to explore it even though it's not needed for a default run. It's more technical to set up and can make an impression that you need to know how to set it so the tool works.


![Input schema example](/assets/images/worse-input-schema-f6354139a96611112dbeb1f9882ab2e9.png)


## Best practices


1. Keep it short. Don’t rely too much on text - most users prefer to read as little as possible.
2. Use formatting to your advantage (bold, italic, underline), links, and breaks to highlight key points.
3. Use specific terminology (e.g., posts, images, tweets) from the target website instead of generic terms like "results" or "pages."
4. Group related items for clarity and ease of use.
5. Use emojis as shortcuts and visual anchors to guide attention.
6. Avoid technical jargon — keep the language simple.
7. Minimize cognitive load wherever possible.


## Signs and tools for improving input schema


* *User feedback*. If they're asking obvious things, complaining, or consistently making silly mistakes with input, take notes. Feedback from users can help you understand their experience and identify areas for improvement.
* *High churn rates*. If your users are trying your tool but quickly abandon it, this is a sign they are having difficulties with your schema.
* *Input Schema Viewer*. Write your base schema in any code editor, then copy the file and put it into https://console.apify.com/actors/UHTe5Bcb4OUEkeahZ/source.\*\* This tool should help you visualize your Input Schema before you add it to your Actor and build it. Seeing how your edits look in Apify Console right away will make the process of editing the fields in code easier.


## Resources


* Basics of input schema: https://docs.apify.com/academy/deploying-your-code/input-schema
* Specifications of input schema: https://docs.apify.com/platform/actors/development/actor-definition/input-schema


# Actor bundles


**Learn what an Actor bundle is, explore existing examples, and discover how to promote them.**


***


## What is an Actor bundle?


If an Actor is an example of web automation software, what is an Actor bundle? An Actor bundle is basically a chain of multiple Actors unified by a common use case. Bundles can include both scrapers and automation tools, and they are usually designed to achieve an overarching goal related to scraping or automation.


The concept of an Actor bundle originated from frequent customer requests for comprehensive tools. For example, someone would ask for a Twitter scraper that also performs additional tasks, or for a way to find all profiles of the same public figure across multiple social media platforms without needing to use each platform separately.


For example, consider a bundle that scrapes company reviews from multiple platforms, such as Glassdoor, LinkedIn, and Indeed. Typically, you would need to use several different scrapers and then consolidate the results. But this bundle would do it all in one run, once provided with the name of the company. Or consider a bundle that scrapes all posts and comments of a given profile, and then produces a sentiment score for each scraped comment.


The main advantage of an Actor bundle is its ease of use. The user inputs a keyword or a URL, and the Actor triggers all the necessary Actors sequentially to achieve the desired result. The user is not expected to use each Actor separately and then process and filter the results themselves.


### Examples of bundles


🔍 https://apify.com/tri_angle/social-media-finder searches for profiles on 13 social media sites provided just the (nick)name.


🍝 https://apify.com/tri_angle/restaurant-review-aggregator gets restaurant reviews from Google Maps, DoorDash, Uber Eats, Yelp, Tripadvisor, and Facebook in one place.


🤔 https://apify.com/tri_angle/social-media-sentiment-analysis-tool not only collects comments from Facebook, Instagram, and TikTok but also performs sentiment analysis on them. It unites post scrapers, comments scrapers and a text analysis tool.


🦾 https://apify.com/tri_angle/wcc-pinecone-integration scrapes a website and stores the data in a Pinecone database to build and improve your own AI chatbot assistant.


🤖 https://apify.com/tri_angle/pinecone-gpt-chatbot combines OpenAI's GPT models with Pinecone's vector database, which simplifies creating a GPT chatbot.


As you can see, they vary in complexity and range.


***


## Caveats


### Pricing model


Since bundles are still relatively experimental, profitability is not guaranteed and will depend heavily on the complexity of the bundle.


However, if you have a solid idea for a bundle, don’t hesitate to reach out. Prepare your case, write to our support team, and we’ll help determine if it’s worth it.


### Specifics of bundle promotion


First of all, when playing with the idea of creating a bundle, always check the keyword potential. Sometimes, there are true keyword gems just waiting to be discovered, with high search volume and little competition.


However, bundles may face the challenge of being "top-of-the-funnel" solutions. People might not search for them directly because they don't have a specific keyword in mind. For instance, someone is more likely to search for an Instagram comment scraper than imagine a bundle that scrapes comments from 10 different platforms, including Instagram.


Additionally, Google tends to favor tools with rather focused descriptions. If your tool offers multiple functions, it can send mixed signals that may conflict with each other rather than accumulate.


Sometimes, even though a bundle can be a very innovative tool product-wise, it can be hard to market from an SEO perspective and match the search intent.


In such cases, you may need to try different marketing and promotion strategies. Once you’ve exhausted every angle of SEO research, be prepared to explore non-organic marketing channels like Product Hunt, email campaigns, community engagement, Reddit, other social media, your existing customer base, word-of-mouth promotion, etc.


Remember, bundles originated as customized solutions for specific use cases - they were not primarily designed to be easily found.


This is also an opportunity to tell a story rather than just presenting a tool. Consider writing a blog post about how you created this tool, recording a video, or hosting a live webinar. If you go this route, it’s important to emphasize how the tool was created and what a technical feat it represents.


That said, don’t abandon SEO entirely. You can still capture some SEO value by referencing the bundle in the READMEs of the individual Actors that comprise it. For example, if a bundle collects reviews from multiple platforms, potential users are likely to search for review scrapers for each specific platform—Google Maps reviews scraper, Tripadvisor reviews scraper, Booking reviews scraper, etc. These keywords may not lead directly to your review scraping bundle, but they can guide users to the individual scrapers, where you can then present the bundle as a more comprehensive solution.


***


## Resources


Learn more about Actor Bundles: https://blog.apify.com/apify-power-actors/


# Run a web server on the Apify platform


**A web server running in an Actor can act as a communication channel with the outside world. Learn how to set one up with Node.js.**


***


Sometimes, an Actor needs a channel for communication with other systems (or humans). This channel might be used to receive commands, to provide info about progress, or both. To implement this, we will run a HTTP web server inside the Actor that will provide:


* An API to receive commands.
* An HTML page displaying output data.


Running a web server in an Actor is a piece of cake! Each Actor run is available at a unique URL (container URL) which always takes the form `https://CONTAINER-KEY.runs.apify.net`. This URL is available in the https://docs.apify.com/api/v2/actor-run-get.md returned by the Apify API, as well as in the Apify console.


If you start a web server on the port defined by the **APIFY\_CONTAINER\_PORT** environment variable (the default value is **4321**), the container URL becomes available and gets displayed in the **Live View** tab in the Actor run console.


For more details, see https://docs.apify.com/platform/actors/development/programming-interface/container-web-server.md.


## Building the Actor


Let's try to build the following Actor:


* The Actor will provide an API to receive URLs to be processed.
* For each URL, the Actor will create a screenshot.
* The screenshot will be stored in the key-value store.
* The Actor will provide a web page displaying thumbnails linked to screenshots and a HTML form to submit new URLs.


To achieve this we will use the following technologies:


* https://expressjs.com framework to create the server
* https://pptr.dev to grab screenshots.
* The https://docs.apify.com/sdk/js to access Apify storages to store the screenshots.


Our server needs two paths:


* `/` - Index path will display a page form to submit a new URL and the thumbnails of processed URLs.
* `/add-url` - Will provide an API to add new URLs using a HTTP POST request.


First, we'll import `express` and create an Express.js app. Then, we'll add some middleware that will allow us to receive form submissions.




```
import { Actor } from 'apify';
import express from 'express';


await Actor.init();


const app = express();


app.use(express.json());
app.use(express.urlencoded({ extended: true }));
```




Now we need to read the following environment variables:


* **APIFY\_CONTAINER\_PORT** contains a port number where we must start the server.
* **APIFY\_CONTAINER\_URL** contains a URL under which we can access the container.
* **APIFY\_DEFAULT\_KEY\_VALUE\_STORE\_ID** is the ID of the default key-value store of this Actor where we can store screenshots.




```
const {
    APIFY_CONTAINER_PORT,
    APIFY_CONTAINER_URL,
    APIFY_DEFAULT_KEY_VALUE_STORE_ID,
} = process.env;
```




Next, we'll create an array of the processed URLs where the **n**th URL has its screenshot stored under the key **n**.jpg in the key-value store.




```
const processedUrls = [];
```




After that, the index route is ready to be defined.




```
app.get('/', (req, res) => {
    let listItems = '';


    // For each of the processed
    processedUrls.forEach((url, index) => {
        const imageUrl = `https://api.apify.com/v2/key-value-stores/${APIFY_DEFAULT_KEY_VALUE_STORE_ID}/records/${index}.jpg`;


        // Display the screenshots below the form
        listItems += `<li>
    <a href="${imageUrl}" target="_blank">
        <img src="${imageUrl}" width="300px" />
        <br />
        ${url}
    </a>
</li>`;
    });


    const pageHtml = `<html>
    <head><title>Example</title></head>
    <body>
        <form method="POST" action="/add-url">
            URL: <input type="text" name="url" placeholder="http://example.com" />
            <input type="submit" value="Add" />
            <hr />
            <ul>${listItems}</ul>
        </form>
    </body>
</html>`;


    res.send(pageHtml);
});
```




And then a second path that receives the new URL submitted using the HTML form; after the URL is processed, it redirects the user back to the root path.




```
import { launchPuppeteer } from 'crawlee';


app.post('/add-url', async (req, res) => {
    const { url } = req.body;
    console.log(`Got new URL: ${url}`);


    // Start chrome browser and open new page ...
    const browser = await launchPuppeteer();
    const page = await browser.newPage();


    // ... go to our URL and grab a screenshot ...
    await page.goto(url);
    const screenshot = await page.screenshot({ type: 'jpeg' });


    // ... close browser ...
    await page.close();
    await browser.close();


    // ... save screenshot to key-value store and add URL to processedUrls.
    await Actor.setValue(`${processedUrls.length}.jpg`, screenshot, { contentType: 'image/jpeg' });
    processedUrls.push(url);


    res.redirect('/');
});
```




And finally, we need to start the web server.




```
// Start the web server!
app.listen(APIFY_CONTAINER_PORT, () => {
    console.log(`Application is listening at URL ${APIFY_CONTAINER_URL}.`);
});
```




### Final code




```
import { Actor } from 'apify';
import express from 'express';


await Actor.init();


const app = express();


app.use(express.json());
app.use(express.urlencoded({ extended: true }));


const {
    APIFY_CONTAINER_PORT,
    APIFY_CONTAINER_URL,
    APIFY_DEFAULT_KEY_VALUE_STORE_ID,
} = process.env;


const processedUrls = [];


app.get('/', (req, res) => {
    let listItems = '';


    // For each of the processed
    processedUrls.forEach((url, index) => {
        const imageUrl = `https://api.apify.com/v2/key-value-stores/${APIFY_DEFAULT_KEY_VALUE_STORE_ID}/records/${index}.jpg`;


        // Display the screenshots below the form
        listItems += `<li>
    <a href="${imageUrl}" target="_blank">
        <img src="${imageUrl}" width="300px" />
        <br />
        ${url}
    </a>
</li>`;
    });


    const pageHtml = `<html>
    <head><title>Example</title></head>
    <body>
        <form method="POST" action="${APIFY_CONTAINER_URL}/add-url">
            URL: <input type="text" name="url" placeholder="http://example.com" />
            <input type="submit" value="Add" />
            <hr />
            <ul>${listItems}</ul>
        </form>
    </body>
</html>`;


    res.send(pageHtml);
});


app.post('/add-url', async (req, res) => {
    const { url } = req.body;
    console.log(`Got new URL: ${url}`);


    // Start chrome browser and open new page ...
    const browser = await Actor.launchPuppeteer();
    const page = await browser.newPage();


    // ... go to our URL and grab a screenshot ...
    await page.goto(url);
    const screenshot = await page.screenshot({ type: 'jpeg' });


    // ... close browser ...
    await page.close();
    await browser.close();


    // ... save screenshot to key-value store and add URL to processedUrls.
    await Actor.setValue(`${processedUrls.length}.jpg`, screenshot, { contentType: 'image/jpeg' });
    processedUrls.push(url);


    res.redirect('/');
});


app.listen(APIFY_CONTAINER_PORT, () => {
    console.log(`Application is listening at URL ${APIFY_CONTAINER_URL}.`);
});
```




When we deploy and run this Actor on the Apify platform, then we can open the **Live View** tab in the Actor console to submit the URL to your Actor through the form. After the URL is successfully submitted, it appears in the Actor log.


With that, we're done! And our application works like a charm :)


The complete code of this Actor is available on its Store https://apify.com/apify/example-web-server/source-code. You can run it there or copy it to your account.


# Puppeteer and Playwright course


**Learn in-depth how to use two of the most popular Node.js libraries for controlling a headless browser - Puppeteer and Playwright.**


***


https://pptr.dev/ and https://playwright.dev/ are libraries that allow you to automate browsing. Based on your instructions, they can open a browser window, load a website, click on links, etc. They can also do this *headlessly*, i.e., in a way that the browser window isn't visible, which is faster.


Both packages were developed by the same team and are very similar, which is why we have combined the Puppeteer course and the Playwright course into one super-course that shows code examples for both technologies. The two differ in only small ways, and those will always be highlighted in the examples.


> Each lesson's activity will contain examples for both libraries, but we recommend using Playwright, as it is newer and has more features and better https://playwright.dev/docs/intro


## Advantages of using a headless browser


When automating a headless browser, you can do a whole lot more in comparison to making HTTP requests for static content. In fact, you can programmatically do pretty much anything a human could do with a browser, such as clicking elements, taking screenshots, typing into text areas, etc.


Additionally, since the requests aren't static, https://docs.apify.com/academy/concepts/dynamic-pages.md can be rendered and interacted with (or, data from the dynamic content can be scraped). Turn on the https://playwright.dev/docs/api/class-testoptions#test-options-headless (`headless: false`) to see exactly what the browser is doing.


Browsers can also be effective for https://docs.apify.com/academy/anti-scraping.md, especially if the website is running https://docs.apify.com/academy/anti-scraping/techniques/browser-challenges.md.


## Disadvantages of headless browsers


Browsers are slow and expensive to run. In the follow-up courses, the Apify Academy will show you how to scrape websites without a browser. Every website can potentially be reverse-engineered into a series of quick and cheap HTTP calls, but it might require significant effort and specialized knowledge.


## Setup


For this course, we'll be jumping right into the features of these awesome libraries and expecting you to already have an environment set up. Here's how we set up our environment:


1. Make sure you've installed https://nodejs.org/en/
2. Create a new folder called **puppeteer-playwright** (or whatever you want to call it)
3. Run the command `npm init -y` within your new folder to automatically initialize the project
4. Add `"type": "module"` to the **package.json** file
5. Create a new file named **index.js**
6. Install the library you're going to be using during this course:


* Install Playwright
* Install Puppeteer




```
npm install playwright
```






```
npm install puppeteer
```




## Course overview


1. https://docs.apify.com/academy/puppeteer-playwright/browser.md


2. https://docs.apify.com/academy/puppeteer-playwright/page.md


   * https://docs.apify.com/academy/puppeteer-playwright/page/interacting-with-a-page.md
   * https://docs.apify.com/academy/puppeteer-playwright/page/waiting.md
   * https://docs.apify.com/academy/puppeteer-playwright/page/page-methods.md


3. https://docs.apify.com/academy/puppeteer-playwright/executing-scripts.md


   * https://docs.apify.com/academy/puppeteer-playwright/executing-scripts/injecting-code.md
   * https://docs.apify.com/academy/puppeteer-playwright/executing-scripts/collecting-data.md


4. https://docs.apify.com/academy/puppeteer-playwright/reading-intercepting-requests.md


5. https://docs.apify.com/academy/puppeteer-playwright/proxies.md


6. https://docs.apify.com/academy/puppeteer-playwright/browser-contexts.md


7. https://docs.apify.com/academy/puppeteer-playwright/common-use-cases.md


## First up


In the https://docs.apify.com/academy/puppeteer-playwright/browser.md of this course, we'll be learning a bit about how to create and use the **Browser** object.


# I - Launching a browser


**Understand what the Browser object is in Puppeteer/Playwright, how to create one, and a bit about how to interact with one.**


***


In order to automate a browser in Playwright or Puppeteer, we need to open one up programmatically. Playwright supports Chromium, Firefox, and Webkit (Safari), while Puppeteer only supports Chromium based browsers. For ease of understanding, we've chosen to use Chromium in the Playwright examples to keep things working on the same plane.


Let's start by using the `launch()` function in the **index.js** file we created in the intro to this course:


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


await chromium.launch();


console.log('launched!');
```






```
import puppeteer from 'puppeteer';


await puppeteer.launch();


console.log('launched!');
```




When we run this code with the command `node index.js`, a browser will open up; however, we won't actually see anything. This is because the default mode of a browser after `launch()`ing it is **headless**, meaning that it has no visible UI.


> If you run this code right now, it will hang. Use **control^** + **C** to force quit the program.


## Launch options


In order to see what's actually happening, we can pass an **options** object (https://pptr.dev/#?product=Puppeteer&version=v13.7.0&show=api-puppeteerlaunchoptions, https://playwright.dev/docs/api/class-browsertype#browser-type-launch) with **headless** set to **false**.


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const browser = await chromium.launch({ headless: false });
await browser.newPage();
```






```
import puppeteer from 'puppeteer';


const browser = await puppeteer.launch({ headless: false });
await browser.newPage();
```




Now we'll actually see a browser open up.


![Chromium browser opened by Puppeteer/Playwright](/assets/images/chromium-844298b27f771e8c1bb0441bf5572180.jpg)


You can pass a whole lot more options to the `launch()` function. We'll be getting into those a little bit later on.


## Browser methods


The `launch()` function also returns a **Browser** object (https://pptr.dev/#?product=Puppeteer&version=v13.7.0&show=api-class-browser, https://playwright.dev/docs/api/class-browser), which is a representation of the browser. This object has many methods, which allow us to interact with the browser from our code. One of them is `close()`. Until now, we've been using **control^** + **C** to force quit the process, but with this function, we'll no longer have to do that.


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const browser = await chromium.launch({ headless: false });
await browser.newPage();


// code will be here in the future


await browser.close();
```






```
import puppeteer from 'puppeteer';


const browser = await puppeteer.launch({ headless: false });
await browser.newPage();


// code will be here in the future


await browser.close();
```




## Next up


Now that we can open a browser, let's move onto the https://docs.apify.com/academy/puppeteer-playwright/page.md where we will learn how to create pages and visit websites programmatically.


# II - Opening & controlling a page


**Learn how to create and open a Page with a Browser, and how to use it to visit and programmatically interact with a website.**


***


When you open up your regular browser and visit a website, you open up a new page (or tab) before entering the URL in the search bar and hitting the **Enter** key. In Playwright and Puppeteer, you also have to open up a new page before visiting a URL. This can be done with the `browser.newPage()` function, which will return a **Page** object (https://pptr.dev/#?product=Puppeteer&version=v13.7.0&show=api-class-page, https://playwright.dev/docs/api/class-page).


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const browser = await chromium.launch({ headless: false });


// Open a new page
const page = await browser.newPage();


await browser.close();
```






```
import puppeteer from 'puppeteer';


const browser = await puppeteer.launch({ headless: false });


// Open a new page
const page = await browser.newPage();


await browser.close();
```




Then, we can visit a website with the `page.goto()` method. Let's go to https://google.com for now. We'll also use the `page.waitForTimeout()` function, which will force the program to wait for a number of seconds before quitting (otherwise, everything will flash before our eyes and we won't really be able to tell what's going on):


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const browser = await chromium.launch({ headless: false });


// Open a new page
const page = await browser.newPage();


// Visit Google
await page.goto('https://google.com');


// wait for 10 seconds before shutting down
await page.waitForTimeout(10000);


await browser.close();
```






```
import puppeteer from 'puppeteer';


const browser = await puppeteer.launch({ headless: false });


// Open a new page
const page = await browser.newPage();


// Visit Google
await page.goto('https://google.com');


// wait for 10 seconds before shutting down
await page.waitForTimeout(10000);


await browser.close();
```




> If you haven't already, go ahead and run this code to see what happens.


## Next up


Now that we know how to open up a page, https://docs.apify.com/academy/puppeteer-playwright/page/interacting-with-a-page.md how to automate page interaction, such as clicking, typing, and pressing keys.


# Interacting with a page


**Learn how to programmatically do actions on a page such as clicking, typing, and pressing keys. Also, discover a common roadblock that comes up when automating.**


***


The **Page** object has a whole boat-load of functions which can be used to interact with the loaded page. We're not going to go over every single one of them right now, but we *will* use a few of the most common ones to add some functionality to our current project.


Let's say that we want to automate searching for **hello world** on Google, then click on the first result and log the title of the page to the console, then take a screenshot and write it to the filesystem. In order to understand how we're going to automate this, let's break down how we would do it manually:


1. Click on the button which accepts Google's cookies policy (To see how it looks, open Google in an anonymous window.)
2. Type **hello world** into the search bar
3. Press **Enter**
4. Wait for the results page to load
5. Click on the first result
6. Read the title of the clicked result's loaded page
7. Screenshot the page


Though it seems complex, the wonderful **Page** API can help us with all the steps.


## Clicking & pressing keys


Let's first focus on the first 3 steps listed above. By using `page.click()` and the CSS selector of the element to click, we can click an element:


* Playwright
* Puppeteer




```
// Click the "Accept all" button
await page.click('button:has-text("Accept all")');
```






```
// Click the "Accept all" button
await page.click('button + button');
```




With `page.click()`, Puppeteer and Playwright actually drag the mouse and click, allowing the bot to act more human-like. This is different from programmatically clicking with `Element.click()` in vanilla client-side JavaScript.


Notice that in the Playwright example, we are using a different selector than in the Puppeteer example. This is because Playwright supports https://playwright.dev/docs/other-locators#css-elements-matching-one-of-the-conditions, such as the **has-text** pseudo class. As a rule of thumb, using text selectors is much more preferable to using regular selectors, as they are much less likely to break. If Google makes the sibling above the **Accept all** button a `<div>` element instead of a `<button>` element, our `button + button` selector will break. However, the button will always have the text **Accept all**; therefore, `button:has-text("Accept all")` is more reliable.


> If you're not already familiar with CSS selectors and how to find them, we recommend referring to the https://docs.apify.com/academy/scraping-basics-javascript/devtools-locating-elements.md from the **Web scraping basics for JavaScript devs** course.


Then, we can type some text into an input field `<textarea>` with `page.type()`; passing a CSS selector as the first, and the string to input as the second parameter:




```
// Type the query into the search box
await page.type('textarea[title]', 'hello world');
```




Finally, we can press a single key by accessing the `keyboard` property of `page` and calling the `press()` function on it:




```
// Press enter
await page.keyboard.press('Enter');
```




This is what we've got so far:


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const browser = await chromium.launch({ headless: false });


const page = await browser.newPage();


await page.goto('https://www.google.com/');


// Click the "Accept all" button
await page.click('button:has-text("Accept all")');


// Type the query into the search box
await page.type('textarea[title]', 'hello world');


// Press enter
await page.keyboard.press('Enter');


await page.waitForTimeout(10000);
await browser.close();
```






```
import puppeteer from 'puppeteer';


const browser = await puppeteer.launch({ headless: false });


const page = await browser.newPage();


await page.goto('https://www.google.com/');


// Click the "Accept all" button
await page.click('button + button');


// Type the query into the search box
await page.type('textarea[title]', 'hello world');


// Press enter
await page.keyboard.press('Enter');


await page.waitForTimeout(10000);
await browser.close();
```




When we run it, we leave off on the results page:


![Google results page reached by headless browser](/assets/images/google-results-7c52a69dcd7170b0a8d1a8b93b321811.png)


Great! Now all we have to do is click the first result which matches the CSS selector `.g a`:


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const browser = await chromium.launch({ headless: false });


const page = await browser.newPage();


await page.goto('https://www.google.com/');


await page.click('button:has-text("Accept all")');


await page.type('textarea[title]', 'hello world');


await page.keyboard.press('Enter');


// Click the first result
await page.click('.g a');


await page.waitForTimeout(10000);
await browser.close();
```






```
// This code will throw an error!
import puppeteer from 'puppeteer';


const browser = await puppeteer.launch({ headless: false });


const page = await browser.newPage();


await page.goto('https://www.google.com/');


await page.click('button + button');


await page.type('textarea[title]', 'hello world');


await page.keyboard.press('Enter');


// Click the first result
await page.click('.g a');


await page.waitForTimeout(10000);
await browser.close();
```




But wait, when we try to run the Puppeteer code, we run into this nasty error:


> The following error won't be present if you're following the Playwright examples. You'll learn why in the next lesson.




```
/Users/me/Desktop/playwright-puppeteer/node_modules/puppeteer/lib/cjs/puppeteer/common/assert.js:26
        throw new Error(message);
              ^


Error: No node found for selector: .g a
    at assert (/Users/me/Desktop/playwright-puppeteer/node_modules/puppeteer/lib/cjs/puppeteer/common/assert.js:26:15)
...
```




We hit this error because we attempted to click an element that wasn't yet present on the page. The results page hadn't even loaded yet!


## Next up


In the https://docs.apify.com/academy/puppeteer-playwright/page/waiting.md, we'll be taking a look at how to **wait for** navigation, events, and content before resuming interactions.


# Interacting with a page


**Learn how to programmatically do actions on a page such as clicking, typing, and pressing keys. Also, discover a common roadblock that comes up when automating.**


***


The **Page** object has a whole boat-load of functions which can be used to interact with the loaded page. We're not going to go over every single one of them right now, but we *will* use a few of the most common ones to add some functionality to our current project.


Let's say that we want to automate searching for **hello world** on Google, then click on the first result and log the title of the page to the console, then take a screenshot and write it to the filesystem. In order to understand how we're going to automate this, let's break down how we would do it manually:


1. Click on the button which accepts Google's cookies policy (To see how it looks, open Google in an anonymous window.)
2. Type **hello world** into the search bar
3. Press **Enter**
4. Wait for the results page to load
5. Click on the first result
6. Read the title of the clicked result's loaded page
7. Screenshot the page


Though it seems complex, the wonderful **Page** API can help us with all the steps.


## Clicking & pressing keys


Let's first focus on the first 3 steps listed above. By using `page.click()` and the CSS selector of the element to click, we can click an element:


* Playwright
* Puppeteer




```
// Click the "Accept all" button
await page.click('button:has-text("Accept all")');
```






```
// Click the "Accept all" button
await page.click('button + button');
```




With `page.click()`, Puppeteer and Playwright actually drag the mouse and click, allowing the bot to act more human-like. This is different from programmatically clicking with `Element.click()` in vanilla client-side JavaScript.


Notice that in the Playwright example, we are using a different selector than in the Puppeteer example. This is because Playwright supports https://playwright.dev/docs/other-locators#css-elements-matching-one-of-the-conditions, such as the **has-text** pseudo class. As a rule of thumb, using text selectors is much more preferable to using regular selectors, as they are much less likely to break. If Google makes the sibling above the **Accept all** button a `<div>` element instead of a `<button>` element, our `button + button` selector will break. However, the button will always have the text **Accept all**; therefore, `button:has-text("Accept all")` is more reliable.


> If you're not already familiar with CSS selectors and how to find them, we recommend referring to the https://docs.apify.com/academy/scraping-basics-javascript/devtools-locating-elements.md from the **Web scraping basics for JavaScript devs** course.


Then, we can type some text into an input field `<textarea>` with `page.type()`; passing a CSS selector as the first, and the string to input as the second parameter:




```
// Type the query into the search box
await page.type('textarea[title]', 'hello world');
```




Finally, we can press a single key by accessing the `keyboard` property of `page` and calling the `press()` function on it:




```
// Press enter
await page.keyboard.press('Enter');
```




This is what we've got so far:


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const browser = await chromium.launch({ headless: false });


const page = await browser.newPage();


await page.goto('https://www.google.com/');


// Click the "Accept all" button
await page.click('button:has-text("Accept all")');


// Type the query into the search box
await page.type('textarea[title]', 'hello world');


// Press enter
await page.keyboard.press('Enter');


await page.waitForTimeout(10000);
await browser.close();
```






```
import puppeteer from 'puppeteer';


const browser = await puppeteer.launch({ headless: false });


const page = await browser.newPage();


await page.goto('https://www.google.com/');


// Click the "Accept all" button
await page.click('button + button');


// Type the query into the search box
await page.type('textarea[title]', 'hello world');


// Press enter
await page.keyboard.press('Enter');


await page.waitForTimeout(10000);
await browser.close();
```




When we run it, we leave off on the results page:


![Google results page reached by headless browser](/assets/images/google-results-7c52a69dcd7170b0a8d1a8b93b321811.png)


Great! Now all we have to do is click the first result which matches the CSS selector `.g a`:


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const browser = await chromium.launch({ headless: false });


const page = await browser.newPage();


await page.goto('https://www.google.com/');


await page.click('button:has-text("Accept all")');


await page.type('textarea[title]', 'hello world');


await page.keyboard.press('Enter');


// Click the first result
await page.click('.g a');


await page.waitForTimeout(10000);
await browser.close();
```






```
// This code will throw an error!
import puppeteer from 'puppeteer';


const browser = await puppeteer.launch({ headless: false });


const page = await browser.newPage();


await page.goto('https://www.google.com/');


await page.click('button + button');


await page.type('textarea[title]', 'hello world');


await page.keyboard.press('Enter');


// Click the first result
await page.click('.g a');


await page.waitForTimeout(10000);
await browser.close();
```




But wait, when we try to run the Puppeteer code, we run into this nasty error:


> The following error won't be present if you're following the Playwright examples. You'll learn why in the next lesson.




```
/Users/me/Desktop/playwright-puppeteer/node_modules/puppeteer/lib/cjs/puppeteer/common/assert.js:26
        throw new Error(message);
              ^


Error: No node found for selector: .g a
    at assert (/Users/me/Desktop/playwright-puppeteer/node_modules/puppeteer/lib/cjs/puppeteer/common/assert.js:26:15)
...
```




We hit this error because we attempted to click an element that wasn't yet present on the page. The results page hadn't even loaded yet!


## Next up


In the https://docs.apify.com/academy/puppeteer-playwright/page/waiting.md, we'll be taking a look at how to **wait for** navigation, events, and content before resuming interactions.


# Page methods


**Understand that the Page object has many different methods to offer, and learn how to use two of them to capture a page's title and take a screenshot.**


***


Other than having methods for interacting with a page and waiting for events and elements, the **Page** object also supports various methods for doing other things, such as https://pptr.dev/api/puppeteer.page.reload, https://playwright.dev/docs/api/class-page#page-screenshot, https://playwright.dev/docs/api/class-page#page-set-extra-http-headers, and extracting the https://pptr.dev/api/puppeteer.page.content.


Last lesson, we left off at a point where we were waiting for the page to navigate so that we can extract the page's title and take a screenshot of it. In this lesson, we'll be learning about the two methods we can use to achieve both of those things.


## Grabbing the title


Two main page functions exist that will return general data:


1. `page.content()` will return the entire HTML content of the page.
2. `page.title()` will return the title of the current page found in the `<title>` tag.


For our case, we'll utilize the `page.title()` function to grab the title and log it to the console:




```
// Grab the title and set it to a variable
const title = await page.title();


// Log the title to the console
console.log(title);
```




## Screenshotting


The `page.screenshot()` function will return a buffer which can be written to the filesystem as an image:




```
// Take the screenshot and write it to the filesystem
await page.screenshot({ path: 'screenshot.png' });
```




> The image will by default be **.png**. To change the image to **.jpeg** type, set the (optional) `type` option to **jpeg**.


## Final code


Here's our final code which extracts the page's title, takes a screenshot and saves it to our project's folder as `screenshot.png`:


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const browser = await chromium.launch({ headless: false });


// Create a page and visit Google
const page = await browser.newPage();
await page.goto('https://google.com');


// Agree to the cookies policy
await page.click('button:has-text("Accept all")');


// Type the query and visit the results page
await page.type('textarea[title]', 'hello world');
await page.keyboard.press('Enter');


// Click on the first result
await page.click('.g a');
await page.waitForLoadState('load');


// Grab the page's title and log it to the console
const title = await page.title();
console.log(title);


// Take a screenshot and write it to the filesystem
await page.screenshot({ path: 'screenshot.png' });


await browser.close();
```






```
import puppeteer from 'puppeteer';


const browser = await puppeteer.launch({ headless: false });


// Create a page and visit Google
const page = await browser.newPage();
await page.goto('https://google.com');


// Agree to the cookies policy
await page.click('button + button');


// Type the query and visit the results page
await page.type('textarea[title]', 'hello world');
await page.keyboard.press('Enter');


// Wait for the first result to appear on the page,
// then click on it
await page.waitForSelector('.g a');
await Promise.all([page.waitForNavigation(), page.click('.g a')]);


// Grab the page's title and log it to the console
const title = await page.title();
console.log(title);


// Take a screenshot and write it to the filesystem
await page.screenshot({ path: 'screenshot.png' });


await browser.close();
```




When you run this code, you should see this logged to the console:




```
"Hello, World!" program - Wikipedia
```




Additionally, you should see a new image named **screenshot.png** in your project's folder that looks something like this:


![Screenshot from Playwright/Puppeteer](/assets/images/wikipedia-screenshot-e418e43eabee246c354755dd29f091c5.png)


## Next up


In the https://docs.apify.com/academy/puppeteer-playwright/executing-scripts.md, we'll gain a solid understanding of the two different contexts we can run our code in when using Puppeteer and Playwright, as well as how to run code in the context of the browser.


# III - Executing scripts


**Understand the two different contexts which your code can be run in, and how to run custom scripts in the context of the browser.**


***


An important concept to understand when dealing with headless browsers is the **context** in which your code is being run. For example, if you try to use the native `fs` Node.js module (used in the previous lesson) while running code in the context of the browser, errors will be thrown saying that it is undefined. Similarly, if you are trying to use `document.querySelector()` or other browser-specific functions in the server-side Node.js context, errors will also be thrown.


![Diagram explaining the two different contexts your code can be run in](/assets/images/context-diagram-f4475f84c3ebf68da73881f283fbc174.jpg)


Here is an example of a common mistake made by beginners to Puppeteer/Playwright:




```
// This code is incorrect!
import { chromium } from 'playwright';


const browser = await chromium.launch({ headless: false });
const page = await browser.newPage();


// visit google
await page.goto('https://www.google.com/');


// change background to green
document.body.style.background = 'green';


await page.waitForTimeout(10000);


await browser.close();
```




When we try and run this, we get this error:




```
ReferenceError: document is not defined
```




The reason this is happening is because we're trying to run browser-side code on the server-side where it is not supported. https://developer.mozilla.org/en-US/docs/Web/API/Document is a property of the browser https://developer.mozilla.org/en-US/docs/Web/API/Window instance that holds the rendered website; therefore, this API is not available in Node.js. How are we supposed to run code within the context of the browser?


## Running code in the context of the browser


We will use `page.evaluate()` to run our code in the browser. This method takes a callback as its first parameter, which will be executed within the browser.


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const browser = await chromium.launch({ headless: false });
const page = await browser.newPage();


await page.goto('https://www.google.com/');


await page.evaluate(() => {
    document.body.style.background = 'green';
});


await page.waitForTimeout(10000);


await browser.close();
```






```
import puppeteer from 'puppeteer';


const browser = await puppeteer.launch({ headless: false });
const page = await browser.newPage();


await page.goto('https://www.google.com/');


await page.evaluate(() => {
    document.body.style.background = 'green';
});


await page.waitForTimeout(10000);


await browser.close();
```




Here's what we see in the automated browser when we run this code:


![Google with the background color changed to green](/assets/images/green-google-c009bd62b8a1b2ec669f6e5ccef214fc.png)


## Using variables in `page.evaluate()`


Within our code, we generate a `randomString` in the Node.js context:




```
const randomString = Math.random().toString(36).slice(2);
```




Now, let's say we want to change the title of the document to be this random string. To have the random string available in the callback of our `page.evaluate()`, we'll pass it in a second parameter. It's best practice to have this second parameter as an object, because in real world situations you often need to pass more than one value.


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const browser = await chromium.launch({ headless: false });
const page = await browser.newPage();


await page.goto('https://www.google.com/');


const params = { randomString: Math.random().toString(36).slice(2) };


await page.evaluate(({ randomString }) => {
    document.querySelector('title').textContent = randomString;
}, params);


await page.waitForTimeout(10000);


await browser.close();
```






```
import puppeteer from 'puppeteer';


const browser = await puppeteer.launch({ headless: false });
const page = await browser.newPage();


await page.goto('https://www.google.com/');


const params = { randomString: Math.random().toString(36).slice(2) };


await page.evaluate(({ randomString }) => {
    document.querySelector('title').textContent = randomString;
}, params);


await page.waitForTimeout(10000);


await browser.close();
```




Now, when we run this code, we can see the title change on the page's tab:


![Google with the background color changed to green](data:image/png;base64,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)


## Next up


The https://docs.apify.com/academy/puppeteer-playwright/executing-scripts/injecting-code.md will be a short one discussing two different ways of executing scripts on a page.


# Injecting code


**Learn how to inject scripts prior to a page's load (pre-injecting), as well as how to expose functions to be run at a later time on the page.**


***


In the previous lesson, we learned how to execute code on the page using `page.evaluate()`, and though this fits the majority of use cases, there are still some more unusual cases. For example, what if we want to execute our custom script prior to the page's load? Or, what if we want to define a function in the page's context to be run at a later time?


We'll be covering both of these cases in this brief lesson.


## Pre-injecting scripts


Sometimes, you need your custom code to run before any other code is run on the page. Perhaps you need to modify an object's prototype, or even re-define certain global variables before they are used by the page's native scripts.


Luckily, Puppeteer and Playwright both have functions for this. In Puppeteer, we use the https://pptr.dev/api/puppeteer.page.evaluateonnewdocument function, while in Playwright we use https://playwright.dev/docs/api/class-page#page-add-init-script. We'll use these functions to override the native `addEventListener` function, setting it to a function that does nothing. This will prevent event listeners from being added to elements.


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const browser = await chromium.launch({ headless: false });
const page = await browser.newPage();


await page.addInitScript(() => {
    // Override the prototype
    Node.prototype.addEventListener = null; /* do nothing */
});


await page.goto('https://google.com');


await page.waitForTimeout(10000);
await browser.close();
```






```
import puppeteer from 'puppeteer';


const browser = await puppeteer.launch({ headless: false });
const page = await browser.newPage();


await page.evaluateOnNewDocument(() => {
    // Override the prototype
    Node.prototype.addEventListener = null; /* do nothing */
});


await page.goto('https://google.com');


await page.waitForTimeout(10000);
await browser.close();
```




> Go ahead and run this code. Can you click the **I accept** button to accept Google's cookies policy?


## Exposing functions


Here's a super awesome function we've created called `returnMessage()`, which returns the string **Apify Academy!**:




```
const returnMessage = () => 'Apify academy!';
```




We want to **expose** this function to our loaded page so that it can be later executed there, which can be done with https://playwright.dev/docs/api/class-page#page-expose-function. This will make `returnMessage()` available when running scripts not only inside of `page.evaluate()`, but also directly from DevTools.


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const browser = await chromium.launch({ headless: false });
const page = await browser.newPage();
await page.goto('https://google.com');


const returnMessage = () => 'Apify academy!';


await page.exposeFunction(returnMessage.name, returnMessage);


const msg = await page.evaluate(() => returnMessage());


console.log(msg);


await page.waitForTimeout(10000);
await browser.close();
```






```
import puppeteer from 'puppeteer';


const browser = await puppeteer.launch({ headless: false });
const page = await browser.newPage();
await page.goto('https://google.com');


const returnMessage = () => 'Apify academy!';


await page.exposeFunction(returnMessage.name, returnMessage);


const msg = await page.evaluate(() => returnMessage());


console.log(msg);


await page.waitForTimeout(10000);
await browser.close();
```




## Next up


Next, we'll be learning a bit about how to extract data using Playwright/Puppeteer. You can use one of the two main ways to do this, so https://docs.apify.com/academy/puppeteer-playwright/executing-scripts/collecting-data.md will be about both of them!


# Extracting data


**Learn how to extract data from a page with evaluate functions, then how to parse it by using a second library called Cheerio.**


***


Now that we know how to execute scripts on a page, we're ready to learn a bit about data extraction. In this lesson, we'll be scraping all the on-sale products from our https://demo-webstore.apify.org/search/on-sale website. Playwright & Puppeteer offer two main methods for data extraction:


1. Directly in `page.evaluate()` and other evaluate functions such as `page.$$eval()`.
2. In the Node.js context using a parsing library such as https://www.npmjs.com/package/cheerio


Crawlee and parsing with Cheerio


If you are using Crawlee, we highly recommend the https://crawlee.dev/api/playwright-crawler/interface/PlaywrightCrawlingContext#parseWithCheerio function for unified data extraction syntax. This way, switching between browser and plain HTTP scraping is a breeze.


## Setup


Here is the base setup for our code, upon which we'll be building off of in this lesson:


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const browser = await chromium.launch({ headless: false });
const page = await browser.newPage();


await page.goto('https://demo-webstore.apify.org/search/on-sale');


// code will go here


await page.waitForTimeout(10000);


await browser.close();
```






```
import puppeteer from 'puppeteer';


const browser = await puppeteer.launch({ headless: false });
const page = await browser.newPage();


await page.goto('https://demo-webstore.apify.org/search/on-sale');


// code will go here


await page.waitForTimeout(10000);


await browser.close();
```




## Extracting from the browser context


Whatever is returned from the callback function in `page.evaluate()` will be returned by the evaluate function, which means that we can set it to a variable like so:




```
const products = await page.evaluate(() => ({ foo: 'bar' }));


console.log(products); // -> { foo: 'bar' }
```




We'll be returning a bunch of product objects from this function, which will be accessible back in our Node.js context after the promise has resolved. Let's now go ahead and write some data extraction code to collect each product:




```
const products = await page.evaluate(() => {
    const productCards = Array.from(document.querySelectorAll('a[class*="ProductCard_root"]'));


    return productCards.map((element) => {
        const name = element.querySelector('h3[class*="ProductCard_name"]').textContent;
        const price = element.querySelector('div[class*="ProductCard_price"]').textContent;


        return {
            name,
            price,
        };
    });
});


console.log(products);
```




When we run this code, we see this logged to our console:


![Products logged to the console](/assets/images/log-products-f59a9aaf95e34ba0915ff44098f8fef4.png)


## Using jQuery


Working with `document.querySelector` is cumbersome and quite verbose, but with the `page.addScriptTag()` function and the latest https://releases.jquery.com/, we can inject jQuery into the current page to gain access to its syntactical sweetness:




```
await page.addScriptTag({ url: 'https://code.jquery.com/jquery-3.6.0.min.js' });
```




This function will literally append a `<script>` tag to the `<head>` element of the current page, allowing access to jQuery's API when using `page.evaluate()` to run code in the browser context.


Now, since we're able to use jQuery, let's translate our vanilla JavaScript code within the `page.evaluate()` function to jQuery:




```
await page.addScriptTag({ url: 'https://code.jquery.com/jquery-3.6.0.min.js' });


const products = await page.evaluate(() => {
    const productCards = Array.from($('a[class*="ProductCard_root"]'));


    return productCards.map((element) => {
        const card = $(element);


        const name = card.find('h3[class*="ProductCard_name"]').text();
        const price = card.find('div[class*="ProductCard_price"]').text();


        return {
            name,
            price,
        };
    });
});


console.log(products);
```




This will output the same exact result as the code in the previous section.


## Parsing in the Node.js context


One of the most popular parsing libraries for Node.js is https://www.npmjs.com/package/cheerio, which can be used in tandem with Playwright and Puppeteer. It is extremely beneficial to parse the page's HTML in the Node.js context for a number of reasons:


* You can port the code between headless browser data extraction and plain HTTP data extraction
* You don't have to worry in which context you're working (which can sometimes be confusing)
* Errors are easier to handle when running in the base Node.js context


To install it, we can run the following command within your project's directory:




```
npm install cheerio
```




Then, we'll import the `load` function like so:




```
import { load } from 'cheerio';
```




Finally, we can create a `Cheerio` object based on our page's current content like so:




```
const $ = load(await page.content());
```




> It's important to note that this `$` object is static. If any content on the page changes, the `$` variable will not automatically be updated. It will need to be re-declared or re-defined.


Here's our full code so far:


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';
import { load } from 'cheerio';


const browser = await chromium.launch({ headless: false });
const page = await browser.newPage();


await page.goto('https://demo-webstore.apify.org/search/on-sale');


const $ = load(await page.content());


// code will go here


await browser.close();
```






```
import puppeteer from 'puppeteer';
import { load } from 'cheerio';


const browser = await puppeteer.launch({ headless: false });
const page = await browser.newPage();


await page.goto('https://demo-webstore.apify.org/search/on-sale');


const $ = load(await page.content());


// code will go here


await browser.close();
```




Now, to loop through all of the products, we'll make use of the `$` object and loop through them while safely in the server-side context rather than running the code in the browser. Notice that this code is nearly exactly the same as the jQuery code above - it is just not running inside of a `page.evaluate()` in the browser context.




```
const $ = load(await page.content());


const productCards = Array.from($('a[class*="ProductCard_root"]'));


const products = productCards.map((element) => {
    const card = $(element);


    const name = card.find('h3[class*="ProductCard_name"]').text();
    const price = card.find('div[class*="ProductCard_price"]').text();


    return {
        name,
        price,
    };
});


console.log(products);
```




## Final code


Here's what our final optimized code looks like:


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';
import { load } from 'cheerio';


const browser = await chromium.launch({ headless: false });
const page = await browser.newPage();


await page.goto('https://demo-webstore.apify.org/search/on-sale');


const $ = load(await page.content());


const productCards = Array.from($('a[class*="ProductCard_root"]'));


const products = productCards.map((element) => {
    const card = $(element);


    const name = card.find('h3[class*="ProductCard_name"]').text();
    const price = card.find('div[class*="ProductCard_price"]').text();


    return {
        name,
        price,
    };
});


console.log(products);


await browser.close();
```






```
import puppeteer from 'puppeteer';
import { load } from 'cheerio';


const browser = await puppeteer.launch({ headless: false });
const page = await browser.newPage();


await page.goto('https://demo-webstore.apify.org/search/on-sale');


const $ = load(await page.content());


const productCards = Array.from($('a[class*="ProductCard_root"]'));


const products = productCards.map((element) => {
    const card = $(element);


    const name = card.find('h3[class*="ProductCard_name"]').text();
    const price = card.find('div[class*="ProductCard_price"]').text();


    return {
        name,
        price,
    };
});


console.log(products);


await browser.close();
```




## Next up


Our https://docs.apify.com/academy/puppeteer-playwright/reading-intercepting-requests.md will be discussing something super cool - request interception and reading data from requests and responses. It's like using DevTools, except programmatically!


# IV - Reading & intercepting requests


**You can use DevTools, but did you know that you can do all the same stuff (plus more) programmatically? Read and intercept requests in Puppeteer/Playwright.**


***


On any website that serves up images, makes https://developer.mozilla.org/en-US/docs/Web/API/XMLHttpRequest, or fetches content in some other way, you can see those requests (and their responses) in the https://docs.apify.com/academy/api-scraping/general-api-scraping/locating-and-learning.md of your browser's DevTools. Lots of data about the request can be found there, such as the headers, payload, and response body.


In Playwright and Puppeteer, it is also possible to read (and even intercept) requests being made on the page - programmatically. This is very useful for things like reading dynamic headers, saving API responses, blocking certain resources, and much more.


During this lesson, we'll be using https://soundcloud.com/tiesto/following on SoundCloud to demonstrate request/response reading and interception. Here's our basic setup for opening the page:


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const browser = await chromium.launch({ headless: false });
const page = await browser.newPage();


// Our code will go here


await page.goto('https://soundcloud.com/tiesto/following');


await page.waitForTimeout(10000);
await browser.close();
```






```
import puppeteer from 'puppeteer';


const browser = await puppeteer.launch({ headless: false });
const page = await browser.newPage();


// Our code will go here


await page.goto('https://soundcloud.com/tiesto/following');


await page.waitForTimeout(10000);
await browser.close();
```




## Reading requests


We can use the https://pptr.dev/#?product=Puppeteer&version=v14.0.0&show=api-event-close function to listen for the **request** event, passing in a callback function. The first parameter of the passed in callback function is an object representing the request.


Upon visiting Tiësto's following page, we can see in the **Network** tab that a request is made to fetch all of the users which he is following.


![Request to grab Tiësto\&#39;s following list](/assets/images/tiesto-request-e84745c5eb8edc0ec84bfeea4472ae7a.png)


Let's go ahead and listen for this request in our code:


* Playwright
* Puppeteer




```
// Listen for all requests
page.on('request', (req) => {
    // If the URL doesn't include our keyword, ignore it
    if (!req.url().includes('followings')) return;


    console.log('Request for followers was made!');
});
```






```
// Listen for all requests
page.on('request', (req) => {
    // If the URL doesn't include our keyword, ignore it
    if (!req.url().includes('followings')) return;


    console.log('Request for followers was made!');
});
```




> Note that you should always define any request reading/interception code prior to calling the `page.goto()` function.


Cool! Now when we run our code, we'll see this logged to the console:




```
Request for followers was made!
```




This request includes some useful query parameters, namely the `client_id`. Let's go ahead and grab these values from the request URL and print them to the console:


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const browser = await chromium.launch({ headless: false });
const page = await browser.newPage();


// Listen for all requests
page.on('request', (req) => {
    // If the URL doesn't include our keyword, ignore it
    if (!req.url().includes('followings')) return;


    // Convert the request URL into a URL object
    const url = new URL(req.url());


    // Print the search parameters in object form
    console.log(Object.fromEntries(url.searchParams));
});


await page.goto('https://soundcloud.com/tiesto/following');


await page.waitForTimeout(10000);
await browser.close();
```






```
import puppeteer from 'puppeteer';


const browser = await puppeteer.launch({ headless: false });
const page = await browser.newPage();


// Listen for all requests
page.on('request', (req) => {
    // If the URL doesn't include our keyword, ignore it
    if (!req.url().includes('followings')) return;


    // Convert the request URL into a URL object
    const url = new URL(req.url());


    // Print the search parameters in object form
    console.log(Object.fromEntries(url.searchParams));
});


await page.goto('https://soundcloud.com/tiesto/following');


await page.waitForTimeout(10000);
await browser.close();
```




After running this code, we can see this logged to the console:




```
{
  client_id: 'llCGDUjKpxUslgO1yEce7Zh95PXE78Bo',
  limit: '12',
  offset: '0',
  linked_partitioning: '1',
  app_version: '1652347025',
  app_locale: 'en'
}
```




## Reading responses


Listening for and reading responses is very similar to reading requests. The only difference is that we need to listen for the **response** event instead of **request**. Additionally, the object passed into the callback function represents the response instead of the request.


This time, instead of grabbing the query parameters of the request URL, let's grab hold of the response body and print it to the console in JSON format:


* Playwright
* Puppeteer




```
// Notice that the callback function is now async
page.on('response', async (res) => {
    if (!res.request().url().includes('followings')) return;


    // Grab the response body in JSON format
    try {
        const json = await res.json();
        console.log(json);
    } catch (err) {
        console.error('Response wasn\'t JSON or failed to parse response.');
    }
});
```






```
// Notice that the callback function is now async
page.on('response', async (res) => {
    if (!res.request().url().includes('followings')) return;


    // Grab the response body in JSON format
    try {
        const json = await res.json();
        console.log(json);
    } catch (err) {
        console.error('Response wasn\'t JSON or failed to parse response.');
    }
});
```




> Take notice of our usage of a `try...catch` block. This is because if the response is not JSON, the `res.json()` function will fail and throw an error, which we must handle to prevent any unexpected crashes.


Upon running this code, we'll see the API response logged into the console:


![API response in console](/assets/images/api-response-tiesto-dafcb582f617c9ceae890c19b8faa6c4.png)


## Intercepting requests


One of the most popular ways of speeding up website loading in Puppeteer and Playwright is by blocking certain resources from loading. These resources are usually CSS files, images, and other miscellaneous resources that aren't super necessary (mainly because the computer doesn't have eyes - it doesn't care how the website looks!).


In Puppeteer, we must first enable request interception with the `page.setRequestInterception()` function. Then, we can check whether or not the request's resource ends with one of our blocked file extensions. If so, we'll abort the request. Otherwise, we'll let it continue. All of this logic will still be within the `page.on()` method.


With Playwright, request interception is a bit different. We use the https://playwright.dev/docs/api/class-page#page-route function instead of `page.on()`, passing in a string, regular expression, or a function that will match the URL of the request we'd like to read from. The second parameter is also a callback function, but with the https://playwright.dev/docs/api/class-route object passed into it instead.


### Blocking resources


We'll first create an array of some file extensions that we'd like to block:




```
const blockedExtensions = ['.png', '.css', '.jpg', '.jpeg', '.pdf', '.svg'];
```




Then, we'll `abort()` all requests that end with any of these extensions.


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const browser = await chromium.launch({ headless: false });
const page = await browser.newPage();


const blockedExtensions = ['.png', '.css', '.jpg', '.jpeg', '.pdf', '.svg'];


// Only listen for requests with one of our blocked extensions
// Abort all matching requests
page.route(`**/*{${blockedExtensions.join(',')}}`, async (route) => route.abort());


await page.goto('https://soundcloud.com/tiesto/following');


await page.waitForTimeout(10000);
await browser.close();
```






```
import puppeteer from 'puppeteer';


const browser = await puppeteer.launch({ headless: false });
const page = await browser.newPage();


const blockedExtensions = ['.png', '.css', '.jpg', '.jpeg', '.pdf', '.svg'];


// Enable request interception (skipping this step will result in an error)
await page.setRequestInterception(true);


// Listen for all requests
page.on('request', async (req) => {
    // If the request ends in a blocked extension, abort the request
    if (blockedExtensions.some((str) => req.url().endsWith(str))) return req.abort();
    // Otherwise, continue
    await req.continue();
});


await page.goto('https://soundcloud.com/tiesto/following');


await page.waitForTimeout(10000);
await browser.close();
```




> You can also use `request.resourceType()` to grab the resource type.


Here's what we see when we run this logic:


![SoundCloud with no CSS or image resources loaded](/assets/images/ugly-soundcloud-28eb2e994a6aca46ad03a97b7102f066.png)


This confirms that we've successfully blocked the CSS and image resources from loading.


#### Quick note about resource blocking


Something **very** important to note is that by using request interception, the browser's cache is turned **off**. This means that resources on websites that would normally be cached (and pulled from the cache instead on the next request for those resources) will not be cached, which can have varying negative effects on performance, especially when making many requests to the same domain, which is very common in web scraping. You can learn how to solve this problem in https://docs.apify.com/academy/node-js/caching-responses-in-puppeteer.md.


To block resources, it is better to use a CDP (Chrome DevTools Protocol) Session (https://playwright.dev/docs/api/class-cdpsession/https://pptr.dev/#?product=Puppeteer&version=v14.1.0&show=api-class-cdpsession) to set the blocked URLs. Here is an implementation that achieves the same goal as our above example above; however, the browser's cache remains enabled.


* Playwright
* Puppeteer




```
// Note, you can't use CDP session in other browsers!
// Only in Chromium.
import { chromium } from 'playwright';


const browser = await chromium.launch({ headless: false });
const page = await browser.newPage();


// Define our blocked extensions
const blockedExtensions = ['.png', '.css', '.jpg', '.jpeg', '.pdf', '.svg'];


// Use CDP session to block resources
const client = await page.context().newCDPSession(page);


await client.send('Network.setBlockedURLs', { urls: blockedExtensions });


await page.goto('https://soundcloud.com/tiesto/following');


await page.waitForTimeout(10000);
await browser.close();
```






```
import puppeteer from 'puppeteer';


const browser = await puppeteer.launch({ headless: false });
const page = await browser.newPage();


// Define our blocked extensions
const blockedExtensions = ['.png', '.css', '.jpg', '.jpeg', '.pdf', '.svg'];


// Use CDP session to block resources
await page.client().send('Network.setBlockedURLs', { urls: blockedExtensions });


await page.goto('https://soundcloud.com/tiesto/following');


await page.waitForTimeout(10000);
await browser.close();
```




### Modifying the request


There's much more to intercepting requests than just aborting them though. We can change the payload, headers, query parameters, and even the base URL.


Let's go ahead and intercept and modify the initial request we fire off with the `page.goto()` by making it go to https://soundcloud.com/mestomusic instead.


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const browser = await chromium.launch({ headless: false });
const page = await browser.newPage();


// Only listen for requests matching this regular expression
page.route(/soundcloud.com\/tiesto/, async (route) => {
    // Continue  the route, but replace "tiesto" in the URL with "mestomusic"
    return route.continue({ url: route.request().url().replace('tiesto', 'mestomusic') });
});


await page.goto('https://soundcloud.com/tiesto/following');


await page.waitForTimeout(10000);
await browser.close();
```






```
import puppeteer from 'puppeteer';


const browser = await puppeteer.launch({ headless: false });
const page = await browser.newPage();


await page.setRequestInterception(true);


// Listen for all requests
page.on('request', async (req) => {
    // If it doesn't match, continue the route normally
    if (!/soundcloud.com\/tiesto/.test(req.url())) return req.continue();
    // Otherwise, continue  the route, but replace "tiesto"
    // in the URL with "mestomusic"
    await req.continue({ url: req.url().replace('tiesto', 'mestomusic') });
});


await page.goto('https://soundcloud.com/tiesto/following');


await page.waitForTimeout(10000);
await browser.close();
```




> Note that this **is not** a redirect, because Tiësto's page was never even visited. The request was changed before it was even fulfilled.


Here's what we see when we run `node index.js`:


![Request intercepted and sent to Mesto\&#39;s page instead](/assets/images/mesto-following-bb43b980a4fe0f6afdaefd7e548530f3.jpg)


## Next up


The https://docs.apify.com/academy/puppeteer-playwright/proxies.md will teach you how to use proxies in Playwright and Puppeteer in order to avoid blocking or to appear as if you are requesting from a different location.


# V - Using proxies


**Understand how to use proxies in your Puppeteer and Playwright requests, as well as a couple of the most common use cases for proxies.**


***


https://docs.apify.com/academy/anti-scraping/mitigation/proxies.md are a great way of appearing as if you are making requests from a different location. A common use case for proxies is to avoid https://docs.apify.com/academy/anti-scraping/techniques/geolocation.md restrictions. For example your favorite TV show might not be available on Netflix in your country, but it might be available for Vietnamese Netflix watchers.


In this lesson, we'll be learning how to use proxies with Playwright and Puppeteer. This will be demonstrated with a Vietnamese proxy that we got by running https://apify.com/mstephen190/proxy-scraper proxy-scraping Actor on the Apify platform.


## Adding a proxy


First, let's add our familiar boilerplate code for visiting Google and also create a variable called `proxy` which will point to our proxy server:


> Note that this proxy may no longer be working at the time of reading. If you don't have a proxy to use during this lesson, we recommend using Proxy Scraper for a list of free ones, or checking out https://apify.com/proxy


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


// our proxy server
const proxy = '103.214.9.13:3128';


const browser = await chromium.launch({ headless: false });
const page = await browser.newPage();
await page.goto('https://google.com');


await page.waitForTimeout(10000);
await browser.close();
```






```
import puppeteer from 'puppeteer';


// our proxy server
const proxy = '103.214.9.13:3128';


const browser = await puppeteer.launch({ headless: false });
const page = await browser.newPage();
await page.goto('https://google.com');


await page.waitForTimeout(10000);
await browser.close();
```




For both Puppeteer and Playwright, the proxy server's URL should be passed into the options of the `launch()` function; however, it's done a bit differently depending on which library you're using.


In Puppeteer, the server must be passed within the **--proxy-server** https://peter.sh/experiments/chromium-command-line-switches/, while in Playwright, it can be passed into the **proxy** option.


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const proxy = '103.214.9.13:3128';


const browser = await chromium.launch({
    headless: false,
    // Using the "proxy" option
    proxy: {
        // Pass in the server URL
        server: proxy,


    },
});
const page = await browser.newPage();
await page.goto('https://google.com');


await page.waitForTimeout(10000);
await browser.close();
```






```
import puppeteer from 'puppeteer';


const proxy = '103.214.9.13:3128';


// Using the "args" option, which is an array of Chromium command
// line switches, we pass the server URL in with "--proxy-server"
const browser = await puppeteer.launch({
    headless: false,
    args: [`--proxy-server=${proxy}`],
});
const page = await browser.newPage();
await page.goto('https://google.com');


await page.waitForTimeout(10000);
await browser.close();
```




And that's it! Now, when we visit Google, it's in Vietnamese. Depending on the country of your proxy, the language will vary.


![Vietnamese Google](/assets/images/vietnamese-google-a742c6f89651d9c47a6d3701140a11cd.png)


> Note that in order to rotate through multiple proxies, you must retire a browser instance then create a new one to continue automating with a new proxy.


## Authenticating a proxy


The proxy in the last activity didn't require a username and password, but let's say that this one does:




```
proxy.example.com:3001
```




One might automatically assume that this would be the solution:


* Playwright
* Puppeteer




```
// This code is wrong!
import { chromium } from 'playwright';


const proxy = 'proxy.example.com:3001';
const username = 'someUsername';
const password = 'password123';


const browser = await chromium.launch({
    headless: false,
    proxy: {
        server: `http://${username}:${password}@${proxy}`,


    },
});
```






```
// This code is wrong!
import puppeteer from 'puppeteer';


const proxy = 'proxy.example.com:3001';
const username = 'someUsername';
const password = 'password123';


const browser = await puppeteer.launch({
    headless: false,
    args: [`--proxy-server=http://${username}:${password}@${proxy}`],
});
```




However, authentication parameters need to be passed in separately in order to work. In Puppeteer, the username and password need to be passed to the `page.authenticate()` prior to any navigations being made, while in Playwright they can be passed to the **proxy** option object.


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const proxy = 'proxy.example.com:3001';
const username = 'someUsername';
const password = 'password123';


const browser = await chromium.launch({
    headless: false,
    proxy: {
        server: proxy,
        username,
        password,
    },
});
// Proxy will now be authenticated
```






```
import puppeteer from 'puppeteer';


const proxy = 'proxy.example.com:3001';
const username = 'someUsername';
const password = 'password123';


const browser = await puppeteer.launch({
    headless: false,
    args: [`--proxy-server=${proxy}`],
});


const page = await browser.newPage();


await page.authenticate({ username, password });
// Proxy will now be authenticated
```




## Next up


You already know how to launch a browser with various configurations, which means you're ready to https://docs.apify.com/academy/puppeteer-playwright/browser-contexts.md. Browser contexts can be used to automate multiple sessions at once with completely different configurations. You'll also learn how to emulate different devices, such as iPhones, iPads, and Androids.


# VI - Creating multiple browser contexts


**Learn what a browser context is, how to create one, how to emulate devices, and how to use browser contexts to automate multiple sessions at one time.**


***


A https://playwright.dev/docs/api/class-browsercontext is an isolated incognito session within a **Browser** instance. This means that contexts can have different device/screen size configurations, different language and color scheme settings, etc. It is useful to use multiple browser instances when dealing with automating logging into multiple accounts simultaneously (therefore requiring multiple sessions), or in any cases where multiple sessions are required.


When we create a **Browser** object by using the `launch()` function, a single https://playwright.dev/docs/browser-contexts is automatically created. In order to create more, we use the https://playwright.dev/docs/api/class-browser#browser-new-context function in Playwright, and https://pptr.dev/#?product=Puppeteer&version=v14.1.0&show=api-browsercreateincognitobrowsercontextoptions in Puppeteer.


* Playwright
* Puppeteer




```
const myNewContext = await browser.newContext();
```






```
const myNewContext = await browser.createIncognitoBrowserContext();
```




## Persistent vs non-persistent browser contexts


In both examples above, we are creating a new **non-persistent** browser context, which means that once it closes, all of its cookies, cache, etc. will be lost. For some cases, that's okay, but in most situations, the performance hit from this is too large. This is why we have **persistent** browser contexts. Persistent browser contexts open up a bit slower and they store all their cache, cookies, session storage, and local storage in a file on disk.


In Puppeteer, the **default** browser context is the persistent one, while in Playwright we have to use https://playwright.dev/docs/api/class-browsertype#browser-type-launch-persistent-context instead of `BrowserType.launch()` in order for the default context to be persistent.


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


// Here, we launch a persistent browser context. The first
// argument is the location to store the data.
const browser = await chromium.launchPersistentContext('./persistent-context', { headless: false });


const page = await browser.newPage();


await browser.close();
```






```
import puppeteer from 'puppeteer';


const browser = await puppeteer.launch({ headless: false });


// This page will be under the default context, which is persistent.
// Cache, cookies, etc. will be stored on disk and persisted
const page = await browser.newPage();


await browser.close();
```




## Using browser contexts


In both Playwright and Puppeteer, various devices (iPhones, iPads, Androids, etc.) can be emulated by using https://playwright.dev/docs/api/class-playwright#playwright-devices or https://pptr.dev/#?product=Puppeteer&version=v14.1.0&show=api-puppeteerdevices. We'll be using this to create two different browser contexts, one emulating an iPhone, and one emulating an Android device:


* Playwright
* Puppeteer




```
import { chromium, devices } from 'playwright';


// Launch the browser
const browser = await chromium.launch({ headless: false });


const iPhone = devices['iPhone 11 Pro'];
// Create a new context for our iPhone emulation
const iPhoneContext = await browser.newContext({ ...iPhone });
// Open a page on the newly created iPhone context
const iPhonePage = await iPhoneContext.newPage();


const android = devices['Galaxy Note 3'];
// Create a new context for our Android emulation
const androidContext = await browser.newContext({ ...android });
// Open a page on the newly created Android context
const androidPage = await androidContext.newPage();


// The code in the next step will go here


await browser.close();
```






```
import puppeteer from 'puppeteer';


// Launch the browser
const browser = await puppeteer.launch({ headless: false });


const iPhone = puppeteer.devices['iPhone 11 Pro'];
// Create a new context for our iPhone emulation
const iPhoneContext = await browser.createIncognitoBrowserContext();
// Open a page on the newly created iPhone context
const iPhonePage = await iPhoneContext.newPage();
// Emulate the device
await iPhonePage.emulate(iPhone);


const android = puppeteer.devices['Galaxy Note 3'];
// Create a new context for our Android emulation
const androidContext = await browser.createIncognitoBrowserContext();
// Open a page on the newly created Android context
const androidPage = await androidContext.newPage();
// Emulate the device
await androidPage.emulate(android);


// The code in the next step will go here


await browser.close();
```




Then, we'll make both `iPhonePage` and `androidPage` visit https://www.deviceinfo.me/, which is a website that displays the type of device you have, the operating system you're using, and more device and location-specific information.




```
// Go to deviceinfo.me on both at the same time
await Promise.all([iPhonePage.goto('https://www.deviceinfo.me/'), androidPage.goto('https://www.deviceinfo.me/')]);


// Wait for 10 seconds on both before shutting down
await Promise.all([iPhonePage.waitForTimeout(10000), androidPage.waitForTimeout(10000)]);
```




Let's go ahead and run our code and analyze the data on each **deviceinfo.me** page. Here's what we see:


![deviceinfo.me results for both browser contexts](/assets/images/dual-contexts-1cf77aac6062264d0ba205af600f5c5a.jpg)


We see that **deviceinfo.me** detects both contexts as using different devices, despite the fact they're visiting the same page at the same time. This shows firsthand that different browser contexts can have totally different configurations, as they all have separate sessions.


## Accessing browser contexts


When working with multiple browser contexts, it can be difficult to keep track of all of them and making changes becomes a repetitive job. This is why the **Browser** instance returned from the `launch()` function also has a `contexts()` function (`browserContexts()` in Puppeteer). This function returns an array of all the contexts that are currently attached to the browser.


Let's go ahead and use this function to loop through all of our browser contexts and make them log **Site visited** to the console whenever the website is visited:


* Playwright
* Puppeteer




```
for (const context of browser.contexts()) {
    // In Playwright, lots of events are supported in the "on" function of
    // a BrowserContext instance
    context.on('request', (req) => req.url() === 'https://www.deviceinfo.me/' && console.log('Site visited'));
}
```






```
for (const context of browser.browserContexts()) {
    // In Puppeteer, only three events are supported in the "on" function
    // of a BrowserContext instance
    context.on('targetchanged', () => console.log('Site visited'));
}
```




After adding this above our `page.goto`s and running the code once again, we see this logged to the console:




```
Site visited
Site visited
```




Cool! We've modified both our `iPhoneContext` and `androidContext`, as well as our default context, to log the message.


> Note that the Puppeteer code and Playwright code are slightly different in the examples above. The Playwright code will log **Site visited** any time the specific URL is visited, while the Puppeteer code will log any time the target URL is changed to anything.


Finally, in Puppeteer, you can use the `browser.defaultBrowserContext()` function to grab hold of the default context at any point.


## Wrap up


Thus far in this course, you've learned how to launch a browser, open a page, run scripts on a page, extract data from a page, intercept requests made on the page, use proxies, and use multiple browser contexts. Stay tuned for new lessons!


# Common use cases


**Learn about some of the most common use cases of Playwright and Puppeteer, and how to handle these use cases when you run into them.**


***


You can do about anything with a headless browser, but, there are some extremely common use cases that are important to understand and be prepared for when you might run into them. This short section will be all about solving these common situations. Here's what we'll be covering:


1. Login flow (logging into an account)
2. Paginating through results on a website
3. Solving browser challenges (ex. CAPTCHAs)
4. More!


## Next up


The https://docs.apify.com/academy/puppeteer-playwright/common-use-cases/logging-into-a-website.md of this section is all about logging into a website and running multiple concurrent operations within a user's account.


# Logging into a website


**Understand the "login flow" - logging into a website, then maintaining a logged in status within different browser contexts for an efficient automation process.**


***


Whether it's auto-renewing a service, automatically sending a message on an interval, or automatically cancelling a Netflix subscription, one of the most popular things headless browsers are used for is automating things within a user's account on a certain website. Of course, automating anything on a user's account requires the automation of the login process as well. In this lesson, we'll be covering how to build a login flow from start to finish with Playwright or Puppeteer.


> In this lesson, we'll be using https://www.yahoo.com/ as an example. Feel free to follow along using the academy Yahoo account credentials, or even deviate from the lesson a bit and try building a login flow for a different website of your choosing!


## Inputting credentials


The full logging in process on Yahoo goes like this:


1. Accept their cookies policy, then load the main page.
2. Click on the **Sign in** button and load the sign-in page.
3. Enter the username and click the button.
4. Enter the password and click the button, then load the main page again (but now logged in).


When we lay out the steps like this in https://en.wikipedia.org/wiki/Pseudocode, it makes it significantly easier to translate over into code. Here's the four steps above loop in JavaScript:


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


// Launch a browser and open a page
const browser = await chromium.launch({ headless: false });
const page = await browser.newPage();


await page.goto('https://www.yahoo.com/');


// Agree to the cookies terms, then click on the "Sign in" button
await page.click('button[name="agree"]');
await page.waitForSelector('a:has-text("Sign in")');


await page.click('a:has-text("Sign in")');
await page.waitForLoadState('load');


// Type in the username and continue forward
await page.type('input[name="username"]', 'YOUR-LOGIN-HERE');
await page.click('input[name="signin"]');


// Type in the password and continue forward
await page.type('input[name="password"]', 'YOUR-PASSWORD-HERE');
await page.click('button[name="verifyPassword"]');
await page.waitForLoadState('load');


// Wait for 10 seconds so we can see that we have in fact
// successfully logged in
await page.waitForTimeout(10000);
```






```
import puppeteer from 'puppeteer';


// Launch a browser and open a page
const browser = await puppeteer.launch({ headless: false });
const page = await browser.newPage();


await page.goto('https://www.yahoo.com/');


// Agree to the cookies terms, then click on the "Sign in" button
await Promise.all([page.waitForSelector('a[data-ylk*="sign-in"]'), page.click('button[name="agree"]')]);
await Promise.all([page.waitForNavigation(), page.click('a[data-ylk*="sign-in"]')]);


// Type in the username and continue forward
await page.type('input[name="username"]', 'YOUR-LOGIN-HERE');
await Promise.all([page.waitForNavigation(), page.click('input[name="signin"]')]);


// Type in the password and continue forward
await page.type('input[name="password"]', 'YOUR-PASSWORD-HERE');
await Promise.all([page.waitForNavigation(), page.click('button[name="verifyPassword"]')]);


// Wait for 10 seconds so we can see that we have in fact
// successfully logged in
await page.waitForTimeout(10000);
```




Great! If you're following along and you've replaced the placeholder credentials with your own, you should see that on the final navigated page, you're logged into your Yahoo account.


![Successfully logged into Yahoo](/assets/images/logged-in-f2dbd0d55a1fb21609322a97684c600b.jpg)


## Passing around cookies


Now that we all know how to log into a website let's try and solve a more complex problem. Let's say that we want to send 3 different emails at the same exact time, all from the **Academy** Yahoo account.


Here is an object we'll create which represents the three different emails we want to send:




```
const emailsToSend = [
    {
        to: 'alice@example.com',
        subject: 'Hello',
        body: 'This is a message.',
    },
    {
        to: 'bob@example.com',
        subject: 'Testing',
        body: 'I love the academy!',
    },
    {
        to: 'carol@example.com',
        subject: 'Apify is awesome!',
        body: 'Some content.',
    },
];
```




What we could do is log in 3 different times, then automate the sending of each email; however, this is extremely inefficient. When you log into a website, one of the main things that allows you to stay logged in and perform actions on your account is the https://docs.apify.com/academy/concepts/http-cookies.md stored in your browser. These cookies tell the website that you have been authenticated, and that you have the permissions required to modify your account.


With this knowledge of cookies, it can be concluded that we can pass the cookies generated by the code above right into each new browser context that we use to send each email. That way, we won't have to run the login flow each time.


### Retrieving cookies


First, we'll grab the cookies we generated:


* Playwright
* Puppeteer




```
// Grab the cookies from the default browser context,
// which was used to log in
const cookies = await browser.contexts()[0].cookies();
```






```
// Grab the cookies from the page used to log in
const cookies = await page.cookies();
```




Notice that in Playwright, cookies are tied to a **BrowserContext**, while in Puppeteer they are tied to a **Page**.


### Passing cookies to a new browser context


Remembering from the section above, we stored our cookies in a variable named **cookies**. These can now be directly passed into a new browser context like so:


* Playwright
* Puppeteer




```
// Create a fresh non-persistent browser context
const sendEmailContext = await browser.newContext();
// Add the cookies from the previous one to this one so that
// we'll be logged into Yahoo without having to re-do the
// logging in automation
await sendEmailContext.addCookies(cookies);
const page2 = await sendEmailContext.newPage();


// Notice that we are logged in, even though we didn't
// go through the logging in process again!
await page2.goto('https://mail.yahoo.com/');
await page2.waitForTimeout(10000);
```






```
// Create a fresh non-persistent browser context
const sendEmailContext = await browser.createIncognitoBrowserContext();
// Create a new page on the new browser context and set its cookies
// to be the same ones from the page we used to log into the website.
const page2 = await sendEmailContext.newPage();
await page2.setCookie(...cookies);


// Notice that we are logged in, even though we didn't
// go through the logging in process again!
await page2.goto('https://mail.yahoo.com/');
await page2.waitForTimeout(10000);
```




### Completing the flow


Now that passing cookies around is out of the way, we can finally complete the goal at hand and send all three of these emails at once. This can be done by mapping through **emailsToSend**, creating an array of promises where each function creates a new browser context, adds the initial cookies, and sends the email.


* Playwright
* Puppeteer




```
// Grab the cookies from the default browser context,
// which was used to log in
const cookies = await browser.contexts()[0].cookies();


await page.close();


// Create an array of promises, running the cookie passing
// and email sending logic each time
const promises = emailsToSend.map(({ to, subject, body }) => (async () => {
    // Create a fresh non-persistent browser context
    const sendEmailContext = await browser.newContext();
    // Add the cookies from the previous one to this one so that
    // we'll be logged into Yahoo without having to re-do the
    // logging in automation
    await sendEmailContext.addCookies(cookies);
    const page2 = await sendEmailContext.newPage();


    await page2.goto('https://mail.yahoo.com/');


    // Compose an email
    await page2.click('a[aria-label="Compose"]');


    // Populate the fields with the details from the object
    await page2.type('input#message-to-field', to);
    await page2.type('input[data-test-id="compose-subject"]', subject);
    await page2.type('div[data-test-id="compose-editor-container"] div[contenteditable="true"]', body);


    // Send the email
    await page2.click('button[title="Send this email"]');


    await sendEmailContext.close();
})(),
);


// Wait for all emails to be sent
await Promise.all(promises);
```






```
// Create an array of promises, running the cookie passing
// and email sending logic each time
const promises = emailsToSend.map(({ to, subject, body }) => (async () => {
    // Create a fresh non-persistent browser context
    const sendEmailContext = await browser.createIncognitoBrowserContext();
    // Create a new page on the new browser context and set its cookies
    // to be the same ones from the page we used to log into the website.
    const page2 = await sendEmailContext.newPage();
    await page2.setCookie(...cookies);


    await page2.goto('https://mail.yahoo.com/');


    // Compose an email
    await page2.click('a[aria-label="Compose"]');


    // Populate the fields with the details from the object
    await page2.type('input#message-to-field', to);
    await page2.type('input[data-test-id="compose-subject"]', subject);
    await page2.type('div[data-test-id="compose-editor-container"] div[contenteditable="true"]', body);


    // Send the email
    await page2.click('button[title="Send this email"]');


    await sendEmailContext.close();
})(),
);


// Wait for all emails to be sent
await Promise.all(promises);
```




## Final code overview


To sum up what we've built during this lesson:


1. Log into Yahoo.
2. Store the login cookies in a variable.
3. Concurrently create 3 new browser contexts and inject the cookies into each one.
4. Concurrently send 3 emails from the same account logged into in the first step.


Here's what the final code looks like:


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const emailsToSend = [
    {
        to: 'alice@example.com',
        subject: 'Hello',
        body: 'This is a message.',
    },
    {
        to: 'bob@example.com',
        subject: 'Testing',
        body: 'I love the academy!',
    },
    {
        to: 'carol@example.com',
        subject: 'Apify is awesome!',
        body: 'Some content.',
    },
];


const browser = await chromium.launch({ headless: false });
const page = await browser.newPage();


// Login logic
await page.goto('https://www.yahoo.com/');


await page.click('button[name="agree"]');
await page.waitForSelector('a:has-text("Sign in")');


await page.click('a:has-text("Sign in")');
await page.waitForLoadState('load');


await page.type('input[name="username"]', 'YOUR-LOGIN-HERE');
await page.click('input[name="signin"]');


await page.type('input[name="password"]', 'YOUR-PASSWORD-HERE');
await page.click('button[name="verifyPassword"]');
await page.waitForLoadState('load');


const cookies = await browser.contexts()[0].cookies();


await page.close();


// Email sending logic
const promises = emailsToSend.map(({ to, subject, body }) => (async () => {
    const sendEmailContext = await browser.newContext();
    await sendEmailContext.addCookies(cookies);
    const page2 = await sendEmailContext.newPage();


    await page2.goto('https://mail.yahoo.com/');


    await page2.click('a[aria-label="Compose"]');


    await page2.type('input#message-to-field', to);
    await page2.type('input[data-test-id="compose-subject"]', subject);
    await page2.type('div[data-test-id="compose-editor-container"] div[contenteditable="true"]', body);


    await page2.click('button[title="Send this email"]');


    await sendEmailContext.close();
})(),
);


await Promise.all(promises);


await browser.close();
```






```
import puppeteer from 'puppeteer';


const emailsToSend = [
    {
        to: 'alice@example.com',
        subject: 'Hello',
        body: 'This is a message.',
    },
    {
        to: 'bob@example.com',
        subject: 'Testing',
        body: 'I love the academy!',
    },
    {
        to: 'carol@example.com',
        subject: 'Apify is awesome!',
        body: 'Some content.',
    },
];


const browser = await puppeteer.launch({ headless: false });
const page = await browser.newPage();


// Login logic
await page.goto('https://www.yahoo.com/');


await Promise.all([page.waitForSelector('a[data-ylk*="sign-in"]'), page.click('button[name="agree"]')]);
await Promise.all([page.waitForNavigation(), page.click('a[data-ylk*="sign-in"]')]);


await page.type('input[name="username"]', 'YOUR-LOGIN-HERE');
await Promise.all([page.waitForNavigation(), page.click('input[name="signin"]')]);


await page.type('input[name="password"]', 'YOUR-PASSWORD-HERE');
await Promise.all([page.waitForNavigation(), page.click('button[name="verifyPassword"]')]);


const cookies = await page.cookies();
await page.close();


// Email sending logic
const promises = emailsToSend.map(({ to, subject, body }) => (async () => {
    const sendEmailContext = await browser.createIncognitoBrowserContext();
    const page2 = await sendEmailContext.newPage();
    await page2.setCookie(...cookies);


    await page2.goto('https://mail.yahoo.com/');


    await page2.click('a[aria-label="Compose"]');


    await page2.type('input#message-to-field', to);
    await page2.type('input[data-test-id="compose-subject"]', subject);
    await page2.type('div[data-test-id="compose-editor-container"] div[contenteditable="true"]', body);


    await page2.click('button[title="Send this email"]');


    await sendEmailContext.close();
})(),
);


await Promise.all(promises);


await browser.close();
```




## Next up


In the https://docs.apify.com/academy/puppeteer-playwright/common-use-cases/paginating-through-results.md, you'll learn how to paginate through results on a website.


# Paginating through results


**Learn how to paginate through results on websites that use either pagination based on page numbers or dynamic lazy loading.**


***


If you're trying to https://docs.apify.com/academy/puppeteer-playwright/executing-scripts/collecting-data.md on a website that has millions, thousands, or even hundreds of results, it is very likely that they are paginating their results to reduce strain on their back-end as well as on the users loading and rendering the content.


![Amazon pagination](data:image/png;base64,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)


## Page number-based pagination


At the time of writing this lesson, Facebook has https://github.com/orgs/facebook/repositories. By default, GitHub lists repositories in descending order based on when they were last updated (the most recently updated ones are at the top of the list).


We want to scrape the titles, links, and descriptions of all of Facebook's repositories; however, GitHub only displays 30 repositories per page. This means we need to paginate through the results. Let's start by defining some variables:




```
// This is where we'll store scraped data
const repositories = [];


// This will come handy when resolving relative links
const BASE_URL = 'https://github.com';


// We'll use this URL a couple of times within our code
const REPOSITORIES_URL = `${BASE_URL}/orgs/facebook/repositories`;
```




### Finding the last page


Going through each page is easier if we know in advance when to stop. The good news is that GitHub's pagination is upfront about the number of the last page, so the total number of pages is available to us:


![Last page number](/assets/images/github-last-page-008b2aa5e56b82a67ad682bc7398ff65.jpg)


As Facebook adds repositories over time, the number you see in your browser might be different. Let's read the number now with the following code:


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


const repositories = [];
const BASE_URL = 'https://github.com';
const REPOSITORIES_URL = `${BASE_URL}/orgs/facebook/repositories`;


const browser = await chromium.launch({ headless: false });
const firstPage = await browser.newPage();
await firstPage.goto(REPOSITORIES_URL);


const lastPageElement = firstPage.locator('a[aria-label*="Page "]:nth-last-child(2)');
const lastPageLabel = await lastPageElement.getAttribute('aria-label');
const lastPageNumber = Number(lastPageLabel.replace(/\D/g, ''));
console.log(lastPageNumber);


await browser.close();
```






```
import puppeteer from 'puppeteer';


const repositories = [];
const BASE_URL = 'https://github.com';
const REPOSITORIES_URL = `${BASE_URL}/orgs/facebook/repositories`;


const browser = await puppeteer.launch({ headless: false });
const firstPage = await browser.newPage();
await firstPage.goto(REPOSITORIES_URL);


const lastPageLabel = await firstPage.$eval(
    'a[aria-label*="Page "]:nth-last-child(2)',
    (element) => element.getAttribute('aria-label'),
);
const lastPageNumber = Number(lastPageLabel.replace(/\D/g, ''));
console.log(lastPageNumber);


await browser.close();
```




:nth-last-child


https://developer.mozilla.org/en-US/docs/Web/CSS/:nth-last-child about the `:nth-last-child` pseudo-class. It works like `:nth-child`, but starts from the bottom of the parent element's children instead of from the top.


When we run the code, it prints the total number of pages, which is `4` at the time of writing this lesson. Now let's scrape repositories from all the pages.


First, we'll add a function that can handle the data extraction for a single page and return an array of results. Then, to start, we'll run this function just for the first page:


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';
import * as cheerio from 'cheerio';


const repositories = [];
const BASE_URL = 'https://github.com';
const REPOSITORIES_URL = `${BASE_URL}/orgs/facebook/repositories`;


// Scrapes all repositories from a single page
const scrapeRepos = async (page) => {
    const $ = cheerio.load(await page.content());


    return [...$('.list-view-item')].map((item) => {
        const repoElement = $(item);
        return {
            title: repoElement.find('h4').text().trim(),
            description: repoElement.find('.repos-list-description').text().trim(),
            link: new URL(repoElement.find('h4 a').attr('href'), BASE_URL).href,
        };
    });
};


const browser = await chromium.launch({ headless: false });
const firstPage = await browser.newPage();
await firstPage.goto(REPOSITORIES_URL);


const lastPageElement = firstPage.locator('a[aria-label*="Page "]:nth-last-child(2)');
const lastPageLabel = await lastPageElement.getAttribute('aria-label');
const lastPageNumber = Number(lastPageLabel.replace(/\D/g, ''));


// Push all results from the first page to the repositories array
repositories.push(...(await scrapeRepos(firstPage)));


// Log the 30 repositories scraped from the first page
console.log(repositories);


await browser.close();
```






```
import puppeteer from 'puppeteer';
import * as cheerio from 'cheerio';


const repositories = [];
const BASE_URL = 'https://github.com';
const REPOSITORIES_URL = `${BASE_URL}/orgs/facebook/repositories`;


// Scrapes all repositories from a single page
const scrapeRepos = async (page) => {
    const $ = cheerio.load(await page.content());


    return [...$('.list-view-item')].map((item) => {
        const repoElement = $(item);
        return {
            title: repoElement.find('h4').text().trim(),
            description: repoElement.find('.repos-list-description').text().trim(),
            link: new URL(repoElement.find('h4 a').attr('href'), BASE_URL).href,
        };
    });
};


const browser = await puppeteer.launch({ headless: false });
const firstPage = await browser.newPage();
await firstPage.goto(REPOSITORIES_URL);


const lastPageLabel = await firstPage.$eval(
    'a[aria-label*="Page "]:nth-last-child(2)',
    (element) => element.getAttribute('aria-label'),
);
const lastPageNumber = Number(lastPageLabel.replace(/\D/g, ''));


// Push all results from the first page to the repositories array
repositories.push(...(await scrapeRepos(firstPage)));


// Log the 30 repositories scraped from the first page
console.log(repositories);


await browser.close();
```




If we run the code above, it outputs data about the first 30 repositories listed:




```
$ node index.js
[
  {
    title: 'react-native',
    description: 'A framework for building native applications using React',
    link: 'https://github.com/facebook/react-native'
  },
  {
    title: 'fboss',
    description: 'Facebook Open Switching System Software for controlling network switches.',
    link: 'https://github.com/facebook/fboss'
  },
  ...
]
```




### Making a request for each results page


If we click around the pagination links, we can observe that all the URLs follow certain format. For example, we can find page number 2 at `https://github.com/orgs/facebook/repositories?page=2`.


That means we could construct URL for each page if we had an array of numbers with the same range as the pages. If `lastPageNumber` is `4`, the following code creates `[0, 1, 2, 3, 4]`:




```
const array = Array(lastPageNumber + 1); // getting an array of certain size
const numbers = [...array.keys()]; // getting the keys (the actual numbers) as another array
```




Page `0` doesn't exist though and we've already scraped page `1`, so we need one more step to remove those:




```
const pageNumbers = numbers.slice(2); // removes the first two numbers
```




To have our code examples shorter, we'll squash the above to a single line of code:




```
const pageNumbers = [...Array(lastPageNumber + 1).keys()].slice(2);
```




Now let's scrape repositories for each of these numbers. We'll create promises for each request and collect results to a single `repositories` array:




```
const pageNumbers = [...Array(lastPageNumber + 1).keys()].slice(2);
const promises = pageNumbers.map((pageNumber) => (async () => {
    const paginatedPage = await browser.newPage();


    // Construct the URL by setting the ?page=... parameter to value of pageNumber
    const url = new URL(REPOSITORIES_URL);
    url.searchParams.set('page', pageNumber);


    // Scrape the page
    await paginatedPage.goto(url.href);
    const results = await scrapeRepos(paginatedPage);


    // Push results to the repositories array
    repositories.push(...results);


    await paginatedPage.close();
})(),
);
await Promise.all(promises);


// For brievity logging just the count of repositories scraped
console.log(repositories.length);
```




Scaling to hundreds of requests


Using `Promise.all()` is okay for up to ten or maybe tens of requests, but won't work well for large numbers. When scraping hundreds or even thousands of pages, it's necessary to have more robust infrastructure in place, such as a request queue.


### Final code


The code below puts all the bits together:


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';
import * as cheerio from 'cheerio';


const repositories = [];
const BASE_URL = 'https://github.com';
const REPOSITORIES_URL = `${BASE_URL}/orgs/facebook/repositories`;


// Scrapes all repositories from a single page
const scrapeRepos = async (page) => {
    const $ = cheerio.load(await page.content());


    return [...$('.list-view-item')].map((item) => {
        const repoElement = $(item);
        return {
            title: repoElement.find('h4').text().trim(),
            description: repoElement.find('.repos-list-description').text().trim(),
            link: new URL(repoElement.find('h4 a').attr('href'), BASE_URL).href,
        };
    });
};


const browser = await chromium.launch({ headless: false });
const firstPage = await browser.newPage();


await firstPage.goto(REPOSITORIES_URL);


const lastPageElement = firstPage.locator('a[aria-label*="Page "]:nth-last-child(2)');
const lastPageLabel = await lastPageElement.getAttribute('aria-label');
const lastPageNumber = Number(lastPageLabel.replace(/\D/g, ''));


// Push all results from the first page to the repositories array
repositories.push(...(await scrapeRepos(firstPage)));


await firstPage.close();


const pageNumbers = [...Array(lastPageNumber + 1).keys()].slice(2);
const promises = pageNumbers.map((pageNumber) => (async () => {
    const paginatedPage = await browser.newPage();


    // Construct the URL by setting the ?page=... parameter to value of pageNumber
    const url = new URL(REPOSITORIES_URL);
    url.searchParams.set('page', pageNumber);


    // Scrape the page
    await paginatedPage.goto(url.href);
    const results = await scrapeRepos(paginatedPage);


    // Push results to the repositories array
    repositories.push(...results);


    await paginatedPage.close();
})(),
);
await Promise.all(promises);


// For brievity logging just the count of repositories scraped
console.log(repositories.length);


await browser.close();
```






```
import puppeteer from 'puppeteer';
import * as cheerio from 'cheerio';


const repositories = [];
const BASE_URL = 'https://github.com';
const REPOSITORIES_URL = `${BASE_URL}/orgs/facebook/repositories`;


// Scrapes all repositories from a single page
const scrapeRepos = async (page) => {
    const $ = cheerio.load(await page.content());


    return [...$('.list-view-item')].map((item) => {
        const repoElement = $(item);
        return {
            title: repoElement.find('h4').text().trim(),
            description: repoElement.find('.repos-list-description').text().trim(),
            link: new URL(repoElement.find('h4 a').attr('href'), BASE_URL).href,
        };
    });
};


const browser = await puppeteer.launch({ headless: false });
const firstPage = await browser.newPage();


await firstPage.goto(REPOSITORIES_URL);


const lastPageLabel = await firstPage.$eval(
    'a[aria-label*="Page "]:nth-last-child(2)',
    (element) => element.getAttribute('aria-label'),
);
const lastPageNumber = Number(lastPageLabel.replace(/\D/g, ''));


// Push all results from the first page to the repositories array
repositories.push(...(await scrapeRepos(page)));


await firstPage.close();


const pageNumbers = [...Array(lastPageNumber + 1).keys()].slice(2);
const promises = pageNumbers.map((pageNumber) => (async () => {
    const paginatedPage = await browser.newPage();


    // Construct the URL by setting the ?page=... parameter to value of pageNumber
    const url = new URL(REPOSITORIES_URL);
    url.searchParams.set('page', pageNumber);


    // Scrape the page
    await paginatedPage.goto(url.href);
    const results = await scrapeRepos(paginatedPage);


    // Push results to the repositories array
    repositories.push(...results);


    await paginatedPage.close();
})(),
);
await Promise.all(promises);


// For brievity logging just the count of repositories scraped
console.log(repositories.length);


await browser.close();
```




At the time of writing this lesson, a summary at the top of the https://github.com/orgs/facebook/repositories claims that Facebook has 115 repositories. Whatever is the number you are seeing, it should be equal to the number you get if you run the program:




```
$ node index.js
115
```




## Lazy-loading pagination


Pagination based on page numbers is straightforward to automate, but many websites use https://en.wikipedia.org/wiki/Lazy_loading instead.


> On websites with lazy-loading pagination, if https://docs.apify.com/academy/api-scraping.md is a viable option, it is a much better approach due to reliability and performance.


Take a moment to look at and scroll through the women's clothing section https://www.aboutyou.com/c/women/clothing-20204. Notice that the items are loaded as you scroll, and that there are no page numbers. Because of how drastically different this pagination implementation is from the previous one, it also requires a different workflow to scrape.


We're going to scrape the brand and price from the first 75 results on the **About You** page linked above. Here's our basic setup:


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';


// Create an array where all scraped products will
// be pushed to
const products = [];


const browser = await chromium.launch({ headless: false });
const page = await browser.newPage();


await page.goto('https://www.aboutyou.com/c/women/clothing-20204');


await browser.close();
```






```
import puppeteer from 'puppeteer';


// Create an array where all scraped products will
// be pushed to
const products = [];


const browser = await puppeteer.launch({ headless: false });
const page = await browser.newPage();


await page.goto('https://www.aboutyou.com/c/women/clothing-20204');


await browser.close();
```




### Auto scrolling


Now, what we'll do is grab the height in pixels of a result item to have somewhat of a reference to how much we should scroll each time, as well as create a variable for keeping track of how many pixels have been scrolled.




```
// Grab the height of result item in pixels, which will be used to scroll down
const itemHeight = await page.$eval('a[data-testid*="productTile"]', (elem) => elem.clientHeight);


// Keep track of how many pixels have been scrolled down
const totalScrolled = 0;
```




Then, within a `while` loop that ends once the length of the **products** array has reached 75, we'll run some logic that scrolls down the page and waits 1 second before running again.


* Playwright
* Puppeteer




```
while (products.length < 75) {
    await page.mouse.wheel(0, itemHeight * 3);
    totalScrolled += itemHeight * 3;
    // Allow the products 1 second to load
    await page.waitForTimeout(1000);
}
```






```
while (products.length < 75) {
    await page.mouse.wheel({ deltaY: itemHeight * 3 });
    totalScrolled += itemHeight * 3;
    // Allow the products 1 second to load
    await page.waitForTimeout(1000);
}
```




This will work; however, what if we reach the bottom of the page and there are say, only 55 total products on the page? That would result in an infinite loop. Because of this edge case, we have to keep track of the constantly changing available scroll height on the page.


* Playwright
* Puppeteer




```
while (products.length < 75) {
    const scrollHeight = await page.evaluate(() => document.body.scrollHeight);


    await page.mouse.wheel(0, itemHeight * 3);
    totalScrolled += itemHeight * 3;
    // Allow the products 1 second to load
    await page.waitForTimeout(1000);


    // Data extraction login will go here


    const innerHeight = await page.evaluate(() => window.innerHeight);


    // if the total pixels scrolled is equal to the true available scroll
    // height of the page, we've reached the end and should stop scraping.
    // even if we haven't reach our goal of 75 products.
    if (totalScrolled >= scrollHeight - innerHeight) {
        break;
    }
}
```






```
while (products.length < 75) {
    const scrollHeight = await page.evaluate(() => document.body.scrollHeight);


    await page.mouse.wheel({ deltaY: itemHeight * 3 });
    totalScrolled += itemHeight * 3;
    // Allow the products 1 second to load
    await page.waitForTimeout(1000);


    // Data extraction login will go here


    const innerHeight = await page.evaluate(() => window.innerHeight);


    // if the total pixels scrolled is equal to the true available scroll
    // height of the page, we've reached the end and should stop scraping.
    // even if we haven't reach our goal of 75 products.
    if (totalScrolled >= scrollHeight - innerHeight) {
        break;
    }
}
```




Now, the `while` loop will exit out if we've reached the bottom of the page.


> Generally, you'd want to create a utility function that handles this scrolling logic instead of putting all of the code directly into the while loop.


### Extracting data


Within the loop, we can grab hold of the total number of items on the page. To avoid extracting and pushing duplicate items to the **products** array, we can use the `.slice()` method to cut out the items we've already scraped.




```
import * as cheerio from 'cheerio';


const $ = cheerio.load(await page.content());


// Grab the newly loaded items
const items = [...$('a[data-testid*="productTile"]')].slice(products.length);


const newItems = items.map((item) => {
    const elem = $(item);


    return {
        brand: elem.find('p[data-testid="brandName"]').text().trim(),
        price: elem.find('span[data-testid="finalPrice"]').text().trim(),
    };
});


products.push(...newItems);
```




### Final code


With everything completed, this is what we're left with:


* Playwright
* Puppeteer




```
import { chromium } from 'playwright';
import * as cheerio from 'cheerio';


const products = [];


const browser = await chromium.launch({ headless: false });
const page = await browser.newPage();


await page.goto('https://www.aboutyou.com/c/women/clothing-20204');


// Grab the height of result item in pixels, which will be used to scroll down
const itemHeight = await page.$eval('a[data-testid*="productTile"]', (elem) => elem.clientHeight);


// Keep track of how many pixels have been scrolled down
let totalScrolled = 0;


while (products.length < 75) {
    const scrollHeight = await page.evaluate(() => document.body.scrollHeight);


    await page.mouse.wheel(0, itemHeight * 3);
    totalScrolled += itemHeight * 3;
    // Allow the products 1 second to load
    await page.waitForTimeout(1000);


    const $ = cheerio.load(await page.content());


    // Grab the newly loaded items
    const items = [...$('a[data-testid*="productTile"]')].slice(products.length);


    const newItems = items.map((item) => {
        const elem = $(item);


        return {
            brand: elem.find('p[data-testid="brandName"]').text().trim(),
            price: elem.find('span[data-testid="finalPrice"]').text().trim(),
        };
    });


    products.push(...newItems);


    const innerHeight = await page.evaluate(() => window.innerHeight);


    // if the total pixels scrolled is equal to the true available scroll
    // height of the page, we've reached the end and should stop scraping.
    // even if we haven't reach our goal of 75 products.
    if (totalScrolled >= scrollHeight - innerHeight) {
        break;
    }
}


console.log(products.slice(0, 75));


await browser.close();
```






```
import puppeteer from 'puppeteer';
import * as cheerio from 'cheerio';


const products = [];


const browser = await puppeteer.launch({ headless: false });
const page = await browser.newPage();


await page.goto('https://www.aboutyou.com/c/women/clothing-20204');


// Grab the height of result item in pixels, which will be used to scroll down
const itemHeight = await page.$eval('a[data-testid*="productTile"]', (elem) => elem.clientHeight);


// Keep track of how many pixels have been scrolled down
let totalScrolled = 0;


while (products.length < 75) {
    const scrollHeight = await page.evaluate(() => document.body.scrollHeight);


    await page.mouse.wheel({ deltaY: itemHeight * 3 });
    totalScrolled += itemHeight * 3;
    // Allow the products 1 second to load
    await page.waitForTimeout(1000);


    const $ = cheerio.load(await page.content());


    // Grab the newly loaded items
    const items = [...$('a[data-testid*="productTile"]')].slice(products.length);


    const newItems = items.map((item) => {
        const elem = $(item);


        return {
            brand: elem.find('p[data-testid="brandName"]').text().trim(),
            price: elem.find('span[data-testid="finalPrice"]').text().trim(),
        };
    });


    products.push(...newItems);


    const innerHeight = await page.evaluate(() => window.innerHeight);


    // if the total pixels scrolled is equal to the true available scroll
    // height of the page, we've reached the end and should stop scraping.
    // even if we haven't reach our goal of 75 products.
    if (totalScrolled >= scrollHeight - innerHeight) {
        break;
    }
}


console.log(products.slice(0, 75));


await browser.close();
```




## Quick note


The examples shown in this lesson are not the only ways to paginate through websites. They are here to serve as solid examples, but don't view them as the end-all be-all of scraping paginated websites. The methods you use and the algorithms you write might differ to various degrees based on what pages you're scraping and how your specific target website implemented pagination.


## Next up


We're actively working in expanding this section of the course, so stay tuned!


# Scraping iFrames


**Extracting data from iFrames can be frustrating. In this tutorial, we will learn how to scrape information from iFrames using Puppeteer or Playwright.**


***


Getting information from inside iFrames is a known pain, especially for new developers. After spending some time on Stack Overflow, you usually find answers like jQuery's `contents()` method or native contentDocument property, which can guide you to the insides of an iframe. But still, getting the right identifiers and holding that new context is a little annoying. Fortunately, you can make everything simpler and more straightforward by scraping iFrames with Puppeteer.


## Finding the right `<iframe>`


If you are using basic methods of page objects like `page.evaluate()`, you are actually already working with frames. Behind the scenes, Puppeteer will call `page.mainFrame().evaluate()`, so most of the methods you are using with page object can be used the same way with frame object. To access frames, you need to loop over the main frame's child frames and identify the one you want to use.


As a demonstration, we'll scrape the Twitter widget iFrame from https://www.imdb.com/.




```
import puppeteer from 'puppeteer';


const browser = await puppeteer.launch();


const page = await browser.newPage();


await page.goto('https://www.imdb.com');
await page.waitForTimeout(5000); // we need to wait for Twitter widget to load


let twitterFrame; // this will be populated later by our identified frame


for (const frame of page.mainFrame().childFrames()) {
    // Here you can use few identifying methods like url(),name(),title()
    if (frame.url().includes('twitter')) {
        console.log('we found the Twitter iframe');
        twitterFrame = frame;
        // we assign this frame to myFrame to use it later
    }
}


await browser.close();
```




If it is hard to identify the iframe you want to access, don't worry. You can already use any Puppeteer method on the frame object to help you identify it, scrape it or manipulate it. You can also go through any nested frames.




```
let twitterFrame;


for (const frame of page.mainFrame().childFrames()) {
    if (frame.url().includes('twitter')) {
        for (const nestedFrame of frame.childFrames()) {
            const tweetList = await nestedFrame.$('.timeline-TweetList');
            if (tweetList) {
                console.log('We found the frame with tweet list');
                twitterFrame = nestedFrame;
            }
        }
    }
}
```




Here we used some more advanced techniques to find a nested `<iframe>`. Now when we have it assigned to our twitterFrame object, the hard work is over and we can start working with it (almost) like with a regular page object.




```
const textFeed = await twitterFrame.$$eval('.timeline-Tweet-text', (pElements) => pElements.map((elem) => elem.textContent));


for (const text of textFeed) {
    console.log(text);
    console.log('**********');
}
```




With a little more effort, we could also follow different links from the feed or even play a video, but that is not within the scope of this article. For all references about page and frame objects (and Puppeteer generally), you should study https://pub.dev/documentation/puppeteer/latest/puppeteer/Frame-class.html. New versions are released quite often, so checking the docs regularly can help you to stay on top of web scraping and automation.


from crawlee.crawlers import PlaywrightCrawler, PlaywrightCrawlingContext




async def main() -> None:
    crawler = PlaywrightCrawler(
        max_requests_per_crawl=5,  # Limit the crawl to 5 requests at most.
        headless=False,  # Show the browser window.
        browser_type='firefox',  # Use the Firefox browser.
    )


    # Define the default request handler, which will be called for every request.
    @crawler.router.default_handler
    async def request_handler(context: PlaywrightCrawlingContext) -> None:
        context.log.info(f'Processing {context.request.url} ...')


        # Extract and enqueue all links found on the page.
        await context.enqueue_links()


        # Extract data from the page using Playwright API.
        data = {
            'url': context.request.url,
            'title': await context.page.title(),
            'content': (await context.page.content())[:100],
        }


        # Push the extracted data to the default dataset.
        await context.push_data(data)


    # Run the crawler with the initial list of URLs.
    await crawler.run(['https://crawlee.dev'])


    # Export the entire dataset to a JSON file.
    await crawler.export_data('results.json')


    # Or work with the data directly.
    data = await crawler.get_data()
    crawler.log.info(f'Extracted data: {data.items}')
# Introduction


Copy for LLM


The Apify SDK for Python is the official library for creating [Apify Actors](https://docs.apify.com/platform/actors) using Python.


[Run on](https://console.apify.com/actors/HH9rhkFXiZbheuq1V?runConfig=eyJ1IjoiRWdQdHczb2VqNlRhRHQ1cW4iLCJ2IjoxfQ.eyJpbnB1dCI6IntcImNvZGVcIjpcImltcG9ydCBhc3luY2lvXFxuXFxuaW1wb3J0IGh0dHB4XFxuZnJvbSBiczQgaW1wb3J0IEJlYXV0aWZ1bFNvdXBcXG5cXG5mcm9tIGFwaWZ5IGltcG9ydCBBY3RvclxcblxcblxcbmFzeW5jIGRlZiBtYWluKCkgLT4gTm9uZTpcXG4gICAgYXN5bmMgd2l0aCBBY3RvcjpcXG4gICAgICAgIGFjdG9yX2lucHV0ID0gYXdhaXQgQWN0b3IuZ2V0X2lucHV0KCkgb3Ige31cXG4gICAgICAgIHVybCA9IGFjdG9yX2lucHV0LmdldCgndXJsJywgJ2h0dHBzOi8vYXBpZnkuY29tJylcXG4gICAgICAgIGFzeW5jIHdpdGggaHR0cHguQXN5bmNDbGllbnQoKSBhcyBjbGllbnQ6XFxuICAgICAgICAgICAgcmVzcG9uc2UgPSBhd2FpdCBjbGllbnQuZ2V0KHVybClcXG4gICAgICAgIHNvdXAgPSBCZWF1dGlmdWxTb3VwKHJlc3BvbnNlLmNvbnRlbnQsICdodG1sLnBhcnNlcicpXFxuICAgICAgICBkYXRhID0ge1xcbiAgICAgICAgICAgICd1cmwnOiB1cmwsXFxuICAgICAgICAgICAgJ3RpdGxlJzogc291cC50aXRsZS5zdHJpbmcgaWYgc291cC50aXRsZSBlbHNlIE5vbmUsXFxuICAgICAgICB9XFxuICAgICAgICBhd2FpdCBBY3Rvci5wdXNoX2RhdGEoZGF0YSlcXG5cXG5cXG5pZiBfX25hbWVfXyA9PSAnX19tYWluX18nOlxcbiAgICBhc3luY2lvLnJ1bihtYWluKCkpXFxuXCJ9Iiwib3B0aW9ucyI6eyJidWlsZCI6ImxhdGVzdCIsImNvbnRlbnRUeXBlIjoiYXBwbGljYXRpb24vanNvbjsgY2hhcnNldD11dGYtOCIsIm1lbW9yeSI6MTAyNCwidGltZW91dCI6MTgwfX0.qbLvqUlgBmTHKyE5wQ_XXZSI_6-WbQQ0L-BHrVgRp-Q\&asrc=run_on_apify)


```
import asyncio


import httpx
from bs4 import BeautifulSoup


from apify import Actor




async def main() -> None:
    async with Actor:
        actor_input = await Actor.get_input() or {}
        url = actor_input.get('url', 'https://apify.com')
        async with httpx.AsyncClient() as client:
            response = await client.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        data = {
            'url': url,
            'title': soup.title.string if soup.title else None,
        }
        await Actor.push_data(data)




if __name__ == '__main__':
    asyncio.run(main())
```


## What are Actors?[](#what-are-actors)


Actors are serverless cloud programs capable of performing tasks in a web browser, similar to what a human can do. These tasks can range from simple operations, such as filling out forms or unsubscribing from services, to complex jobs like scraping and processing large numbers of web pages.


Actors can be executed locally or on the [Apify platform](https://docs.apify.com/platform/), which provides features for running them at scale, monitoring, scheduling, and even publishing and monetizing them.


If you're new to Apify, refer to the Apify platform documentation to learn [what Apify is](https://docs.apify.com/platform/about).


## Quick start[](#quick-start)


This section provides a quick start guide for creating and running Actors.


### Creating Actors[](#creating-actors)


To create and run Actors using the Apify Console, see the [Console documentation](https://docs.apify.com/platform/console).


For creating and running Python Actors locally, refer to the documentation for [creating and running Python Actors locally](https://docs.apify.com/sdk/python/sdk/python/docs/overview/running-actors-locally.md).


### Guides[](#guides)


Integrate the Apify SDK with popular web scraping libraries by following these guides:


* [BeautifulSoup with HTTPX](https://docs.apify.com/sdk/python/sdk/python/docs/guides/beautifulsoup-httpx.md)
* [Crawlee](https://docs.apify.com/sdk/python/sdk/python/docs/guides/crawlee.md)
* [Playwright](https://docs.apify.com/sdk/python/sdk/python/docs/guides/playwright.md)
* [Selenium](https://docs.apify.com/sdk/python/sdk/python/docs/guides/selenium.md)
* [Scrapy](https://docs.apify.com/sdk/python/sdk/python/docs/guides/scrapy.md)


### Usage concepts[](#usage-concepts)


For a deeper understanding of the Apify SDK's features, refer to the **Usage concepts** section in the sidebar. Key topics include:


* [Actor lifecycle](https://docs.apify.com/sdk/python/sdk/python/docs/concepts/actor-lifecycle.md)
* [Working with storages](https://docs.apify.com/sdk/python/sdk/python/docs/concepts/storages.md)
* [Handling Actor events](https://docs.apify.com/sdk/python/sdk/python/docs/concepts/actor-events.md)
* [Using proxies](https://docs.apify.com/sdk/python/sdk/python/docs/concepts/proxy-management.md)


## Installing the Apify SDK separately[](#installing-the-apify-sdk-separately)


When creating an Actor using the Apify CLI, the Apify SDK for Python is installed automatically. If you want to install it independently, use the following command:


```
pip install apify
```


If your goal is not to develop Apify Actors but to interact with the Apify API from Python, consider using the [Apify API client for Python](https://docs.apify.com/api/client/python) directly.


# Running Actors locally


Copy for LLM


In this page, you'll learn how to create and run Apify Actors locally on your computer.


## Requirements[](#requirements)


The Apify SDK requires Python version 3.10 or above to run Python Actors locally.


## Creating your first Actor[](#creating-your-first-actor)


To create a new Apify Actor on your computer, you can use the [Apify CLI](https://docs.apify.com/cli), and select one of the [Python Actor templates](https://apify.com/templates/categories/python).


For example, to create an Actor from the Python SDK template, you can use the [`apify create`](https://docs.apify.com/cli/docs/reference#apify-create-actorname) command.


```
apify create my-first-actor --template python-start
```


This will create a new folder called `my-first-actor`, download and extract the "Getting started with Python" Actor template there, create a virtual environment in `my-first-actor/.venv`, and install the Actor dependencies in it.


## Running the Actor[](#running-the-actor)


To run the Actor, you can use the [`apify run`](https://docs.apify.com/cli/docs/reference#apify-run) command:


```
cd my-first-actor
apify run
```


This will activate the virtual environment in `.venv` (if no other virtual environment is activated yet), then start the Actor, passing the right environment variables for local running, and configure it to use local storages from the `storage` folder.


The Actor input, for example, will be in `storage/key_value_stores/default/INPUT.json`.


## Adding dependencies[](#adding-dependencies)


Adding dependencies into the Actor is simple.


First, add them in the [`requirements.txt`](https://pip.pypa.io/en/stable/reference/requirements-file-format/) file in the Actor source folder.


Then activate the virtual environment in `.venv`:


* Linux / macOS
* Windows


```
source .venv/bin/activate
```


```
.venv\Scripts\activate
```


Then install the dependencies:


```
python -m pip install -r requirements.txt
```


# Actor structure


Copy for LLM


All Python Actor templates follow the same structure.


The `.actor/` directory contains the [Actor configuration](https://docs.apify.com/platform/actors/development/actor-config), such as the Actor's definition and input schema, and the Dockerfile necessary to run the Actor on the Apify platform.


The Actor's runtime dependencies are specified in the `requirements.txt` file, which follows the [standard requirements file format](https://pip.pypa.io/en/stable/reference/requirements-file-format/).


The Actor's source code is in the `src/` folder. This folder contains two important files: `main.py`, which contains the main function of the Actor, and `__main__.py`, which is the entrypoint of the Actor package, setting up the Actor [logger](https://docs.apify.com/sdk/python/sdk/python/docs/concepts/logging.md) and executing the Actor's main function via [`asyncio.run`](https://docs.python.org/3/library/asyncio-runner.html#asyncio.run).


* \_\_main.py\_\_
* main.py


```
import asyncio


from .main import main


if __name__ == '__main__':
    asyncio.run(main())
```


```
from apify import Actor




async def main() -> None:
    async with Actor:
        actor_input = await Actor.get_input()
        Actor.log.info('Actor input: %s', actor_input)
        await Actor.set_value('OUTPUT', 'Hello, world!')
```


If you want to modify the Actor structure, you need to make sure that your Actor is executable as a module, via `python -m src`, as that is the command started by `apify run` in the Apify CLI. We recommend keeping the entrypoint for the Actor in the `src/__main__.py` file.


Skip to main content
Need a proxy solution? Try ScrapeOps and get 1,000 free requests here, or compare all proxy providers here!

ScrapeOps
Solutions
DocsProxy ComparisonBlogGuides
Login
Signup
 Make Playwright Undetectable 

How To Make Playwright Undetectable
Websites deploy a variety of techniques to detect and block automated tools, ranging from simple measures like monitoring IP addresses and user-agent strings to more advanced methods involving behavioral analysis and fingerprinting. As anti-bot measures become more sophisticated, simply using Playwright is no longer enough to ensure seamless and undetected interactions with web pages.
In this article, we will learn how to make Playwright undetectable.
* TLDR - How To Make Playwright Undetectable
* Understanding Website Bot Detection Mechanisms
* How To Make Playwright Undetectable To Anti-Bots
* Strategies To Make Playwright Undetectable
* Testing Your Playwright Scraper
* Handling Errors and Captchas
* Why Make Playwright Undetectable
* Benefits of Making Playwright Undetectable
* Case Study: Evading Playwright Detection on G2
* Best Practices and Considerations
* Conclusion
* More Playwright Web Scraping Guides
Need help scraping the web?
Then check out ScrapeOps, the complete toolkit for web scraping.

Proxy Manager

Scraper Monitoring

Job Scheduling
________________


TLDR: How to Make Playwright Undetectable
To make Playwright undetectable, you must customize browser settings, spoof user agent strings, disable automation flags, and use realistic interaction patterns. By doing this, you can reduce the likelihood of websites detecting your automated scripts.
We can see an example of implementing this in a Playwright script:
const { chromium } = require('playwright');


(async () => {
   const browser = await chromium.launch({
       headless: false,
       args: ['--disable-blink-features=AutomationControlled']
   });
   const context = await browser.newContext({
       userAgent: 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
       viewport: { width: 1280, height: 720 },
       deviceScaleFactor: 1,
   });
   const page = await context.newPage();
   await page.goto('https://cnn.com');


   await browser.close();
})();
* Disable Automation Flags: We can use the --disable-blink-features=AutomationControlled argument to remove the automation flag, which can help prevent detection.
* Set Realistic Viewport and Device Characteristics: Configure the browser context to match typical user settings. We will set the viewport to a typical display so we don't raise any flags.
* Modify User Agent String: We can use different user agent strings to appear as a more commonly used browser.
________________


Understanding Website Bot Detection Mechanisms
To effectively make Playwright undetectable, it's crucial to understand the techniques websites use to identify bots and automated scripts. This knowledge will help you implement strategies to avoid detection.
1. IP Analysis and Rate Limiting
Websites monitor the number of requests coming from a single IP address over a specific period. The IP may be flagged as a potential bot if the number of requests exceeds a certain threshold.
Rate limiting is a common method to control traffic and reduce the likelihood of automated abuse. To mitigate this, you can use rotating proxies to distribute requests across multiple IP addresses.
2. Browser Fingerprinting
Browser fingerprinting involves collecting information about a user's browser and device configuration to create a unique identifier. This includes data like browser version, operating system, installed plugins, screen resolution, and more. By comparing this fingerprint to known patterns, websites can identify automated scripts. To counteract this, you can modify your browser's fingerprint to mimic a real user more closely.
3. Checking for Headless Browser Environments
Many bots run in headless browser environments, which lack a graphical user interface. Websites can detect headless browsers by looking for specific flags or characteristics unique to headless modes. Running Playwright in non-headless mode and spoofing characteristics typical of full browsers can help bypass this detection method.
4. Analyzing User Behavior Patterns
Websites analyze user interactions to detect unusual patterns indicative of automation. This includes rapid clicking, consistent navigation paths, and uniform mouse movements. To avoid detection, you can simulate human-like interactions with realistic delays and erratic mouse movements, making your automation behavior appear more natural.
Several measures can block your access if a website detects your automation script. You may:
* Encounter CAPTCHA challenges that interrupt your script's flow,
* IP blocking that prevents further requests from your address,
* account suspension if the site requires user accounts, and
* limited functionality that reduces the effectiveness of your automation efforts.
Understanding these risks is crucial for maintaining undetected and efficient Playwright scripts.
________________


How To Make Playwright Undetectable To Anti-Bots
Different techniques can be used to make your Playwright undetectable to websites’ anti-bot mechanisms.
Here are some of the most commonly used methods.
1. Setting User-Agent Strings
The User-Agent request header is a characteristic string that lets servers and network peers identify the application, operating system, vendor, and/or version of the requestingbrowser.
We can modify the user-agent string to mimic popular browsers, ensuring it matches the browser version and operating system. We can set up the browser user agent string like this in Playwright
const context = await browser.newContext({
   userAgent: 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
});
2. Enabling WebGL and Hardware Acceleration
We can ensure WebGL and hardware acceleration are enabled to replicate the typical capabilities of a human-operated web browser by specifying specific arguments when launching our Chromium browser.
const browser = await chromium.launch({
   args: [
       '--enable-webgl',
       '--use-gl=swiftshader',
       '--enable-accelerated-2d-canvas'
   ]
});
3. Mimicking Human-Like Browser Environments
We can configure our browser context to reflect common user settings, such as viewport size, language, and time zone.
const context = await browser.newContext({
   viewport: { width: 1280, height: 720 },
   locale: 'en-US',
   timezoneId: 'America/New_York'
});
4. Using Rotating Residential Proxies
Residential proxies are a type of proxy server that uses IP addresses assigned by Internet Service Providers (ISPs) to regular household users. These proxies differ from datacenter proxies, which use IP addresses provided by cloud service providers and are more easily detectable and blockable by websites. You can employ residential proxies that use real IP addresses from ISPs, making it harder for websites to detect and block your requests.
You can also use rotating residential proxies to change your IP addresses regularly to avoid multiple requests from the same IP, which can raise red flags.
const browser = await chromium.launch({
   proxy: {
       server: 'http://myproxy.com:3128',
       username: 'usr',
       password: 'pwd'
   }
});
If you want a better understanding of residential proxies, consider exploring the article Residential Proxies Explained: How You Can Scrape Without Getting Blocked.
5. Mimicking Human Behavior
We can mimic human behavior when interacting with a website to avoid detection. Introducing realistic delays, random mouse movements and natural scrolling is essential. Here are some techniques to achieve this:
const { chromium } = require("playwright");


(async () => {
 // Launch a new browser instance
 const browser = await chromium.launch({ headless: false }); // Set headless to true if you don't need to see the browser
 const context = await browser.newContext();
 const page = await context.newPage();


 // Navigate to the target website and wait for the page to load completely
 await page.goto("https://www.saucedemo.com/", {
   waitUntil: "domcontentloaded",
 });
 // Function to generate random delays between 50ms and 200ms
 const getRandomDelay = () => Math.random() * (200 - 50) + 50;


 // Type into the username field with a random delay to simulate human typing
 await page.type("#user-name", "text", { delay: getRandomDelay() });


 // Type into the password field with a random delay to simulate human typing
 await page.type("#password", "text", { delay: getRandomDelay() });


 // Click the login button with a random delay to simulate human click
 await page.click("#login-button", { delay: getRandomDelay() });


 // Scroll down the page to simulate reading content after login
 await page.evaluate(() => {
   window.scrollBy(0, window.innerHeight);
 });


 // Introduce random mouse movements to simulate human interaction
 await page.mouse.move(Math.random() * 800, Math.random() * 600);


 // Close the browser
 await browser.close();
})();


6. Using Playwright with Real Browsers
We can run Playwright with full browser environments rather than headless modes to reduce the likelihood of detection. You can launch your Chromium or Firefox browser by following the script below:
const browser = await chromium.launch({ headless: false });
const browser = await firefox.launch({ headless: false });
________________


Strategies To Make Playwright Undetectable
You can use one or more of the following strategies to make your Playwright scraper undetectable. Each strategy can be tailored to enhance your scraping effectiveness while minimizing detection risks.
1. Use Playwright Extra With Residential Proxies
Playwright Extra is a library that helps disguise Playwright scripts to make them appear as human-like as possible. It modifies browser behavior to evade detection techniques like fingerprinting and headless browser detection.
This library, originally developed for Puppeteer, is known as puppeteer-extra-plugin-stealth and is designed to enhance browser automation by minimizing detection. Below, we will see it in action with an example demonstrating how to use this plugin.
* We will start by installing the required libraries:
npm install playwright-extra
npm install puppeteer-extra-plugin-stealth
* We will navigate to bot.sannysoft, it is a website designed to test and analyze how well your browser or automation setup can evade detection by anti-bot systems.
We will also add a residential proxy to enhance our setup's stealth capabilities further. This comprehensive approach will help us ensure that our automated interactions closely mimic real user behavior, reducing the likelihood of detection.
const { chromium } = require("playwright-extra");
const stealth = require("puppeteer-extra-plugin-stealth")();


chromium.use(stealth);
// Replace with your residential proxy address and port


chromium.launch({ headless: true,
   args: [ '--proxy-server=http://your-residential-proxy-address:port'] }).then(async (browser) => {
 const page = await browser.newPage();


 console.log("Testing the stealth plugin..");
 await page.goto("https://bot.sannysoft.com", { waitUntil: "networkidle" });
 await page.screenshot({ path: "stealth.png", fullPage: true });


 console.log("All done, check the screenshot.");
 await browser.close();
});


Below, we can see the results from our Playwright script, which uses the stealth plugin and a residential proxy. This setup has successfully passed most of the website's tests.
 stealth-croped 

In contrast, a Playwright script without any evasion mechanisms fails several tests presented by the website.
 Vanilla Playwright script 

The comparison highlights the effectiveness of using the stealth plugin and residential proxy for evading detection.
Residential and mobile proxies are typically more expensive than data center proxies because they use IP addresses from real residential users or mobile devices. ISPs charge for the bandwidth used, making it crucial to optimize resource usage.
2. Use Hosted Fortified Version of Playwright
Hosted and fortified versions of Playwright are optimized for scraping and include built-in anti-bot bypass mechanisms, including rotating residential proxies.
Below, we will explore the integration of Brightdata with Playwright, a leading provider of hosted web scraping solutions and proxies.
const playwright = require("playwright");


// Here we will input the Username and Password we get from Brightdata
const { AUTH = "USER:PASS", TARGET_URL = "https://bot.sannysoft.com" } =
 process.env;


async function scrape(url = TARGET_URL) {
 if (AUTH == "USER:PASS") {
   throw new Error(
     `Provide Scraping Browsers credentials in AUTH` +
       ` environment variable or update the script.`
   );
 }


 // This is our Brightdata Proxy URL endpoint that we can use
 const endpointURL = `wss://${AUTH}@brd.superproxy.io:9222`;


 /**
  * This is where the magic happens. Here, we connect to a remote browser instance fortified by Brightdata.
  * They utlulize all the methods we talked about and more to make the Chromium instance evade detection effectively.
  */


 const browser = await playwright.chromium.connectOverCDP(endpointURL);


 // Now we have instance of chromium browser that is fortfied we can navigate to our desired url.


 try {
   console.log(`Connected! Navigating to ${url}...`);
   const page = await browser.newPage();
   await page.goto(url, { timeout: 2 * 60 * 1000 });
   console.log(`Navigated! Scraping page content...`);
   const data = await page.content();
   console.log(`Scraped! Data: ${data}`);
 } finally {
   await browser.close();
 }
}


if (require.main == module) {
 scrape().catch((error) => {
   console.error(error.stack || error.message || error);
   process.exit(1);
 });
}
* The script starts by checking if the necessary authentication details for the Brightdata proxy service are provided. If not, it throws an error.
* Next, it connects to a Chromium browser instance through the Brightdata proxy using Playwright’s connectOverCDP method.
* Brightdata fortifies this connection, which means it’s configured to evade common bot detection techniques.
* Once connected, the script navigates to the target URL and retrieves the page content.
Even though this is a great solution, it is very expensive to implement on a large scale.
3. Fortify Playwright Yourself
Fortifying Playwright involves making your automated browser behavior appear more like a real user and less like a bot.
By implementing different techniques, you can make your Playwright scripts more robust and stealthy, reducing the likelihood of being detected as a bot by websites.
Let us visit bot.sannysoft with a Playwright script that we fortified:
const { chromium } = require("playwright");


(async () => {
 const browser = await chromium.launch({
   // Set arguments to mimic real browser
   args: [
     "--disable-blink-features=AutomationControlled",
     "--disable-extensions",
     "--disable-infobars",
     "--enable-automation",
     "--no-first-run",
     "--enable-webgl",
   ],
   ignoreHTTPSErrors: true,
   headless: false,
 });


 const context = await browser.newContext({
   // Emulate user behavior by setting up userAgent and viewport
   userAgent:
     "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36",
   viewport: { width: 1280, height: 720 },
   geolocation: { longitude: 12.4924, latitude: 41.8902 }, // Example: Rome, Italy
   permissions: ["geolocation"],
 });


 const page = await context.newPage();


 await page.goto("https://bot.sannysoft.com");


 await page.screenshot({ path: "screenshot.png" });


 await browser.close();
})();
Flag                                                 
	Description
	-disable-blink-features=AutomationControlled
	This flag disables the JavaScript property navigator.webdriver from being true. Some websites use this property to detect if the browser is being controlled by automation tools like Puppeteer or Playwright.
	--disable-extensions
	This flag disables all Chrome extensions. Extensions can interfere with the behavior of the browser and the webpage, so it’s often best to disable them when automating browser tasks.
	--disable-infobars
	This flag disables infobars on the top of the browser window, such as the “Chrome is being controlled by automated test software” infobar.
	--enable-automation
	This flag enables automation-related APIs in Chrome.
	--no-first-run
	This flag skips the first-run experience in Chrome, which is a series of setup steps shown the first time Chrome is launched.
	--enable-webgl
	This flag enables WebGL in a browser.
	We can now run our fortified Playwright script to get an output of.
 Fortified Playwright Script 

As demonstrated, we got the same result as we did with the Playwright-stealth library. The configuration of our Playwright script can be tailored based on the specific website and the degree of its anti-bot defenses.
We can also enhance our applications by adding rotating residential proxies to make it harder for bot detectors to spot us.
By experimenting with various strategies tailored to our unique requirements, we can achieve an optimal setup robust enough to bypass the protective measures of a website we want to scrape.
4. Leverage ScrapeOps Proxy to Bypass Anti-Bots
Depending on your use case, leveraging a proxy solution with built-in anti-bot bypasses can be a simpler and more cost-effective approach compared to manually optimizing your Playwright scrapers. This way, you can focus on extracting the data you need without worrying about making your bot undetectable.
ScrapeOps Proxy Aggregator is another proxy solution to make Puppeteer undetectable to bot detectors even complex one's like cloudflare.
* Sign Up for ScrapeOps Proxy Aggregator: Start by signing up for ScrapeOps Proxy Aggregator and obtain your API key.
* Installation and Configuration: Install Playwright and set up your scraping environment.
* Initialize Playwright with ScrapeOps Proxy: Set up Playwright to use ScrapeOps Proxy Aggregator by providing the proxy URL and authentication credentials.
Below, we will see how to use Playwright with ScrapeOps to get the ultimate web scraping application.
const { chromium } = require("playwright");


// Replace with your residential proxy details
const PROXY_SERVER = "http://residential-proxy.scrapeops.io:8181";
const PROXY_USERNAME = "scrapeops";
const PROXY_PASSWORD = "YOUR-SCRAPEOPS-RESIDENTIAL-PROXY-API-KEY";


(async () => {
   // Launch browser with proxy settings
   const browser = await chromium.launch({
       headless: false,
       args: [`--proxy-server=${PROXY_SERVER}`], // Only the proxy server URL
   });


   // Create a browser context with authentication for the proxy
   const context = await browser.newContext({
       proxy: {
           server: PROXY_SERVER,
           username: PROXY_USERNAME,
           password: PROXY_PASSWORD
       }
   });


   const page = await context.newPage();
  
   console.log("Testing residential proxy...");
   await page.goto("https://bot.sannysoft.com", { waitUntil: "networkidle" });
  
   await page.screenshot({ path: "screenshot.png" });
   console.log("Screenshot saved as 'screenshot.png'");


   await browser.close();
})();
ScrapeOps offers a variety of bypass levels to navigate through different anti-bot systems.
These bypasses range from generic ones, designed to handle a wide spectrum of anti-bot systems, to more specific ones tailored for platforms like Cloudflare and Datadome.
Each bypass level is crafted to tackle a certain complexity of anti-bot mechanisms, making it possible to scrape data from websites with varying degrees of bot protection.
Bypass Level
	Description
	Typical Use Case
	generic_level_1
	Simple anti-bot bypass
	Low-level bot protections
	generic_level_2
	Advanced configuration for common bot detections
	Medium-level bot protections
	generic_level_3
	High complexity bypass for tough detections
	High-level bot protections
	cloudflare
	Specific for Cloudflare protected sites
	Sites behind Cloudflare
	incapsula
	Targets Incapsula protected sites
	Sites using Incapsula for security
	perimeterx
	Bypasses PerimeterX protections
	Sites with PerimeterX security
	datadome
	Designed for DataDome protected sites
	Sites using DataDome
	With ScrapeOps, you don’t have to worry about maintaining your script’s anti-bot bypassing techniques, as it handles these complexities for you.
________________


Testing Your Playwright Scraper
To ensure your Playwright scraper is effectively fortified against bot detection, it’s essential to test it using fingerprinting tools.
Earlier, we demonstrated using websites like bot.sannysoft.com to test your Playwright code stealthiness.
Another excellent tool for this purpose is bot.incolumitas.com, which provides a comprehensive analysis of your browser's fingerprint and highlights potential leaks that might reveal your browser identity.
Without Fortification
First, let's look at a basic Playwright script without any fortification techniques. We will test this script using the tools mentioned to get a behavioralClassificationScore.
This score will allow us to determine if we are being detected as human or a bot.
const playwright = require("playwright");


function sleep(ms) {
 return new Promise((resolve) => setTimeout(resolve, ms));
}


/**
* This is obviously not the best approach to
* solve the bot challenge. Here comes your creativity.
*
* @param {*} page
*/
async function solveChallenge(page) {
 // wait for form to appear on page
 await page.waitForSelector("#formStuff");
 // overwrite the existing text by selecting it
 // with the mouse with a triple click
 const userNameInput = await page.$('[name="userName"]');
 await userNameInput.click({ clickCount: 3 });
 await userNameInput.type("bot3000");
 // same stuff here
 const emailInput = await page.$('[name="eMail"]');
 await emailInput.click({ clickCount: 3 });
 await emailInput.type("bot3000@gmail.com");
 await page.selectOption('[name="cookies"]', "I want all the Cookies");
 await page.click("#smolCat");
 await page.click("#bigCat");
 // submit the form
 await page.click("#submit");


 // handle the dialog
 page.on("dialog", async (dialog) => {
   console.log(dialog.message());
   await dialog.accept();
 });


 // wait for results to appear
 await page.waitForSelector("#tableStuff tbody tr .url");
 // just in case
 await sleep(100);


 // now update both prices
 // by clicking on the "Update Price" button
 await page.waitForSelector("#updatePrice0");
 await page.click("#updatePrice0");
 await page.waitForFunction(
   '!!document.getElementById("price0").getAttribute("data-last-update")'
 );


 await page.waitForSelector("#updatePrice1");
 await page.click("#updatePrice1");
 await page.waitForFunction(
   '!!document.getElementById("price1").getAttribute("data-last-update")'
 );


 // now scrape the response
 let data = await page.evaluate(function () {
   let results = [];
   document.querySelectorAll("#tableStuff tbody tr").forEach((row) => {
     results.push({
       name: row.querySelector(".name").innerText,
       price: row.querySelector(".price").innerText,
       url: row.querySelector(".url").innerText,
     });
   });
   return results;
 });


 console.log(data);
}


(async () => {
 const browser = await playwright["chromium"].launch({
   headless: false,
   args: ["--start-maximized"],
 });
 const context = await browser.newContext({ viewport: null });
 const page = await context.newPage();


 await page.goto("https://bot.incolumitas.com/");


 await solveChallenge(page);


 await sleep(6000);


 const new_tests = JSON.parse(
   await page.$eval("#new-tests", (el) => el.textContent)
 );
 const old_tests = JSON.parse(
   await page.$eval("#detection-tests", (el) => el.textContent)
 );


 console.log(new_tests);
 console.log(old_tests);


 //await page.close();
 await browser.close();
})();
 Behavioral Classification Score Vanilla Playwright 

As you can see from the result above, we get a behavioralClassificationScore of 22% human , indicating it is likely to be detected as a bot.
This low score reflects that the script's behavior does not closely mimic that of a real human user, making it more susceptible to bot detection mechanisms.
With Fortification
Now, let's see a fortified version of the Playwright script:
We will be implementing different enhancements to make the script's behavior appear more like that of a real human user.
Here are the key fortification techniques implemented:
1. Random Mouse Movements :
* Adding random mouse movements simulates the unpredictable nature of human mouse behavior.
* This is achieved by moving the mouse cursor to random positions on the screen with slight pauses between movements.
2. Delays Between Actions :
* Introducing delays between key presses, clicks, and dialog interactions mimics the natural time a human takes to perform these actions.
* This includes delays between typing characters, clicking elements, and handling dialog boxes.
3. User Agent and Context Configuration :
* Setting a common user agent helps in blending the browser's identity with real user patterns.
* The browser context is configured with typical user settings, such as geolocation, permissions, and locale.
4. Browser Launch Arguments :
* Modifying the browser launch arguments to disable features that can reveal automation, such as --disable-blink-features=AutomationControlled, --disable-extensions, --disable-infobars, --enable-automation, and --no-first-run.
* These arguments help make the browser appear more like a standard user browser.
Here's the fortified version of the Playwright script:
const playwright = require("playwright");


function sleep(ms) {
return new Promise((resolve) => setTimeout(resolve, ms));
}


async function randomMouseMove(page) {
for (let i = 0; i < 5; i++) {
await page.mouse.move(Math.random() _ 1000, Math.random() _ 1000);
await sleep(100);
}
}


async function solveChallenge(page) {
await page.evaluate(() => {
const form = document.querySelector("#formStuff");
form.scrollIntoView();
});
await sleep(1000);


const userNameInput = await page.$('[name="userName"]');
await userNameInput.click({ clickCount: 3 });
for (let char of "bot3000") {
await userNameInput.type(char);
await sleep(500);
}


const emailInput = await page.$('[name="eMail"]');
await emailInput.click({ clickCount: 3 });
for (let char of "bot3000@gmail.com") {
await emailInput.type(char);
await sleep(600);
}


await page.selectOption('[name="cookies"]', "I want all the Cookies");
await sleep(1200);
await page.click("#smolCat");


await sleep(900);
await page.click("#submit");


page.on("dialog", async (dialog) => {
console.log(dialog.message());
await sleep(2000); // add delay before accepting the dialog
await dialog.accept();
});


await page.waitForSelector("#tableStuff tbody tr .url");
await sleep(100);


await page.waitForSelector("#updatePrice0");
await page.click("#updatePrice0");
await page.waitForFunction(
'!!document.getElementById("price0").getAttribute("data-last-update")'
);
await sleep(1000);


await page.waitForSelector("#updatePrice1");
await page.click("#updatePrice1");
await page.waitForFunction(
'!!document.getElementById("price1").getAttribute("data-last-update")'
);
await sleep(800);


let data = await page.evaluate(function () {
let results = [];
document.querySelectorAll("#tableStuff tbody tr").forEach((row) => {
results.push({
name: row.querySelector(".name").innerText,
price: row.querySelector(".price").innerText,
url: row.querySelector(".url").innerText,
});
});
return results;
});


console.log(data);
}


(async () => {
const browser = await playwright["chromium"].launch({
headless: false,
args: [
"--start-maximized",
"--disable-blink-features=AutomationControlled",
"--disable-extensions",
"--disable-infobars",
"--enable-automation",
"--no-first-run",
],
});
const context = await browser.newContext({
viewport: null,
userAgent:
"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3",
geolocation: { longitude: 12.4924, latitude: 41.8902 },
permissions: ["geolocation"],
locale: "en-US",
});
const page = await context.newPage();


await page.goto("https://bot.incolumitas.com/");


await randomMouseMove(page); // add random mouse movements before solving the challenge


await solveChallenge(page);


await sleep(6000);


const new_tests = JSON.parse(
await page.$eval("#new-tests", (el) => el.textContent)
 );
 const old_tests = JSON.parse(
   await page.$eval("#detection-tests", (el) => el.textContent)
);


console.log(new_tests);
console.log(old_tests);


await browser.close();
})();


 InColumitas Test Score 

We’ve made several enhancements to a script that interacts with a webpage using the Playwright library. The goal was to make the script’s behavior appear more like that of a real human user on the webpage.
* Firstly, we introduced delays between key presses and clicks to simulate the time a real user might take to perform these actions. We also added random mouse movements across the page to mimic a user’s cursor movements.
* Secondly, we added a delay before accepting any dialog boxes that appear on the webpage, simulating the time a real user might take to read and respond to the dialog.
* Lastly, we set a common user agent for the browser to make it appear more like a typical user’s browser.
These enhancements significantly improved the script’s ability to mimic a real user’s behavior. In our case, our behavioralClassificationScore comes out to be 70% human, which is quite good considering the simplicity of our script.
Our script is now less likely to be detected as a bot by the webpage’s bot detection mechanisms.
Please note that this is a basic approach, and some websites may have more sophisticated bot detection mechanisms. You might need to consider additional strategies like integrating residential and mobile proxies.
________________


Handling Errors and CAPTCHAs
Playwright is a powerful tool for automating browser interactions, but like any tool, it’s not immune to errors or challenges such as CAPTCHAs.
In this chapter, we’ll explore how to handle errors and CAPTCHAs when using Playwright.
Handling Errors
Errors can occur at any point during the execution of your script. Handling these errors is important so your script can recover or exit cleanly.
Here’s an example of how you might handle errors in Playwright:
const playwright = require('playwright');
(async () => {
try {
const browser = await playwright['chromium'].launch();
const context = await browser.newContext();
const page = await context.newPage();


   await page.goto('https://cnn.com');


   // We can perform our task here


   await browser.close();


} catch (error) {
console.error('An error occurred:', error);
}
})();


We can also modify the script to retry the operation a specified number of times. If the operation fails repeatedly, the script will log an error message and terminate.
const playwright = require('playwright');


const MAX_RETRIES = 5; // Maximum number of retries


async function run() {
let retries = 0;


while (retries < MAX_RETRIES) {
try {
const browser = await playwright['chromium'].launch();
const context = await browser.newContext();
const page = await context.newPage();


   await page.goto('https://cnn.com');


   // Perform actions...


   await browser.close();


   // If the actions are performed successfully, break the loop
     break;
   } catch (error) {
     console.error('An error occurred:', error);
     console.log('Retrying...');


   retries++;
     if (retries === MAX_RETRIES) {
       console.log('Maximum retries reached. Exiting.');
       break;
     }
   }


}
}


run();


Handling CAPTCHAs
CAPTCHAs are designed to prevent automated interactions, so they can pose a significant challenge to a Playwright script.
There are several strategies you can use to handle CAPTCHAs:
1. Avoidance: Some sites may present a CAPTCHA challenge after a certain number of actions have been performed in a short period of time. By introducing the different methods that we discussed above in your Playwright script, you may be able to avoid triggering the CAPTCHA.
2. Manual Intervention: If a CAPTCHA does appear, you could pause the execution of your script and wait for a human to solve it manually.
3. Third-Party Services: Several third-party services, such as 2Captcha and Anti-Captcha provide APIs to solve CAPTCHAs. These services generally work by sending the CAPTCHA image to a server, where a human or advanced OCR software solves it and sends back the solution.
Here’s an example of how you might use such a service in your Playwright script
const playwright = require('playwright');
const axios = require('axios'); // for making HTTP requests


// Replace with your 2Captcha API key
const captchaApiKey = 'YOUR_2CAPTCHA_API_KEY';


async function solveCaptcha(captchaImage) {
// Send a request to the 2Captcha service
const response = await axios.post('https://2captcha.com/in.php', {
method: 'base64',
key: captchaApiKey,
body: captchaImage,
});


if (response.data.status !== 1) {
throw new Error('Failed to submit CAPTCHA for solving');
}


// Poll the 2Captcha service for the solution
while (true) {
const solutionResponse = await axios.get(
`https://2captcha.com/res.php?key=${captchaApiKey}&action=get&id=${response.data.request}`
);


   if (solutionResponse.data.status === 1) {
     return solutionResponse.data.request;
   }


   // If the CAPTCHA is not solved yet, wait for a few seconds before polling again
   await new Promise(resolve => setTimeout(resolve, 5000));


}
}


(async () => {
const browser = await playwright['chromium'].launch();
const context = await browser.newContext();
const page = await context.newPage();


await page.goto('https://example.com');


// If a CAPTCHA appears...
const captchaElement = await page.$('#captcha');
if (captchaElement) {
// Take a screenshot of the CAPTCHA image
const captchaImage = await captchaElement.screenshot({ encoding: 'base64' });


   // Solve the CAPTCHA
   const captchaSolution = await solveCaptcha(captchaImage);


   // Enter the CAPTCHA solution
   await page.fill('#captchaSolution', captchaSolution);


}


await browser.close();
})();


In this example, when a CAPTCHA is detected, the script takes a screenshot of the CAPTCHA image and sends it to the 2Captcha service for solving.
It then polls the 2Captcha service for the solution and enters it on the page. Please be aware that this solution depends highly on the website you are trying to access.
________________


Why Make Playwright Undetectable
Making Playwright undetectable to web services is a critical aspect of this field, and there are several compelling reasons for this.
Automation Resistance
The first reason lies in the phenomenon known as Automation Resistance. Websites today are more intelligent than ever. They have mechanisms in place to block or limit automated traffic. This is primarily to prevent spamming, abuse, and to ensure a fair usage policy for all users. However, this poses a significant challenge for legitimate automation tasks.
For instance, if you’re running an automated script to test the functionality of a website, the last thing you want is for your script to be blocked or limited. This is where the stealthy nature of Playwright comes into play.
Making Playwright undetectable ensures that your automation tasks can run smoothly without being hindered by the website’s anti-automation measures.
Data Collection
The second reason is related to Data Collection. In the age of information, data is king. However, access to this data is often restricted or monitored. Being undetectable is a significant advantage for tasks that involve data collection or web scraping.
If a website detects that a bot is scraping its data, it might block access or serve misleading information. Therefore, making Playwright undetectable allows for uninterrupted scraping, ensuring you can collect the data you need without any obstacles.
Testing
The third reason pertains to Testing scenarios. When testing a website, it’s crucial to simulate a real user’s browser as closely as possible. If a website detects that it’s being accessed by an automated script, it might behave differently, leading to inaccurate test results.
By making Playwright undetectable, it allows the automation script to mimic a real user’s browser without raising any automation flags. This ensures that the testing environment is as close to the real user experience as possible, leading to more accurate and reliable test results.
In conclusion, the undetectability of Playwright is a powerful feature that allows it to overcome automation resistance, facilitate data collection, and enable accurate testing. As we continue to rely on web automation for various tasks, the ability to remain undetectable will only become more valuable.
________________


Benefits of Making Playwright Undetectable
A stealthy Playwright setup offers significant advantages. It provides improved access to web content, bypassing measures that limit or block automated access. This is particularly beneficial for web scraping or testing tasks, allowing for efficient data collection and smooth operation.
Furthermore, it creates an accurate testing environment by mimicking a real user’s browser without detection, leading to reliable and precise test results.
Moreover, a stealthy setup reduces the risk of being identified as a bot, thereby minimizing the chance of IP bans or rate limiting.
This ensures the uninterrupted execution of automation tasks, making Playwright an invaluable tool in web automation.
________________


Case Study: Evading Playwright Detection on G2
Many sites employ sophisticated anti-scraping mechanisms to detect and block automated scripts. G2, a popular platform for software reviews, employs various measures to prevent automated access to its product categories pages.
In this case study we will explore the experience of scraping the G2 categories page using Playwright. We compare the results of a bare minimum script versus a fortified version.
Our goal is to scrape the product information without getting blocked.
Step 1: Standard Playwright Setup for Scraping G2
With our first approach We will use a basic Playwright script to navigate to the G2 product page and capture a screenshot
const playwright = require("playwright");


(async () => {
 const browser = await playwright.firefox.launch({ headless: false });
 const context = await browser.newContext();
 const page = await context.newPage();
 await page.goto("https://www.g2.com/categories");
 await page.screenshot({ path: "product-page.png" });
 await browser.close();
})();
This is a typical setup for using Playwright to scrape G2's product page.
* It launches a new Chromium browser, opens a new page, and navigates to G2 product page.
* In the standard setup, no specific configurations are made to evade detection.
* This means that the website will be detected.
This is the output:
 Page Blocked 

This script failed to retrieve the desired information. The page encountered CAPTCHA challenges. This result highlighted the limitations of a bare minimum approach, which does not mimic real user behavior.
Step 2: Fortified Playwright Script
To overcome these challenges, we fortified our script by modifying the browser-context and set-up session cookies. These modifications aimed to make our automated session appear more like a real user.
Finding Session Cookies
1. Open Browser Developer Tools : Navigate to the G2 categories page in a real browser (e.g., Chrome or Firefox).
2. Access Developer Tools : Right-click on the page and select "Inspect" or press Control + Shift + I to open the Developer Tools.
3. Navigate to Application/Storage : In Developer Tools, go to the "Application" tab in Chrome or "Storage" tab in Firefox.
 Chrome Developer Tools Application 
4. Locate Cookies : Under "Cookies," select the https://www.g2.com entry. Here, you will see a list of cookies set by the site.
5. Copy Session Cookie : Find the cookie named _g2_session_id. Copy its value, as this will be used in your Playwright script.
 Chrome Developer Tools Cookies 

Here's the fortified Playwright script that includes the browser context modification and session cookies:
const playwright = require("playwright");


(async () => {
 const browser = await playwright.firefox.launch({ headless: false });
 const context = await browser.newContext({
   userAgent:
     "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:127.0) Gecko/20100101 Firefox/127.0",
   viewport: { width: 1280, height: 800 },
   locale: "en-US",
   geolocation: { longitude: 12.4924, latitude: 41.8902 },
   permissions: ["geolocation"],
   extraHTTPHeaders: {
     "Accept-Language": "en-US,en;q=0.9",
     "Accept-Encoding": "gzip, deflate, br",
     Referer: "https://www.g2.com/",
   },
 });


 // Set the session ID cookie
 await context.addCookies([
   {
     name: "_g2_session_id",
     value: "0b484c21dba17c9e2fff8a4da0bac12d",
     domain: "www.g2.com",
     path: "/",
   },
 ]);


 const page = await context.newPage();
 await page.goto("https://www.g2.com/categories");
 await page.screenshot({ path: "product-page.png" });
 await browser.close();
})();
 G2 Product Page Passed 

Outcome
With the fortified script, the page loaded correctly, and we successfully bypassed the initial bot detection mechanisms. The addition of user agent modification and session cookies helped in simulating a real user session, which was crucial for avoiding detection and scraping the necessary data.
Comparison
   * Bare Minimum Script : Failed to bypass bot detection, leading to incomplete page loads and CAPTCHA challenges.
   * Fortified Script : Successfully mimicked a real user, allowing us to load the page and scrape data without interruptions.
________________


Best Practices and Considerations
Ethical Usage and Legal Implications
Respecting the website’s terms of service and privacy policies is crucial when scraping. Unauthorized data extraction can lead to legal consequences. Always seek permission when necessary and avoid scraping sensitive information.
Balancing Scraping Speed and Stealth
While speed is important in scraping, too fast can lead to detection and blocking. To maintain stealth, implement delays between requests, mimic human behavior, and use rotating IP addresses.
Monitoring and Adjusting Strategies for Evolving Bot Detection Techniques
Bot detection techniques are constantly evolving. Regularly monitor your scraping strategies and adjust them as needed. Keep an eye on changes in website structures and update your scraping code accordingly.
Combining Multiple Techniques for Enhanced Effectiveness
For effective scraping, combine multiple techniques, such as using different user agents, IP rotation, and CAPTCHA-solving services. This can help bypass anti-scraping measures and improve the success rate of your scraping tasks.
________________


Conclusion
In conclusion, ensuring that Playwright remains undetectable when automating web interactions involves employing various sophisticated techniques. By customizing browser settings, spoofing user agent strings, disabling automation flags, and simulating realistic user behaviors, You can minimize the risk of detection by websites' anti-bot mechanisms.
Additionally, leveraging tools like residential proxies and browser fingerprinting evasion techniques further enhances the stealthiness of Playwright scripts. These strategies not only optimize scraping efficiency but also mitigate potential interruptions such as CAPTCHA challenges and IP blocking.
Ultimately, by implementing these measures effectively,you can maintain the reliability and effectiveness of Playwright's automated data retrieval processes.
Check the official documentation of Playwright library to get more information.
________________


More Playwright Web Scraping Guides
To learn more about Playwright and web scraping techniques, check out some of our other articles.
   * The NodeJS Playwright Guide
   * Using Proxies With NodeJS Playwright
   * What Is Web Scraping? A Beginner's Guide On How To Get Started
   * How To Make Playwright Undetectable
   * TLDR: How to Make Playwright Undetectable
   * Understanding Website Bot Detection Mechanisms
   * How To Make Playwright Undetectable To Anti-Bots
   * 1. Setting User-Agent Strings
   * 2. Enabling WebGL and Hardware Acceleration
   * 3. Mimicking Human-Like Browser Environments
   * 4. Using Rotating Residential Proxies
   * 5. Mimicking Human Behavior
   * 6. Using Playwright with Real Browsers
   * Strategies To Make Playwright Undetectable
   * 1. Use Playwright Extra With Residential Proxies
   * 2. Use Hosted Fortified Version of Playwright
   * 3. Fortify Playwright Yourself
   * 4. Leverage ScrapeOps Proxy to Bypass Anti-Bots
   * Testing Your Playwright Scraper
   * Without Fortification
   * With Fortification
   * Handling Errors and CAPTCHAs
   * Handling Errors
   * Handling CAPTCHAs
   * Why Make Playwright Undetectable
   * Automation Resistance
   * Data Collection
   * Testing
   * Benefits of Making Playwright Undetectable
   * Case Study: Evading Playwright Detection on G2
   * Step 1: Standard Playwright Setup for Scraping G2
   * Step 2: Fortified Playwright Script
   * Best Practices and Considerations
   * Ethical Usage and Legal Implications
   * Balancing Scraping Speed and Stealth
   * Monitoring and Adjusting Strategies for Evolving Bot Detection Techniques
   * Combining Multiple Techniques for Enhanced Effectiveness
   * Conclusion
   * More Playwright Web Scraping Guides
Solutions & Resources
   * Proxy API Aggregator
   * Residential & Mobile Proxy Aggregator
   * Monitoring & Scheduler
   * Documentation
   * Proxy Comparison Tool
   * Blog
   * GitHub
Web Scraping Guides
   * Web Scraping Playbook
   * Python Web Scraping Playbook
   * NodeJs Web Scraping Playbook
   * Python Scrapy Playbook
   * Selenium Web Scraping Playbook
   * Puppeteer Web Scraping Playbook
   * Playwright Web Scraping Playbook
Company
   * Affiliate Program
   * Privacy Policy
   * Terms Of Service
   * Data Protection Policy
   * Data Processing Agreement
Copyright © 2025 ScrapeOps.